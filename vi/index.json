[{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/","title":"Báo cáo thực tập","tags":[],"description":"","content":"Báo cáo thực tập Thông tin sinh viên: Họ và tên: Lưu Vĩ Khánh\nSố điện thoại: 0345158878\nEmail: karlpro812005@gmail.com\nTrường: Đại học FPT\nNgành: Hệ thống thông tin\nLớp: AWS082025\nCông ty thực tập: Công ty TNHH Amazon Web Services Vietnam\nVị trí thực tập: FCJ Cloud Intern\nThời gian thực tập: Từ ngày 08/09/2025 đến ngày 28/11/2025 Nội dung báo cáo Worklog Proposal Các bài blogs đã dịch Các events đã tham gia Workshop Tự đánh giá Chia sẻ, đóng góp ý kiến "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Đánh Giá Khả Năng Phục Hồi (Resilience Assessment) Ứng Dụng Tập Trung Đa Tài Khoản Sử Dụng AWS Resilience Hub Bởi tác giả Venkata Kommuri, Venkata Moparthi, Santosh Vallurupalli, và Tracy Honeycutt | Ngày 05 tháng 08 năm 2025 | trong Management Tools\nGiới thiệu Khi các tổ chức mở rộng môi trường điện toán đám mây của họ trên nhiều tài khoản và khu vực AWS, việc quản lý và truy cập resilience trở nên ngày càng phức tạp. Các phương pháp truyền thống trong việc đánh giá resilience riêng biệt cho từng workload, tài khoản hoặc khu vực có thể dẫn đến sự kém hiệu quả, thiếu nhất quán và tạo ra các khoảng trống trong phạm vi bao phủ. Thách thức này đặc biệt rõ ràng trong các kiến trúc phân tán sử dụng nhiều công cụ Infrastructure as Code (IaC) như AWS CloudFormation và Terraform.\nAWS Resilience Hub giải quyết những thách thức này bằng cách cung cấp một dịch vụ được quản lý tập trung để giám sát và đánh giá resilience trên các kiến trúc đám mây đa tài khoản. Dịch vụ này cho phép các tổ chức xác định, theo dõi và tăng cường resilience của ứng dụng thông qua việc giám sát thống nhất trong một centralized hub master account. Việc tích hợp với Amazon Simple Notification System (SNS) mang lại khả năng phát hiện drift theo thời gian thực, giúp duy trì năng lực khôi phục mạnh mẽ trên các triển khai phân tán, đồng thời hỗ trợ các yêu cầu về độ trễ thấp, tuân thủ vị trí lưu trữ dữ liệu, và tăng cường resilience trước các sự cố gián đoạn.\nTìm hiểu AWS Resilience Hub AWS Resilience Hub là một vị trí trung tâm trong bảng điều khiển AWS (AWS console) giúp bạn quản lý và cải thiện tư thế phục hồi (resilience posture) của các ứng dụng trên AWS. AWS Resilience Hub cho phép bạn xác định mục tiêu phục hồi, đánh giá tư thế phục hồi của mình so với các mục tiêu đó và triển khai các khuyến nghị cải thiện dựa trên AWS Well-Architected Framework.\nKhả năng phục hồi được định nghĩa bằng các chỉ số gọi là RTO (Recovery Time Objective) và RPO (Recovery Point Objective). RTO là thước đo thời gian mà ứng dụng có thể khôi phục sau một sự cố ngừng hoạt động và RPO là thước đo lượng dữ liệu tối đa mà ứng dụng có thể chịu mất mát. AWS Resilience Hub cung cấp cái nhìn thống nhất về tư thế phục hồi của ứng dụng, xác định các rủi ro tiềm ẩn và đưa ra các khuyến nghị có thể hành động để tăng cường khả năng chống chịu trước sự gián đoạn. Bằng cách căn chỉnh với các thực tiễn tốt nhất của AWS Well-Architected Framework, Resilience Hub đảm bảo rằng các ứng dụng có thể chịu đựng và phục hồi từ nhiều tình huống lỗi khác nhau.\nNhững thách thức với các đánh giá phục hồi phân tán (Disaggregated Resilience Assessments) Dưới đây là một số thách thức chính khi sử dụng phương pháp đánh giá resilience phân tán:\nResilience Evaluations bị phân mảnh Các tổ chức quản lý workload trên nhiều tài khoản AWS thường tiến hành các đánh giá resilience độc lập, dẫn đến sự thiếu nhất quán trong tiêu chí đánh giá và xác định rủi ro. Không có cái nhìn thống nhất, các nhóm có thể gặp khó khăn trong việc liên kết các kết quả resilience giữa các workload, dẫn đến những \u0026ldquo;điểm mù\u0026rdquo; trong tổng thể resilience của ứng dụng.\nThiếu các Resilience Policies chuẩn hóa Khi không có các Resilience Policies tiêu chuẩn hóa, các nhóm khác nhau có thể định nghĩa mục tiêu khả dụng, RTO và RPO khác nhau. Sự thiếu nhất quán này có thể dẫn đến việc không đồng bộ giữa kế hoạch liên tục kinh doanh và khả năng thực tế của hạ tầng đám mây.\nKhó khăn trong việc tổng hợp dữ liệu Việc tổng hợp dữ liệu resilience từ nhiều nguồn khác nhau là một thách thức lớn khác. Không có nền tảng thống nhất, quản trị viên phải tổng hợp thủ công chi tiết của cơ sở hạ tầng được triển khai từ các công cụ như AWS CloudFormation hoặc các tệp Terraform state trên nhiều Region và account. Quy trình thủ công này có thể tạo ra lỗi và chậm trễ, khiến việc xác định các điểm lỗi trải dài trên nhiều accounts trở nên khó khăn, kéo dài thời gian phơi nhiễm với các gián đoạn tiềm ẩn.\nTrong bài viết này, chúng ta sẽ khám phá một kiến trúc giúp giải quyết các thách thức được nêu ở trên. Kiến trúc này sẽ hỗ trợ bạn thực hiện các resilience assessment cho các ứng dụng được triển khai trên nhiều AWS accounts và Regions.\nĐiều kiện tiên quyết Một tài khoản AWS Các mẫu AWS CloudFormation hoặc tệp Terraform định nghĩa ứng dụng của bạn Hướng dẫn Recovery Time Objective (RTO) và Recovery Point Objective (RPO) được xác định trước theo yêu cầu kinh doanh của bạn Solution Architecture AWS Resilience Hub Solution sử dụng mô hình hub and spoke để cung cấp khả năng quản lý resilience tập trung trên toàn bộ môi trường AWS, giúp các tổ chức đánh giá và nâng cao resilience của ứng dụng một cách hiệu quả. Giải pháp này hoạt động thông qua một central AWS hub account, đóng vai trò là điểm quản trị trung tâm cho việc định nghĩa các resilience policies, quản lý các bài đánh giá, và giám sát các ứng dụng trên các spoke account được kết nối. Tài khoản hub kết nối đến nhiều spoke account thông qua IAM roles và trust relationships. Các spoke account này chứa các workload và tài nguyên cần được đánh giá resilience.\nAWS Resilience Hub yêu cầu hai IAM roles để đảm bảo đánh giá resilience an toàn giữa các tài khoản. Vai trò chính AWSResilienceHubAssessmentRole trong hub account quản lý các hoạt động đánh giá, trong khi vai trò phụ AWSResilienceHubCrossAccountRole trong spoke account thực thi việc đánh giá workload. Vai trò đánh giá của hub account điều phối các đánh giá resilience bằng cách quản lý cấu hình, giả định các cross-account role, và phối hợp các hoạt động đánh giá workload.\nKhách hàng có thể tận dụng khả năng drift detection của AWS Resilience Hub để tự động đánh giá các workload của họ hằng ngày. Việc đánh giá này diễn ra một lần mỗi 24 giờ. Để tăng cường giám sát và phản ứng, khách hàng có thể tích hợp kết quả đánh giá drift detection này với một SNS topic. Bằng cách này, các nhóm liên quan có thể đăng ký nhận thông báo, cho phép họ nhận cảnh báo kịp thời và thực hiện các hành động khắc phục nhanh chóng khi phát hiện sai lệch. Sự tích hợp này giúp xây dựng phương pháp tiếp cận chủ động trong việc duy trì resilience của ứng dụng và đảm bảo rằng mọi thay đổi ảnh hưởng đến tư thế resilience đều được xử lý nhanh chóng.\nHình 1 – Sơ đồ kiến trúc của giải pháp tập trung hóa việc đánh giá resilience của ứng dụng đa tài khoản bằng AWS Resilience Hub.\nKey Components of the Solution Dưới đây là các thành phần cấu thành giải pháp:\nCentral Hub Account Đóng vai trò là trung tâm quản trị cho AWS Resilience Hub, nơi quản trị viên có thể quản lý các chính sách resilience, xem kết quả đánh giá, và điều phối chiến lược resilience trên nhiều tài khoản và ứng dụng. Tài khoản này cũng bao gồm một SNS topic nhận thông báo drift, cung cấp cảnh báo theo thời gian thực cho người dùng đã đăng ký.\nSpoke Accounts Chứa các workload và tài nguyên được AWS Resilience Hub đánh giá, kết nối đến central hub account thông qua IAM roles và trust relationships để cho phép đánh giá resilience.\nIAM Roles AWSResilienceHubAssessmentRole Nằm trong central hub account; quản lý cấu hình tổng thể, tạo các chính sách resilience, thiết lập ứng dụng để đánh giá, và giả định các cross-account role trong các spoke account để ủy quyền nhiệm vụ một cách an toàn.\nAWSResilienceHubCrossAccountRole Nằm trong spoke account; thực hiện đánh giá workload, thực thi các thao tác cấp tài nguyên, và báo cáo kết quả về hub admin role, đảm bảo việc đánh giá resilience chi tiết.\nTrust Relationships Cho phép vai trò quản trị trong hub account ủy quyền nhiệm vụ cho vai trò thực thi trong spoke account, duy trì sự phân tách rõ ràng về trách nhiệm đồng thời tạo điều kiện cho việc quản lý và đánh giá resilience hiệu quả trên toàn bộ môi trường AWS.\nBắt đầu Tải về các tệp CloudFormation cần thiết wget \\ https://d2908q01vomqb2.cloudfront.net/artifacts/MTBlog/cloudops-1962/ard.yaml \\ https://d2908q01vomqb2.cloudfront.net/artifacts/MTBlog/cloudops-1962/AWSResilienceHubAssessmentRole.yaml \\ https://d2908q01vomqb2.cloudfront.net/artifacts/MTBlog/cloudops-1962/AWSResilienceHubCrossAccountRole.yaml \\ https://d2908q01vomqb2.cloudfront.net/artifacts/MTBlog/cloudops-1962/AWSResilienceHubSNSTopic.yaml Triển khai các IAM Roles cho Central Hub Account và Spoke Accounts Trong hub account, chạy lệnh sau:\naws cloudformation create-stack \\ --template-body AWSResilienceHubAssessmentRole.yaml \\ --stack-name AWSResilienceHubAssessmentRole \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM Trong mỗi spoke account, chạy lệnh sau:\naws cloudformation create-stack \\ --template-body AWSResilienceHubCrossAccountRole.yaml \\ --stack-name AWSResilienceHubCrossAccountRole \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \\ --parameter-overrides AWSResilienceHubAssessmentRoleARN=arn:aws:iam::\u0026lt;hubaccount_number\u0026gt;:role/AWSResilienceHubAssessmentRole Tạo SNS topic để nhận thông báo khi ứng dụng phát hiện drift Mẫu CloudFormation này thiết lập một SNS topic cùng các quyền truy cập liên quan. Mẫu này cấu hình cả một SNS topic mới và tạo Resource Policy xác định các dịch vụ và người dùng nào có thể tương tác với topic. Cụ thể, cấu hình này cho phép AWS Resilience Hub gửi thông báo về kết quả đánh giá drift detection đến SNS topic, giúp giám sát các thay đổi trong resilience policies của bạn.\naws cloudformation create-stack \\ --template-body AWSResilienceHubSNStopic.yaml \\ --stack-name AWSResilienceHubSNSTopic \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \\ --parameter-overrides TopicName=dev-sample-topic EmailAddress=\u0026lt;Enter_Email_Address_of_Recipeint\u0026gt; Triển khai workload mẫu trong tài khoản Spoke aws cloudformation create-stack \\ --template-body ard.yaml \\ --stack-name resilience-hub-workloads \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM Sao chép ARN của CloudFormation Stack vừa được triển khai để sử dụng sau:\naws cloudformation describe-stacks --stack-name resilience-hub-workloads --query \u0026#34;Stacks[0].StackId\u0026#34; --output text Để minh họa chức năng đánh giá workload giữa các tài khoản sử dụng Resilience Hub, chúng ta sẽ triển khai một workload mẫu trong Spoke Account và phân tích kết quả đánh giá của nó từ Central Account. Ứng dụng mẫu này có kiến trúc ba tầng với các dịch vụ được triển khai trong một AZ bất cứ khi nào có thể, bao gồm Amazon CloudFront cho việc phân phối và lưu trữ nội dung đệm, Amazon Simple Storage Service (Amazon S3) cho lưu trữ đối tượng, một Amazon Elastic Compute Cloud (Amazon EC2) instance đóng vai trò web server, và một Amazon Relational Database Service (Amazon RDS) instance cho hoạt động cơ sở dữ liệu. Mục tiêu chính của bài viết này là minh họa cách Resilience Hub đánh giá resilience của kiến trúc này và khám phá cách chúng ta có thể cải thiện nó dựa trên các khuyến nghị mà công cụ cung cấp. Lưu ý rằng kiến trúc mẫu này không tuân theo các thực hành tốt nhất của AWS và được sử dụng chỉ nhằm mục đích minh họa cách AWS Resilience Hub xác định các khoảng trống resilience và tạo khuyến nghị cải tiến.\nHình 2 - Sơ đồ kiến trúc của workload mẫu ba tầng\nThiết Lập Resilience Assessment Tập Trung Trong Hub Account Để bắt đầu, đăng nhập vào tài khoản AWS hub và điều hướng đến AWS Management Console.\nChọn AWS Resilience Hub và chọn Add application → Get started, sau đó chọn Add application.\nChọn Add Application trong mục Resilience management. Nhập thông tin ứng dụng và chọn Resource collection. Thêm ARN của AWS CloudFormation stack của workload mẫu mà bạn đã sao chép từ Bước 3 (Deploy Sample Workload in Spoke Account).\nXác định chính sách resilience mới cho workload. Với workload mẫu này, chúng ta sẽ đặt chính sách \u0026ldquo;MissionCritical\u0026rdquo; với RTO là 10 phút và RPO là 4 phút. Nếu bạn đã có các Resiliency Policies được định nghĩa sẵn trong tài khoản, bạn có thể chọn sử dụng chúng bằng cách đánh dấu vào tùy chọn \u0026ldquo;Choose an existing resiliency policy\u0026rdquo;. Đối với đánh giá đa tài khoản, chọn AWSResilienceAssessmentRole từ menu thả xuống và trong mục Add cross account IAM role(s) ARN, thêm ARN của AWSResilienceCrossAccountRole từ Spoke Account(s).\nLưu ý: Nếu triển khai workload trên nhiều spoke accounts, hãy thêm ARN của AWSResilienceCrossAccountRole từ từng spoke account để bật khả năng resilience giữa các tài khoản.\nBằng cách chọn tùy chọn Automatically assess daily, bạn có thể thiết lập việc đánh giá tự động hằng ngày cho các workload được chỉ định trong CloudFormation và Terraform templates. Ngoài ra, bạn có thể nhận thông báo qua email thông qua SNS topic mà bạn đã đăng ký. Những thông báo này sẽ thông tin cho bạn về kết quả drift detection assessment, cảnh báo bạn về các thay đổi trong tư thế resilience của ứng dụng theo thời gian. Đánh dấu chọn \u0026ldquo;Get Notified when the application drifts\u0026rdquo; và chọn SNS topic đã được tạo trong Step 2. Sau đó, chọn Add application. Khi ứng dụng được nhập thành công từ mẫu AWS CloudFormation, chọn Publish application. Sau khi publish, ứng dụng của bạn đã sẵn sàng để được đánh giá. Chạy một Resilience Assessment Đi đến tab Resources để xem các thành phần được triển khai từ mẫu AWS CloudFormation cho workload mẫu trong Spoke Account. Bây giờ, bạn có thể chọn Assess application để bắt đầu quá trình đánh giá. Đặt tên cho báo cáo đánh giá của bạn và chọn Run. Trạng thái đánh giá sẽ hiển thị là \u0026ldquo;in progress\u0026rdquo; và sẽ hoàn tất sau vài phút. Xem Xét Resilience Assessment Sau khi hoàn tất đánh giá, báo cáo sẽ được tạo để xem xét.\nBạn có thể tìm thấy các báo cáo đánh giá của ứng dụng trong phần Assessments của ứng dụng. Trong menu điều hướng bên trái, chọn Applications. Trong Applications, mở ứng dụng của bạn. Trong tab Assessments, chọn một báo cáo từ phần Resiliency assessments. Khi bạn mở báo cáo, bạn sẽ thấy:\nTổng quan chung của báo cáo đánh giá Các khuyến nghị để cải thiện resilience Các khuyến nghị về thiết lập cảnh báo, Standard Operating Procedures (SOPs) và các bài kiểm thử Khi xem xét báo cáo trên, bạn sẽ thấy phần \u0026ldquo;Current Policy Compliance\u0026rdquo; chỉ ra rằng cơ sở hạ tầng hiện tại chưa đáp ứng RTO và RPO được xác định trong chính sách resilience của bạn. Để khắc phục điều này, AWS Resilience Hub cung cấp các khuyến nghị hữu ích, cho phép bạn triển khai các biện pháp điều chỉnh và tăng cường resilience của ứng dụng để phù hợp với mục tiêu đã đặt ra. Ví dụ, khi chọn RDS AppComponent, bạn sẽ thấy một tập hợp chi tiết các khuyến nghị đánh giá, như minh họa trong hình dưới đây.\nVới những khuyến nghị về resilience này, các nhóm ứng dụng và hạ tầng có thể thực hiện các thay đổi phù hợp cho hệ thống của họ, nâng cao khả năng chịu lỗi bất ngờ. Quy trình này thể hiện sức mạnh của chức năng cross-account trong AWS Resilience Hub – trong khi workload mẫu được triển khai trong Spoke Account, việc đánh giá được thực hiện trong Central Hub Account. Cách tiếp cận tập trung này cho phép các báo cáo đánh giá được tổng hợp tại Central Hub Account, mang lại cái nhìn toàn diện về resilience trên nhiều tài khoản và ứng dụng. Sự giám sát tập trung này giúp tổ chức duy trì tiêu chuẩn resilience nhất quán và quản lý hiệu quả các môi trường AWS đa tài khoản.\nKết Luận Giải pháp này mang lại phương pháp tiếp cận tập trung để đánh giá và cải thiện resilience của workload trên nhiều tài khoản và khu vực AWS, điều này rất quan trọng để duy trì tính liên tục của hoạt động kinh doanh và đáp ứng yêu cầu doanh nghiệp. Nó tích hợp liền mạch với các ứng dụng được triển khai thông qua AWS CloudFormation và Terraform, sử dụng mô hình hub and spoke cho phép các tổ chức đánh giá hiệu quả tư thế resilience trong khi vẫn duy trì bảo mật thông qua cơ chế kiểm soát truy cập dựa trên IAM Role.\nKhả năng đánh giá các stack được triển khai bên ngoài AWS Region của Central Hub Account, kết hợp với tích hợp cross-account IAM roles, nâng cao hiệu quả của dịch vụ bằng cách cho phép tổ chức có cái nhìn về các workload toàn cầu và đảm bảo các đánh giá an toàn, có thể mở rộng.\nBằng cách áp dụng AWS Resilience Hub, các tổ chức có thể biến việc quản lý resilience từ một quy trình phản ứng thành một chiến lược chủ động, tận dụng các đánh giá tự động, khuyến nghị chi tiết và các chỉ số resilience rõ ràng để xây dựng các ứng dụng có khả năng chịu lỗi cao, sẵn sàng khôi phục, phù hợp với các thực hành tốt nhất về phục hồi sau thảm họa và resilience vận hành, biến nó thành công cụ thiết yếu để duy trì một môi trường AWS resilient và được thiết kế tốt khi cơ sở hạ tầng đám mây ngày càng phức tạp.\nĐể bắt đầu triển khai giải pháp này trong tổ chức của bạn, hãy tải xuống các CloudFormation templates được cung cấp trong bài viết này và làm theo hướng dẫn triển khai từng bước trong tài liệu của AWS Resilience Hub. Bạn cũng có thể khám phá thêm các chiến lược đánh giá resilience khác thông qua AWS Well-Architected Framework.\nTiểu Sử Tác Giả TAGS: AWS Resilience Hub, Resiliency\nVenkata Kommuri Venkata Kommuri là Kiến trúc sư Hạ tầng (Infrastructure Architect) tại AWS, chuyên thiết kế và triển khai các giải pháp đám mây phức tạp trong cả môi trường thương mại và GovCloud. Ông có chuyên môn sâu trong các lĩnh vực như di chuyển hệ thống lên đám mây (cloud migration), container hóa, mạng (networking) và triển khai AI tạo sinh (Generative AI). Với khả năng xây dựng các kiến trúc doanh nghiệp có tính mở rộng và bảo mật cao, ông giúp thúc đẩy quá trình chuyển đổi số đồng thời duy trì hiệu quả vận hành vượt trội. Venkata Moparthi Venkata Moparthi là Kiến trúc sư Giải pháp Cấp cao (Senior Solutions Architect) tại AWS, chuyên về di chuyển hệ thống lên đám mây, AI tạo sinh và khả năng phục hồi (resilience). Với vai trò là một nhà lãnh đạo công nghệ đáng tin cậy, ông thiết kế các giải pháp đám mây mang tính đột phá cho lĩnh vực dịch vụ tài chính và nhiều ngành công nghiệp khác, luôn mang lại kết quả tối ưu bằng cách kết hợp đổi mới kỹ thuật với mục tiêu kinh doanh. Santosh Vallurupalli Santosh Vallurupalli là Kiến trúc sư Hạ tầng Đám mây Cấp cao (Senior Cloud Infrastructure Architect) trong nhóm Dịch vụ Chuyên nghiệp AWS (AWS ProServ Team). Anh đam mê hỗ trợ người dùng trong hành trình chuyển đổi lên đám mây và xây dựng các giải pháp Cloud Native để giải quyết những thách thức phức tạp. Santosh có chuyên môn về Mạng AWS và Container (EKS). Ngoài công việc, anh thích du lịch và xem lại bộ phim \u0026ldquo;The Office\u0026rdquo; nhiều lần. Tracy Honeycutt Tracy Honeycutt là Quản lý Kiến trúc Giải pháp (Solutions Architecture Manager) tại Atlanta, bang Georgia. Tracy tập trung vào việc hỗ trợ các khách hàng trong ngành Dịch vụ Tài chính (FSI) trên hành trình lên đám mây của họ — từ tăng tốc quá trình di chuyển, hiện đại hóa khối lượng công việc, đến áp dụng các phương thức làm việc mới. Bà có chuyên môn đặc biệt trong lĩnh vực mạng và DNS, đồng thời rất đam mê giúp đỡ các khách hàng mới bắt đầu hành trình điện toán đám mây. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Triển khai cầu nối USDC trên AWS Bởi Corey Cooper và Guillaume Goutaudier | ngày 09 tháng 6 năm 2025 | trong Advanced (300), AWS Step Functions, Blockchain, Technical How-to\nGiới thiệu Stablecoin mang lại những lợi thế đáng kể trong crypto space. Chúng cung cấp sự ổn định giá cả và có thể đóng vai trò như một phương tiện trao đổi đáng tin cậy, nơi lưu trữ giá trị hoặc cầu nối giữa hệ sinh thái fiat và crypto. Khả năng chuyển stablecoin trên nhiều blockchain khác nhau càng nâng cao tiện ích của chúng bằng cách cải thiện khả năng tương tác chuỗi chéo và cho phép người dùng tận dụng thế mạnh của các mạng lưới khác nhau.\nCircle là một nền tảng tài chính cung cấp stablecoin được quản lý, quỹ được mã hóa, dịch vụ dành cho nhà phát triển và dịch vụ thanh khoản để giúp các doanh nghiệp triển khai tương lai của tài chính trên nhiều blockchain. Circle được biết đến rộng rãi với stablecoin đô la nổi bật của mình, USDC.\nVới việc Circle gần đây phát hành dịch vụ khả năng tương tác của mình, CCTP (Cross-Chain Transfer Protocol) V2, chúng tôi muốn khám phá cách USDC có thể được chuyển giữa nhiều chuỗi bằng kiến trúc serverless trên AWS.\nTrong bài viết này, chúng tôi mô tả và triển khai một kiến trúc như vậy. Chúng tôi tập trung vào các thành phần backend, sử dụng Quickstart: Cross-Chain USDC Transfer từ tài liệu dành cho nhà phát triển của Circle làm tham chiếu. Chúng tôi minh họa cách sử dụng các dịch vụ serverless của AWS như AWS Lambda và AWS Step Functions để triển khai các workflow onchain.\nGiải pháp này đặc biệt có giá trị đối với các doanh nghiệp xây dựng nền tảng DeFi đa chuỗi, marketplace NFT hoặc hệ thống thanh toán xuyên biên giới. Bằng cách sử dụng các dịch vụ serverless của AWS, nhà phát triển có thể giảm độ phức tạp của hạ tầng, tăng tốc độ tích hợp các chuyển giao chuỗi chéo, đồng thời duy trì tính sẵn sàng cao và khả năng mở rộng với chi phí vận hành tối thiểu.\nTổng quan giải pháp Kiến trúc cấp cao của giải pháp được thể hiện trong sơ đồ sau.\nWorkflow được triển khai dưới dạng một Step Functions state machine điều phối việc thực thi của nhiều hàm Lambda. Các hàm Lambda này tương tác với mạng nguồn và mạng đích, Circle Attestation Service (quan sát và xác minh hoạt động burn trên chuỗi nguồn), và sử dụng AWS Secrets Manager để lưu trữ private key của tài khoản mà từ đó chúng ta muốn chuyển USDC.\nStep Functions mang lại nhiều lợi ích, bao gồm xử lý lỗi và timeout tích hợp, tích hợp trực tiếp với các dịch vụ AWS, quản lý trạng thái và theo dõi tiến trình workflow tích hợp sẵn, cùng lịch sử thực thi workflow theo thời gian thực và có thể audit.\nSơ đồ dưới đây minh họa workflow của CCTP V2 Fast Transfer.\nWorkflow này triển khai các bước bắt buộc để sử dụng CCTP V2 (để hiểu chi tiết, tham khảo CCTP developer documentation):\nXác thực các biến đầu vào. Tạo biểu diễn bytes32 của địa chỉ nơi USDC sẽ được gửi đến. Chấp thuận việc chi tiêu số lượng USDC cần thiết trên smart contract TokenMessengerV2. Burn USDC trên mạng nguồn bằng smart contract TokenMessengerV2. Lấy message và attestation từ Circle Attestation Service. Mint USDC trên mạng đích bằng smart contract MessageTransmitterV2 (lấy attestation mới nếu cần). Tại thời điểm viết bài, CCTP V2 hỗ trợ các mạng Ethereum, Avalanche, Base, Linea và Arbitrum. Do đó, chúng tôi tập trung triển khai trên các blockchain tương thích với EVM.\nWorkflow được triển khai trên các testnet sử dụng endpoint từ Public Node. Đối với môi trường production, cần xem xét thêm (tối thiểu):\nQuản lý private key cần được chú ý nhiều hơn. Tham khảo How to sign Ethereum EIP-1559 transactions using AWS KMS để tìm hiểu cách các dịch vụ bảo mật AWS như AWS Key Management Service (AWS KMS) có thể cải thiện mức độ bảo mật. Có thể triển khai multi-party authorization cho các giao dịch có giá trị cao.\nBạn có thể muốn chạy node riêng. Tham khảo AWS Blockchain Node Runners để có blueprint triển khai node và loạt bài AWS Nitro Enclaves for running Ethereum validators để biết chi tiết cách ký giao dịch trong Nitro enclave.\nBạn nên gắn các hàm Lambda vào VPC. Tham khảo Giving Lambda functions access to resources in an Amazon VPC để biết thêm chi tiết.\nVới mỗi bước trong workflow, bạn nên xem xét mọi loại lỗi có thể xảy ra và xác định cách xử lý. Ví dụ, uptime và khả năng sẵn sàng của dịch vụ CCTP được thiết kế tương tự như các dịch vụ mint hiện tại, nhưng bạn vẫn nên xác định cách xử lý các giao dịch bị trì hoãn hoặc việc tạm thời không khả dụng của Attestation Service.\nGiải pháp này cũng có thể được tích hợp vào một ứng dụng phi tập trung lớn hơn. Ví dụ: bạn có thể dùng Amazon Simple Notification Service (Amazon SNS) để gửi thông báo cho người dùng cuối, và sử dụng Amazon EventBridge để kích hoạt các hành động hậu chuyển giao.\nĐể triển khai giải pháp, chúng tôi sử dụng AWS CloudFormation template sau để triển khai giải pháp. Template yêu cầu private key làm tham số đầu vào, vì vậy chúng tôi tạo trước một private key trước khi triển khai. Sau đó, chúng tôi kiểm thử việc chuyển USDC từ Ethereum sang Base.\nYêu cầu trước Để triển khai giải pháp này, bạn cần có một tài khoản AWS với quyền thích hợp.\nTạo private key Có nhiều cách để tạo private key. Trong bài viết này, chúng tôi sử dụng Python Web3 library từ AWS CloudShell. Để tạo private key và địa chỉ liên kết, kết nối tới CloudShell và chạy lệnh sau:\npip install web3 \u0026amp;\u0026amp; \\ python -c \u0026#34;from web3 import Web3; w3 = Web3(); acc = w3.eth.account.create(); print(f\u0026#39;PRIVATE_KEY={acc._private_key.hex()}; ADDRESS={acc.address}\u0026#39;)\u0026#34; Thực thi output của lệnh trên để ghi lại PRIVATE_KEY và ADDRESS.\nTriển khai CloudFormation template Từ cùng phiên CloudShell, chạy lệnh sau để triển khai CloudFormation template:\naws cloudformation create-stack --region us-east-1 --stack-name \u0026#39;USDC-Bridge\u0026#39; --template-url \u0026#39;https://aws-blogs-artifacts-public.s3.us-east-1.amazonaws.com/artifacts/WEB3-2/USDC-Bridge-CloudFormation-Template.yml\u0026#39; --capabilities CAPABILITY_NAMED_IAM --parameters ParameterKey=PRIVATEKEY,ParameterValue=$PRIVATE_KEY Kiểm thử giải pháp Để có thể chuyển USDC từ một mạng sang mạng khác, bạn cần native token (ví dụ: ETH cho Ethereum) trên cả mạng nguồn và mạng đích (để trả gas fee) và USDC trên mạng nguồn (để có USDC để chuyển). Vì chúng tôi triển khai giải pháp trên testnet, bạn có thể nhận chúng từ các faucet. Sau đây là một số faucet bạn có thể dùng:\nUSDC Testnet Faucet from Circle Sepolia Faucets from Chainlink Ethereum Sepolia Faucet from Alchemy Multi-Chain Faucet from QuickNode Chainstack multi-chain faucet Free testnet faucets from thirdweb Trong phần này, chúng tôi minh họa việc chuyển USDC từ Ethereum sang Base, nhưng bạn có thể thử nghiệm với các mạng khác.\nĐể thực thi Step Functions state machine, chúng tôi chuẩn bị input như sau:\n{ \u0026#34;source_address\u0026#34;: \u0026#34;\u0026lt;replace_with_public_address_from_previous_step\u0026gt;\u0026#34;, \u0026#34;target_address\u0026#34;: \u0026#34;\u0026lt;replace_with_public_address_from_previous_step\u0026gt;\u0026#34;, \u0026#34;amount\u0026#34;: 10000, \u0026#34;max_fee\u0026#34;: 100, \u0026#34;source_domain\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;target_domain\u0026#34;: \u0026#34;6\u0026#34; } Chúng tôi sử dụng cùng một địa chỉ nguồn và đích, nhưng bạn có thể dùng địa chỉ đích khác. Ngoài ra, giải pháp này sử dụng Fast Transfer (tham khảo CCTP product fee schedule để biết thêm chi tiết).\nĐể thực thi state machine:\nTrên console Step Functions, chọn USDC_Bridge state machine. Chọn Start execution. Dán JSON document đã chuẩn bị. Chọn Start execution. Nếu mọi thứ diễn ra suôn sẻ, bạn sẽ thấy các bước trong state machine được thực thi thành công cho đến cuối workflow CCTP V2 Fast Transfer.\nNếu có lỗi, bạn sẽ dễ dàng xác định bước nào xảy ra sự cố, kiểm tra input và output của bước đó, và xem log của hàm Lambda. Step Functions cũng cung cấp khả năng resume execution sau khi lỗi ban đầu và tích hợp với Amazon CloudWatch để tăng cường khả năng quan sát và cảnh báo.\nSau khi workflow thực thi thành công, bạn có thể xác nhận rằng USDC đã được chuyển thành công và kiểm tra việc thực thi smart contract bằng block explorer. Đặc biệt, bạn có thể ghi chú transaction hash từ output của bước Mint USDC, và xác nhận log của giao dịch này cho thấy USDC mới đã được mint, như minh họa trong ảnh chụp màn hình sau.\nDọn dẹp Từ CloudShell, chạy lệnh sau để dọn dẹp tài nguyên:\naws cloudformation delete-stack --stack-name \u0026#39;USDC-Bridge\u0026#39; Kết luận Trong bài viết này, chúng tôi đã chỉ ra cách sử dụng CCTP V2, Lambda, Step Functions, và Secrets Manager để triển khai một giải pháp serverless nhằm chuyển USDC trên backend của ứng dụng onchain. Bạn có thể mở rộng giải pháp này và tích hợp vào bộ công cụ AWS rộng lớn hơn, biến đây thành một nền tảng lý tưởng để chạy các workload onchain. Chúng tôi khuyến khích bạn thử nghiệm và chia sẻ phản hồi của mình.\nVề các tác giả Corey Cooper Corey là một nhà phát triển tại Circle, người yêu thích bóng rổ và là fan trung thành của đội Lakers đến từ Atlanta. Anh có sự nghiệp gắn liền với việc xây dựng công nghệ doanh nghiệp và thanh toán. Với hơn 15 năm kinh nghiệm trong vai trò kỹ sư giải pháp, Corey mang đến sự kết hợp giữa chuyên môn kỹ thuật sâu sắc, hiểu biết kinh doanh và khả năng lãnh đạo thực tiễn cho các hoạt động ra mắt sản phẩm, triển khai thực tế (go-live) và xây dựng các nền tảng có khả năng mở rộng. Guillaume Goutaudier Guillaume là Kiến trúc sư Doanh nghiệp Cấp cao (Sr Enterprise Architect) tại AWS. Anh hỗ trợ các công ty xây dựng quan hệ đối tác kỹ thuật chiến lược với AWS. Guillaume cũng có niềm đam mê với các công nghệ blockchain và là thành viên của Technical Field Community for blockchain. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Tăng cường bảo mật lưu trữ đám mây AWS của bạn với Superna Defender Bởi Andrew Peng và Andrew MacKay | Ngày 7 tháng 7 năm 2025 | trong Amazon FSx, Amazon FSx for Windows File Server, Amazon Simple Storage Service (S3), AWS Marketplace, Partner solutions, Software, Storage\nBy Andrew MacKay, CTO and Strategy Officer – Superna\nBy Andrew Peng, Sr. Specialist SA, FSx for Windows File Server – AWS\nGiới thiệu Khi khách hàng di chuyển lưu trữ vào Amazon Web Services (AWS), việc bảo vệ lưu trữ tệp và đối tượng trên đám mây là một mối quan tâm về bảo mật. Hiểu và triển khai một khung lưu trữ bao gồm các biện pháp bảo mật mạnh mẽ là rất quan trọng để bảo vệ dữ liệu khỏi ransomware, truy cập trái phép và rò rỉ dữ liệu. Các tổ chức liên tục tìm kiếm một giải pháp có thể cung cấp khả năng bảo vệ và giám sát lưu trữ dựa trên đám mây của họ.\nTrong bài viết này, chúng ta sẽ khám phá cách Superna Defender có thể giúp giám sát và bảo vệ tài nguyên lưu trữ AWS ở cấp độ đối tượng hoặc tệp trong thời gian thực, bằng cách cung cấp thông tin \u0026ldquo;ai, cái gì và ở đâu\u0026rdquo; của mọi sự kiện. Superna Defender có thể cung cấp một giải pháp giúp duy trì sự phù hợp với các tiêu chuẩn Cyberstorage cho lưu trữ tệp và đối tượng, đồng thời cũng có thể giúp bảo vệ, giám sát và khôi phục hệ thống lưu trữ.\nCác mối đe dọa malware và ransomware đối với lưu trữ Mối đe dọa của ransomware và phần mềm độc hại đối với lưu trữ đám mây ngày càng trở nên phức tạp và tinh vi. Các thiết bị đầu cuối bị ảnh hưởng với quyền truy cập hợp lệ vào các chia sẻ mạng có thể mở ra một bề mặt tấn công lớn. Mất dữ liệu, ransomware và rò rỉ dữ liệu có thể xảy ra nếu các bucket Amazon Simple Storage Service (Amazon S3) bị cấu hình sai chính sách bảo mật hoặc các thiết bị bị nhiễm truy cập vào chia sẻ Amazon FSx for Windows File Server.\nThiệt hại từ ransomware vượt xa những tổn thất hoạt động và dữ liệu ngay lập tức; nó ảnh hưởng nghiêm trọng đến danh tiếng lâu dài. Việc một công ty không bảo vệ được dữ liệu khách hàng có thể làm xói mòn niềm tin, gây mất khách hàng và ảnh hưởng đến việc thu hút kinh doanh mới.\nTiến xa hơn lưu trữ đám mây để hướng tới Cyberstorage Các sự kiện phần mềm độc hại tiếp tục là mối đe dọa thường trực—ransomware, zero-day exploits và các mối đe dọa truy cập ngoài ý muốn đang rình rập khắp nơi. Mặc dù đã có các phương pháp và công cụ sao lưu truyền thống, các tổ chức vẫn trở thành nạn nhân. Đây chính là lúc khái niệm Cyberstorage xuất hiện.\nGartner đã tạo ra một danh mục mới gọi là Cyberstorage để phân loại các sản phẩm chủ động bảo vệ lưu trữ và dữ liệu chống lại các sự kiện mạng thông qua phòng ngừa, phát hiện sớm và chặn sự kiện. Superna đã được công nhận là một nhà lãnh đạo trong danh mục này, xuất hiện trong các báo cáo chu kỳ công nghệ của Gartner về lưu trữ dữ liệu phi cấu trúc và bảo vệ điểm cuối.\nCác tổ chức dựa vào nhiều công cụ bảo mật khác nhau từ bảo vệ điểm cuối, tường lửa, quản lý truy cập danh tính và phát hiện mối đe dọa. Họ cũng triển khai các chiến lược quản lý dữ liệu như backups, snapshots và tính năng bất biến. Nhưng với Cyberstorage, bảo mật có thể được nâng lên một bước xa hơn.\nKhi một sự kiện mạng xảy ra, giải pháp của Superna có thể phát hiện các mối đe dọa theo thời gian thực trên Amazon S3 và FSx for Windows File Server, thậm chí dừng ngay lập tức bằng cách cắt quyền truy cập của người dùng có nguy cơ. Các tệp bị ảnh hưởng trước tiên được cách ly, sau đó được khôi phục tự động. Cuối cùng, một bản ghi kiểm toán đầy đủ sẽ cung cấp thông tin \u0026ldquo;ai, cái gì và ở đâu\u0026rdquo; để hỗ trợ tuân thủ và điều tra pháp y sau sự cố.\nBối cảnh bảo mật phức tạp của Cyberstorage Hình 1 minh họa cách khung Cyberstorage định nghĩa nhiều lớp cho một kiến trúc lưu trữ tổng thể.\nHình 1: Superna Defender hoạt động tại lớp Dữ liệu (Data layer)\nHãy xem xét kế hoạch bảo mật doanh nghiệp điển hình bao gồm các thành phần như kiểm soát truy cập, mã hóa, giao thức bảo mật, phần mềm chống virus, tường lửa và ngăn ngừa mất dữ liệu. Chúng thường được bổ sung bởi kiểm toán, kiểm thử xâm nhập, cơ chế sao lưu và khôi phục, và thậm chí cả biện pháp an ninh vật lý như nhân viên bảo vệ và cửa khóa.\nMặc dù bối cảnh bảo mật này mạnh mẽ, nhưng cũng rất phức tạp. Một điểm yếu tại bất kỳ vị trí nào có thể dẫn đến hậu quả nghiêm trọng. Điều này đưa chúng ta đến một câu hỏi quan trọng: thường thì vấn đề không phải là \u0026ldquo;nếu\u0026rdquo; dữ liệu sẽ bị ảnh hưởng bởi một sự kiện mạng, mà là \u0026ldquo;khi nào\u0026rdquo;. Hiệu quả của phản ứng của tổ chức là điều sống còn.\nSuperna Defender bảo vệ lưu trữ đối tượng và tệp của bạn tại lớp Dữ liệu (Data Layer), cung cấp một \u0026ldquo;tuyến phòng thủ cuối cùng\u0026rdquo; nơi dữ liệu sản xuất tồn tại. Bằng cách tích hợp các khái niệm khung Cyberstorage, các tổ chức có thể củng cố tư thế bảo mật để đảm bảo rằng họ được trang bị tốt hơn để ngăn ngừa và ứng phó với truy cập ngoài ý muốn.\nTiêu chí cho danh sách kiểm tra Cyberstorage của NIST Superna Defender cũng có thể giúp khách hàng tuân thủ các điểm trong National Institute of Standards and Technology (NIST) Cyberstorage Checklist, như minh họa trong Hình 2.\nHình 2: Danh sách kiểm tra Cyberstorage của NIST, một phần của Khung bảo mật mạng NIST\nIdentify (Xác định) Để phản ứng với hoạt động đáng ngờ, khách hàng có thể sử dụng mô hình an ninh mạng của NIST kết hợp với các kiểm soát và quy tắc riêng cho tài khoản người dùng và máy.\nDetect (Phát hiện) Sử dụng machine learning, sản phẩm này giám sát Cyberstorage của khách hàng, phát hiện hoạt động đáng ngờ và tạo cảnh báo và báo cáo tương thích với phần mềm bên thứ ba. Nó cũng cung cấp khả năng mô phỏng sự kiện để kiểm tra phản ứng sự cố và tích hợp công cụ bảo mật.\nProtect (Bảo vệ) Superna Defender cung cấp khả năng bảo vệ chống lại xóa hàng loạt, sao chép hàng loạt, và cung cấp ngăn ngừa mất dữ liệu (DLP) cũng như phát hiện mẫu hoạt động tùy chỉnh thông qua Active Auditor dành cho AWS storage. Các sự kiện có thể được phân tích pháp y để cung cấp phân tích nguyên nhân gốc \u0026ldquo;ai, cái gì và khi nào\u0026rdquo; của các sự kiện Cyberstorage.\nHoạt động đáng ngờ vượt quá giới hạn xác định có thể kích hoạt tự động khóa tài khoản người dùng để đảm bảo an ninh ngay lập tức hoặc cảnh báo quản trị viên lưu trữ hoặc nhóm bảo mật để xem xét thủ công nhằm ngăn ngừa gián đoạn kinh doanh không cần thiết từ các dương tính giả tiềm ẩn. Chế độ học (Learning mode) tránh dương tính giả và tự động cấu hình dựa trên việc quan sát hoạt động.\nRespond (Phản ứng) Bảng điều khiển trung tâm và cảnh báo, được hỗ trợ bởi webhooks, Zero Trust API, và tích hợp với ServiceNow, Splunk, Palo Alto Networks, và AWS Security Hub, cung cấp khả năng giám sát, báo cáo và phản ứng sự cố liền mạch. Việc tự động vô hiệu hóa tài khoản người dùng, khóa API và token truy cập có thể giảm thiểu tác động của một sự cố Cyberstorage. Tích hợp với bảo vệ điểm cuối có thể mở rộng phản ứng bao gồm cách ly mạng máy chủ để giảm tác động và sự lây lan.\nRecover (Khôi phục) Cyber Recovery Manager cung cấp khôi phục thông qua phân tích một sự kiện mạng để xác định khi nào sự kiện đã xảy ra, và theo dõi tệp nào đã bị ảnh hưởng. Thông tin này cho phép khôi phục chính xác, chỉ khôi phục các tệp bị ảnh hưởng trong khi giữ nguyên các tệp khác, từ đó rút ngắn thời gian khôi phục và tăng năng suất.\nHệ thống giám sát tiên tiến theo dõi hoạt động của người dùng trong thời gian thực và lịch sử để tăng tốc và cải thiện độ chính xác điều tra sự cố. Người dùng có thể ngay lập tức xem thông tin liên quan đến sự cố để hiểu điều gì đã xảy ra và người dùng nào có liên quan. Người dùng cũng có thể xem lại các tệp bị ảnh hưởng và ưu tiên khôi phục những tệp nào trước.\nTổng quan kiến trúc của Superna Defender trong AWS Superna Defender là một native AWS Marketplace deployment và tận dụng một số dịch vụ AWS như minh họa trong Hình 3. Các dịch vụ này chạy một công cụ xử lý sự kiện theo thời gian thực trong mô hình tiêu dùng pay-as-you-go, có thể dừng và khởi động lại theo yêu cầu. Giải pháp này sử dụng Amazon EC2 Auto Scaling, VPCs, và security rules để giám sát và bảo vệ lưu trữ Amazon S3 và Amazon FSx for Windows File Server.\nHình 3: Sơ đồ kiến trúc Superna Defender\nKết luận Superna Defender, tích hợp AWS và Khung Cyberstorage của NIST, cung cấp một giải pháp bảo mật lưu trữ tệp và đối tượng đám mây toàn diện cho khách hàng. Superna Defender sử dụng với Amazon S3 hoặc Amazon FSx for Windows File Server có thể cung cấp kiểm toán và giám sát theo thời gian thực lưu trữ đối tượng và tệp trong AWS. Điều này cho phép khách hàng thấy \u0026ldquo;ai, cái gì và ở đâu\u0026rdquo; của các sự kiện hệ thống tệp, cho phép khôi phục dữ liệu chính xác theo thời điểm của dữ liệu bị ảnh hưởng.\nBắt đầu với Superna Defender trên AWS Marketplace, xem video demo của Superna Defender, và đọc báo cáo Gartner Innovation Insight về cách Cyberstorage giảm thiểu tác động của các cuộc tấn công mạng.\nKhách hàng nên cân nhắc các dịch vụ họ chọn vì trách nhiệm của họ thay đổi tùy thuộc vào dịch vụ được sử dụng, tích hợp các dịch vụ đó vào môi trường CNTT của họ, và các luật và quy định áp dụng.\nKết nối với Superna Superna – AWS Partner Spotlight Superna cung cấp khả năng kiểm toán tệp và đối tượng phổ quát cùng giảm thiểu mối đe dọa cho môi trường hybrid cloud của bạn với nền tảng hội tụ điều phối và bảo mật.\nContact Superna | Partner Overview | AWS Marketplace\nTAGS: Amazon FSx, AWS Partner Solution, Cloud Storage Security, Storage, Superna\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Bài thu hoạch “[AWS GenAI Builder Club] AI-Driven Development Life Cycle – Reimagining Software Engineering” Mục Đích Của Sự Kiện Sự kiện [AWS GenAI Builder Club] AI-Driven Development Life Cycle được tổ chức với mục tiêu giúp người tham dự hiểu thêm về cách trí tuệ nhân tạo (AI) đang thay đổi toàn bộ quy trình phát triển phần mềm hiện đại. Cụ thể, chương trình nhằm:\nGiới thiệu phương pháp AI-DLC (AI-Driven Life Cycle) – một mô hình mới tích hợp AI vào từng giai đoạn của vòng đời phát triển phần mềm.\nLàm rõ vai trò và lợi ích của AI trong việc rút ngắn thời gian, tối ưu quy trình, và nâng cao chất lượng sản phẩm phần mềm.\nGiới thiệu nền tảng Kiro.dev – một công cụ AI IDE (Integrated Development Environment) do AWS hỗ trợ, giúp các nhóm phát triển chuyển đổi từ ý tưởng đến sản phẩm hoàn thiện nhanh chóng hơn.\nDanh Sách Diễn Giả Toan Huynh - Specialist SA. My Nguyen - Sr. Prototyping Architect, Amazon Web Services - ASEAN. Nội Dung Nổi Bật AI-DLC Methodology – Phương pháp phát triển phần mềm dựa trên AI Phương pháp AI-DLC được xem là một bước tiến trong lĩnh vực kỹ nghệ phần mềm, khi AI không chỉ hỗ trợ lập trình mà còn được tích hợp xuyên suốt toàn bộ vòng đời phát triển:\nTừ giai đoạn ý tưởng, AI giúp gợi ý yêu cầu, xác định phạm vi dự án.\nTrong thiết kế và lập trình, AI hỗ trợ viết code, tạo tài liệu thiết kế, và phát hiện lỗi sớm.\nKhi kiểm thử và triển khai, AI có thể tự động hóa quy trình, dự đoán rủi ro và đề xuất cải tiến.\nKiro.dev – The AI IDE Kiro.dev được giới thiệu như một AI-powered IDE (môi trường phát triển tích hợp với AI) giúp người dùng tạo ứng dụng từ bản thiết kế đến bản chạy thực tế chỉ bằng prompt. Một số tính năng nổi bật gồm:\nTự động sinh cấu trúc dự án gồm các file như plan.md, thư mục /inception/, và các tài liệu mô tả.\nQuản lý nhiệm vụ (AI tasks) theo kế hoạch, có thể giao từng phần cho AI thực hiện và con người giám sát.\nTương tác linh hoạt: nếu prompt chưa chính xác, người dùng có thể chỉnh sửa, tái sinh hoặc chuyển mô hình AI khác để đạt kết quả mong muốn.\nAmazon Q Developer Amazon Q Developer hoạt động như một AI assistant được tích hợp trong AWS Console, VS Code với các khả năng nổi bật:\nTạo và hiểu mã nguồn tự nhiên: bạn có thể hỏi bằng ngôn ngữ tự nhiên và Q Developer sẽ sinh mã hoàn chỉnh Tích hợp ngữ cảnh dự án: Q Developer hiểu toàn bộ mã nguồn trong workspace, từ đó cung cấp câu trả lời chính xác theo ngữ cảnh cụ thể. Những Gì Học Được Hiểu về AI-DLC, Kiro.dev và Amazon Q Developer AI-DLC là phương pháp luận: cách tiếp cận tổng thể trong phát triển phần mềm với AI ở mọi giai đoạn. Kiro.dev là nền tảng thực thi AI-DLC: công cụ hỗ trợ xây dựng ứng dụng nhanh chóng dựa trên ý tưởng. Amazon Q Developer: là trợ lý AI dành riêng cho lập trình viên, giúp tăng tốc phát triển và triển khai trực tiếp trên môi trường AWS. Sử dụng Prompt Template và quản lý output AI Việc học cách thiết kế prompt hiệu quả là kỹ năng then chốt:\nXác định rõ vai trò (role) mà AI sẽ đảm nhận.\nViết rõ plan và AI task.\nQuản lý kết quả AI thông qua thư mục /inception/ để dễ hiệu chỉnh.\nLuôn kiểm duyệt đầu ra vì AI có thể tạo nội dung sai hoặc chưa phù hợp.\nTư duy kết hợp con người và AI Các diễn giả nhấn mạnh rằng AI là cộng sự, không phải người thay thế. AI có thể sinh ý tưởng, viết code, và kiểm thử, nhưng con người mới là người chịu trách nhiệm kiểm chứng, đánh giá và định hướng. Điều này mở ra một tư duy mới trong phát triển phần mềm – nơi AI và con người cùng đồng sáng tạo\nTrải nghiệm trong event Học hỏi từ các diễn giả có chuyên môn cao Các diễn giả đến từ AWS và các tổ chức công nghệ lớn đã chia sẻ best practices trong thiết kế ứng dụng hiện đại. Qua các case study thực tế, tôi hiểu thêm cách áp dụng Domain-Driven Design (DDD) và Event-Driven Architecture vào các project lớn. Ứng dụng công cụ hiện đại Trực tiếp tìm hiểu về Amazon Q Developer, công cụ AI hỗ trợ SDLC từ lập kế hoạch đến maintenance. Học cách tự động hóa code transformation và pilot serverless với AWS Lambda, từ đó nâng cao năng suất phát triển. Bài học rút ra AI-DLC và các công cụ như Kiro.dev hay Amazon Q Developer đang thay đổi cách chúng ta lập trình.\nNhờ AI, quá trình từ ý tưởng → thiết kế → triển khai có thể rút ngắn đáng kể.\nViệc học cách giao tiếp và kiểm soát AI sẽ trở thành kỹ năng cốt lõi của lập trình viên hiện đại.\nTương lai của phần mềm không còn là “AI thay người”, mà là “người + AI = năng suất gấp đôi”.\nMột số hình ảnh khi tham gia sự kiện "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Bài thu hoạch “AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS – Generative AI with Amazon Bedrock” Mục Đích Của Sự Kiện Giới thiệu tổng quan về hệ sinh thái AI/ML/GenAI của AWS. Khái quát về Managed AI Services trên AWS, công dụng, và mô hình giá. Nắm được kỹ thuật Prompt Engineering áp dụng vào các use case thực tế. Tìm hiểu kiến trúc Retrieval-Augmented Generation (RAG) Giới thiệu foundation Models và các loại Foundation Models (FMs) phổ biến Danh Sách Diễn Giả Lam Tuan Kiet – Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Nội Dung Nổi Bật Generative AI với Amazon Bedrock Giới thiệu các Foundation Models phổ biến (Claude, Llama, Titan) và tiêu chí lựa chọn theo bài toán: phân tích văn bản, chatbot, reasoning, sáng tạo nội dung, embedding. Thực hành các Prompting Techniques: Zero-shot: mô hình trả lời trực tiếp không cần ví dụ → phù hợp cho câu hỏi đơn giản. Few-shot: cung cấp ví dụ mẫu để mô hình học pattern → tăng độ chính xác trong các tác vụ chuyên biệt. Chain-of-Thought (CoT): yêu cầu mô hình suy luận từng bước → cải thiện các bài toán logic, phân tích dữ liệu, reasoning. RAG (Retrieval-Augmented Generation): kết hợp LLM với dữ liệu doanh nghiệp (S3, DynamoDB, Aurora…) để giảm hallucination, tạo câu trả lời chính xác theo ngữ cảnh. Bedrock Agents: xây dựng workflow đa bước (multi-step), gọi API, truy cập dữ liệu, tự động hóa process mà không cần viết backend phức tạp. Guardrails: thiết lập chính sách an toàn nội dung (lọc toxic, hạn chế chủ đề, regex filtering, schema bắt buộc) để AI tuân thủ chuẩn doanh nghiệp. Managed AI Services trên AWS Rekognition: phân tích hình ảnh/video (detect objects, faces, text, moderation). Transcribe: chuyển giọng nói thành văn bản, phù hợp cho call center và caption. Polly: tạo giọng nói tự nhiên từ văn bản. Comprehend: phân tích ngôn ngữ tự nhiên (sentiment, key phrases, PII detection). Translate: dịch máy theo thời gian thực hoặc batch. Textract: trích xuất dữ liệu từ tài liệu scan/PDF. Personalize: xây dựng hệ thống gợi ý như Netflix/Amazon. Mini Kahoot Review Mini quiz giúp củng cố kiến thức về prompting techniques (zero-shot, few-shot, CoT, structured output) thông qua các câu hỏi tình huống. Tăng mức độ tương tác và tạo không khí sôi nổi, giúp học viên nắm nội dung dễ hơn so với lý thuyết thuần túy. Trải nghiệm và bài học cá nhân Prompting Techniques: Ấn tượng nhất với các kỹ thuật zero-shot, few-shot, CoT, giúp tôi hiểu thêm cách triển khai GenAI trong các dự án thực tế. Hands-on Demo với Bedrock: Giúp hình dung quy trình xây dựng và triển khai AI end-to-end. Mini Khoot: Củng cố kiến thức ngay tại chỗ, giúp nhớ lâu hơn về các bước quan trọng và kỹ thuật prompt. Networking: Kết nối với cộng đồng AWS và học hỏi từ các chuyên gia trong nước, mở rộng cơ hội hợp tác. Trải nghiệm trong event Tham dự workshop “AI/ML/GenAI on AWS – Generative AI with Amazon Bedrock” là một trải nghiệm cực kỳ giá trị, giúp tôi có cái nhìn hệ thống về cách xây dựng và triển khai các giải pháp GenAI trên nền AWS. Một vài điểm nổi bật mà tôi thu được::\nHọc hỏi từ các chuyên gia AWS Các diễn giả từ AWS đã chia sẻ nhiều góc nhìn thực tế về việc lựa chọn Foundation Model, thiết kế kiến trúc RAG và vận hành GenAI trong doanh nghiệp. Những ví dụ minh họa rõ ràng giúp tôi hiểu thêm cách áp dụng GenAI vào các bài toán thực tiễn như phân tích tài liệu, tạo chatbot, hay tự động hóa nghiệp vụ. Khám phá công cụ \u0026amp; dịch vụ AWS Trải nghiệm trực tiếp với Amazon Bedrock và Guardrails giúp tôi hiểu thêm cách AWS hỗ trợ triển khai GenAI an toàn và hiệu quả. Tôi cũng học được cách tận dụng các dịch vụ AI có sẵn như Rekognition, Comprehend, Transcribe hoặc Textract để giải quyết nhanh các nhu cầu về thị giác máy tính và NLP. Kết nối và trao đổi Workshop là cơ hội tốt để trao đổi với cộng đồng sử dụng AWS, cũng như lắng nghe các kinh nghiệm thực chiến từ những đội đã triển khai GenAI. Bài học rút ra -Sử dụng RAG, Bedrock Agents và Prompting Techniques giúp giảm dependency, tăng tính mở rộng (scalability) và độ bền (resilience) cho giải pháp GenAI.\nCác Managed AI Services và công cụ như Amazon Bedrock có thể tăng năng suất phát triển. Một số hình ảnh khi tham gia sự kiện "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/","title":"Nhật ký công việc","tags":[],"description":"","content":"Trong trang này bạn sẽ cần giới thiệu worklog của bạn như thế nào? Bạn hoàn thành chương trình trong vòng bao nhiêu tuần? Bạn đã làm gì trong các tuần đó?\nThông thường và cũng là tiêu chuẩn, một worklog được thực hiện trong khoảng 3 tháng (trong suốt thời gian thực tập) với nội dung các tuần như sau:\nTuần 1: Làm quen với AWS và các dịch vụ cơ bản trong AWS\nTuần 2: Triển khai và quản lý Amazon EC2, Route 53 Resolver, VPC Peering và Transit Gateway\nTuần 3: Học EC2, triển khai AWS Backup, File Storage Gateway và bắt đầu với Amazon S3\nTuần 4: Học Storage Services, AWS Backup nâng cao, VM Import/Export và tham gia sự kiện AI-Driven Development\nTuần 5: Triển khai Amazon FSx, làm việc nhóm, thiết kế kiến trúc dự án và học AWS Security Services\nTuần 6: Tối ưu chi phí EC2 với Lambda, quản lý tài nguyên với Tag, IAM Permission Boundary và mã hóa với AWS KMS\nTuần 7: Hoàn thiện kiến trúc dự án, IAM Role \u0026amp; Condition, ôn tập Database và chuẩn bị thi giữa kỳ AWS\nTuần 8: Ôn tập toàn diện các dịch vụ AWS, Load Balancing, S3, VPC, Security và tham gia kỳ thi giữa kỳ\nTuần 9: Bắt đầu triển khai dự án với Lambda Trigger, API Gateway, DynamoDB và làm workshop framework\nTuần 10: Tích hợp API vào frontend, chuyển đổi sang Java, giải quyết lỗi ClassNotFoundException và tham dự AWS Cloud Mastery Series\nTuần 11: Tham dự sự kiện AWS, giải quyết lỗi Sandbox Timeout, tích hợp X-Ray và CloudWatch, phối hợp team database\nTuần 12: Hoàn thiện dự án, giải quyết lỗi CORS và JWT, viết workshop, push code lên GitHub và tham dự sự kiện Security Pillar\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.1-week1/","title":"Worklog Tuần 1","tags":[],"description":"","content":"Mục tiêu tuần 1: Tìm hiểu kiến thức nền tảng về AWS và điện toán đám mây. Thiết lập và cấu hình tài khoản AWS Free Tier. Học cách vẽ kiến trúc AWS và sử dụng các công cụ quản lý. Làm quen với các khái niệm về mạng AWS (VPC, networking). Thực hành tạo và quản lý VPC cơ bản. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Xem hướng dẫn vẽ kiến trúc AWS trên draw.io và thực hành - Tải các công cụ cần thiết 8/09/2025 8/09/2025 Xem hướng dẫn vẽ kiến trúc AWS trên draw.io và thực hành 3 - Tìm hiểu AWS + Điện toán đám mây + Điều Gì Tạo Nên Sự Khác Biệt Của AWS + Hạ Tầng Toàn Cầu Của AWS + Công Cụ Quản Lý AWS Services + Bắt Đầu Hành Trình Lên Mây Như Thế Nào + Tối Ưu Hóa Chi Phí Trên AWS và Làm Việc Với AWS Support 9/09/2025 9/09/2025 - Điện toán đám mây - Điều Gì Tạo Nên Sự Khác Biệt Của AWS - Hạ Tầng Toàn Cầu Của AWS - Công Cụ Quản Lý AWS Services - Bắt Đầu Hành Trình Lên Mây Như Thế Nào - Tối Ưu Hóa Chi Phí Trên AWS và Làm Việc Với AWS Support 4 - Thiết lập Tài Khoản AWS: + Tạo mới tài khoản AWS + MFA cho Tài khoản AWS + Tạo Admin Group và Admin User + Hỗ trợ Xác thực Tài khoản - Quản lý chi phí với AWS Budget + Tạo Budget + Tạo Cost Budget + Tạo Usage Budget + Tạo RI Budget + Tạo Saving Plans Budget + Dọn Dẹp Tài nguyên 10/09/2025 10/09/2025 - Thiết lập Tài Khoản AWS - Quản lý chi phí với AWS Budget 5 - Tìm hiểu dịch vụ mạng trên AWS : + Amazon virtual Private Cloud (VPC) + VPC peering \u0026amp; Transit gateway + VPN \u0026amp; Direct Connect + Elastic load balancing - Các cách remote SSH vào EC2 - Tìm hiểu Elastic IP 11/09/2025 11/09/2025 - AWS Virtual Private Cloud - VPC Security and Multi-VPC features - VPN - DirectConnect - LoadBalancer - ExtraResources 6 - Bắt đầu với Amazon Virtual Private Cloud (VPC) và AWS Site-to-Site VPN: + Tường lửa trong VPC + Tạo VPC, subnet + Tạo Internet Gateway + Tạo Route table cho Outbound Internet Routing thông qua Internet Gateway + Tạo security groups 12/09/2025 12/09/2025 - Bắt đầu với Amazon Virtual Private Cloud (VPC) và AWS Site-to-Site VPN Kết quả đạt được tuần 1: Tìm hiểu kiến thức nền tảng AWS:\nHiểu khái niệm điện toán đám mây và lợi ích của AWS Biết về hạ tầng toàn cầu của AWS (Regions, AZs) Học về các công cụ quản lý AWS Services Hiểu cách bắt đầu hành trình lên mây và tối ưu hóa chi phí Thiết lập tài khoản AWS thành công:\nTạo mới tài khoản AWS Free Tier Cấu hình MFA cho bảo mật tài khoản Tạo Admin Group và Admin User Hoàn thành xác thực tài khoản Quản lý chi phí với AWS Budget:\nTạo và cấu hình các loại Budget: Cost, Usage, RI, Saving Plans Thiết lập cảnh báo chi phí để kiểm soát ngân sách Làm quen với kiến thức mạng AWS:\nHiểu về Amazon VPC và các thành phần Biết về VPC peering, Transit Gateway Tìm hiểu VPN, Direct Connect và Elastic Load Balancing Học cách remote SSH vào EC2 và sử dụng Elastic IP Thực hành tạo VPC:\nTạo VPC và subnet Cấu hình Internet Gateway Thiết lập Route table cho Outbound Internet Routing Tạo và cấu hình Security Groups Hiểu về tường lửa trong VPC Kỹ năng vẽ kiến trúc:\nHọc cách vẽ kiến trúc AWS trên draw.io Tải và sử dụng các công cụ cần thiết cho việc thiết kế "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.2-week2/","title":"Worklog Tuần 2","tags":[],"description":"","content":"Mục tiêu tuần 2: Triển khai và quản lý Amazon EC2 instances trong môi trường VPC. Thiết lập hybrid DNS với Route 53 Resolver. Triển khai Microsoft AD và cấu hình DNS endpoints. Thực hành VPC Peering để kết nối các VPC. Thiết lập AWS Transit Gateway cho kết nối multi-VPC. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Triển khai Amazon EC2 Instances: + Tạo máy chủ EC2 + Kiểm tra kết nối + Tạo NAT Gateway + Sử dụng Reachability Analyzer + Tạo EC2 Instance Connect Endpoint + AWS Systems Manager Session Manager CloudWatch Monitoring \u0026amp; Alerting 15/09/2025 15/09/2025 - Bắt đầu với Amazon Virtual Private Cloud (VPC) và AWS Site-to-Site VPN 3 - Thiết lập hybrid DNS với Route 53 Resolver: + Tạo key pair + Khởi tạo CloudFormation Template + Cấu hình security group 16/09/2025 16/09/2025 Thiết lập hybrid DNS với Route 53 Resolver 4 - Kết nối RDGW - Triển khai Microsoft AD - Thiết lập DNS: + Tạo Route 53 Outbound Endpoint + Tạo Route 53 Resolver Rules + Thử nghiệm kết quả + Tạo Route 53 Inbound Endpoints 17/09/2025 17/09/2025 Thiết lập hybrid DNS voi Route 53 Resolver 5 - Lắp đặt VPC peering: + Khởi tạo CloudFormation Template + Tạo Security Group + Tạo EC2 instance + Cập nhật Network ACL + Tạo kết nối Peering - Kích hoạt Cross-Peer DNS 18/09/2025 18/09/2025 Thiết lập VPC Peering 6 - Set up AWS Transit Gateway + Tạo Key Pair + Khởi tạo CloudFormation Template + Tạo Transit Gateway + Tạo Transit Gateway Attachments + Tạo Transit Gateway Route Tables + Thêm Transit Gateway Routes vào VPC Route Tables 19/09/2025 19/09/2025 Set up AWS Transit Gateway Kết quả đạt được tuần 2: Triển khai Amazon EC2 Instances thành công:\nTạo và cấu hình máy chủ EC2 trong VPC Kiểm tra kết nối và troubleshoot network issues Tạo NAT Gateway cho outbound internet access Sử dụng Reachability Analyzer để phân tích connectivity Thiết lập EC2 Instance Connect Endpoint Cấu hình AWS Systems Manager Session Manager Thiết lập CloudWatch Monitoring \u0026amp; Alerting Thiết lập Hybrid DNS với Route 53 Resolver:\nTạo key pair cho bảo mật Khởi tạo và triển khai CloudFormation Template Cấu hình security group cho DNS traffic Hiểu cách DNS resolution hoạt động trong hybrid environment Triển khai Microsoft AD và DNS Configuration:\nKết nối RDGW (Remote Desktop Gateway) Triển khai Microsoft Active Directory Tạo Route 53 Outbound Endpoint Cấu hình Route 53 Resolver Rules Tạo Route 53 Inbound Endpoints Thử nghiệm và xác minh kết quả DNS resolution Thực hành VPC Peering:\nKhởi tạo CloudFormation Template cho multi-VPC setup Tạo Security Group cho cross-VPC communication Tạo EC2 instance trong các VPC khác nhau Cập nhật Network ACL cho peering traffic Tạo kết nối VPC Peering Kích hoạt Cross-Peer DNS resolution Thiết lập AWS Transit Gateway:\nTạo Key Pair cho EC2 instances Triển khai CloudFormation Template cho Transit Gateway Tạo và cấu hình Transit Gateway Thiết lập Transit Gateway Attachments Tạo Transit Gateway Route Tables Thêm Transit Gateway Routes vào VPC Route Tables Hiểu cách Transit Gateway hoạt động như hub trung tâm "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.3-week3/","title":"Worklog Tuần 3","tags":[],"description":"","content":"Mục tiêu tuần 3: Tìm hiểu kiến thức sâu về Amazon EC2 và các thành phần liên quan. Triển khai AWS Backup cho việc bảo vệ dữ liệu hệ thống. Thực hành File Storage Gateway để kết nối on-premises và cloud. Bắt đầu với Amazon S3 và các tính năng cơ bản. Tích hợp S3 với CloudFront và quản lý lifecycle. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Học lý thuyết: + Amazon Elastic Compute Cloud (EC2) - Instance type + Amazon Elastic Compute Cloud (EC2) - AMI / Backup / Key Pair\n+ Amazon Elastic Compute Cloud (EC2) - Elastic block store + Amazon Elastic Compute Cloud (EC2) - Instance store + Amazon Elastic Compute Cloud (EC2) - User data + Amazon Elastic Compute Cloud (EC2) - Meta data + Amazon Elastic Compute Cloud (EC2) - EC2 auto scaling + EC2 Autoscaling - EFS/FSx - Lightsail - MGN 22/09/2025 22/09/2025 - Amazon Elastic Compute Cloud ( EC2 ) - Instance type - Amazon Elastic Compute Cloud ( EC2 ) - AMI / Backup / Key Pair - Amazon Elastic Compute Cloud ( EC2 ) - Elastic block store\n- Amazon Elastic Compute Cloud ( EC2 ) - Instance store\n- Amazon Elastic Compute Cloud ( EC2 ) - User data - Amazon Elastic Compute Cloud ( EC2 ) - Meta data - Amazon Elastic Compute Cloud ( EC2 ) - EC2 auto scaling\n- EC2 Autoscaling - EFS/FSx - Lightsail - MGN 3 - Triển khai AWS Backup cho hệ thống - Giới thiệu + Triển khai hạ tầng + Tạo Backup plan + Test Restore + Dọn tài nguyên\n23/09/2025 23/09/2025 Triển khai AWS Backup cho hệ thống 4 - Triển khai File Storage Gateway + Tạo S3 Bucket + Tạo EC2 cho Storage Gateway + Tạo Storage Gateway + Tạo File Shares 24/09/2025 24/09/2025 Triển khai File Storage Gateway 5 - Khởi Đầu Với Amazon S3\n+ Tạo S3 bucket + Tải dữ liệu + Bật tính năng static website + Cấu hình public access block + Cấu hình public objects - Kiểm tra website 25/09/2025 25/09/2025 Khởi Đầu Với Amazon S3 6 - Khởi Đầu Với Amazon S3 + Khóa tất cả public access + Cấu hình Amazon CloudFront + Kiểm tra Amazon CloudFront + Bucket Versioning + Move objects + Sao chép đối tượng đa vùng\n+ Xóa tài nguyên 26/09/2025 26/09/2025 Khởi Đầu Với Amazon S3 Kết quả đạt được tuần 3: Tìm hiểu kiến thức sâu về Amazon EC2:\nHiểu các loại Instance types và cách chọn phù hợp Học về AMI, Backup strategies và Key Pair management Hiểu Elastic Block Store (EBS) và các loại volume Biết về Instance Store và sự khác biệt với EBS Sử dụng User Data để tự động hóa cấu hình Truy cập và sử dụng Meta Data service Hiểu EC2 Auto Scaling và các chiến lược scaling Tìm hiểu EFS/FSx, Lightsail và MGN Triển khai AWS Backup thành công:\nTriển khai hạ tầng cho backup system Tạo và cấu hình Backup Plan Thực hiện test restore để xác minh backup Dọn dẹp tài nguyên sau khi hoàn thành Hiểu các best practices cho backup strategy Thực hành File Storage Gateway:\nTạo S3 Bucket làm backend storage Triển khai EC2 instance cho Storage Gateway Cấu hình và kích hoạt Storage Gateway Tạo và quản lý File Shares Hiểu cách hybrid storage hoạt động Bắt đầu với Amazon S3 - Cơ bản:\nTạo và cấu hình S3 bucket Tải lên và quản lý dữ liệu Bật tính năng static website hosting Cấu hình public access block cho bảo mật Quản lý public objects và permissions Kiểm tra và xác minh website hoạt động Amazon S3 - Nâng cao và Tích hợp:\nKhóa tất cả public access để bảo mật Cấu hình Amazon CloudFront distribution Kiểm tra và tối ưu hóa CloudFront performance Thiết lập Bucket Versioning cho version control Sử dụng lifecycle policies để move objects Thực hiện sao chép đối tượng đa vùng (cross-region replication) Dọn dẹp tài nguyên và quản lý chi phí "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.4-week4/","title":"Worklog Tuần 4","tags":[],"description":"","content":"Mục tiêu tuần 4: Tìm hiểu kiến thức về các dịch vụ lưu trữ AWS. Thực hành triển khai AWS Backup với các tính năng nâng cao. Học cách sử dụng VM Import/Export để migration. Triển khai File Storage Gateway cho hybrid cloud. Tham gia sự kiện về AI-Driven Development. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Học lý thuyết:\n+ Dịch Vụ Lưu Trữ Trên AWS + Amazon Simple Storage Service (S3) - Access Point - Storage Class + S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier + Snow Family - Storage Gateway - Backup 29/09/2025 29/09/2025 - Dịch Vụ Lưu Trữ Trên AWS - Amazon Simple Storage Service ( S3 ) - Access Point - Storage Class - S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier\n- Snow Family - Storage Gateway - Backup\n3 - Triển khai AWS Backup cho hệ thống: + Tạo S3 Bucket + Triển khai hạ tầng + Tạo Backup plan + Thiết lập thông báo + Kiểm tra hoạt động + Dọn dẹp tài nguyên\n30/09/2025 30/09/2025 Triển khai AWS Backup cho hệ thống 4 - VM Import/Export: + Chuẩn bị máy ảo + Export máy ảo từ On-premise + Tải máy ảo lên AWS + Import máy ảo vào AWS + Triển khai EC2 Instance từ AMI + Thiết lập ACL cho S3 Bucket + Export máy ảo từ EC2 Instance + Export máy ảo từ AMI + Dọn dẹp tài nguyên\n1/10/2025 1/10/2025 VM Import/Export 5 - Triển khai File Storage Gateway: + Tạo S3 Bucket + Tạo EC2 cho Storage Gateway + Tạo Storage Gateway + Tạo File Shares + Kết nối File shares ở máy On-premise + Dọn dẹp tài nguyên 2/10/2025 2/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Tham gia sự kiện AI-Driven Development Life Cycle: Reimagining Software Engineering 3/10/2025 3/10/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 4: Tìm hiểu kiến thức về Storage Services:\nHiểu tổng quan về các dịch vụ lưu trữ trên AWS Học về Amazon S3 Access Points và Storage Classes Hiểu S3 Static Website, CORS, Control Access Biết về Object Key optimization và Performance tuning Tìm hiểu Amazon Glacier cho long-term storage Học về Snow Family cho data migration Làm quen với Storage Gateway và AWS Backup services Triển khai AWS Backup nâng cao:\nTạo S3 Bucket cho backup storage Triển khai hạ tầng backup hoàn chỉnh Tạo và cấu hình Backup Plan chi tiết Thiết lập hệ thống thông báo (notifications) Kiểm tra và monitor hoạt động backup Dọn dẹp tài nguyên và quản lý chi phí Thành thạo VM Import/Export:\nChuẩn bị và đóng gói máy ảo từ on-premises Export máy ảo từ môi trường on-premise Tải máy ảo lên AWS S3 Import máy ảo vào AWS thành AMI Triển khai EC2 Instance từ AMI đã import Thiết lập ACL cho S3 Bucket security Export máy ảo từ EC2 Instance và AMI Dọn dẹp tài nguyên sau migration Triển khai File Storage Gateway hoàn chỉnh:\nTạo S3 Bucket làm backend storage Triển khai EC2 instance cho Storage Gateway Cấu hình và kích hoạt Storage Gateway Tạo và quản lý File Shares Kết nối File Shares từ máy on-premise Test và xác minh kết nối hybrid storage Dọn dẹp tài nguyên và tối ưu hóa Tham gia sự kiện AI-Driven Development:\nTìm hiểu về AI-Driven Development Life Cycle Hiểu cách AI đang thay đổi Software Engineering Tìm hiểu các xu hướng mới trong phát triển phần mềm Kết nối với cộng đồng developer và chia sẻ kinh nghiệm "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.5-week5/","title":"Worklog Tuần 5","tags":[],"description":"","content":"Mục tiêu tuần 5: Triển khai và quản lý Amazon FSx for Windows File Server. Làm việc nhóm và lên kế hoạch cho dự án cuối khóa. Tìm hiểu và thiết kế kiến trúc cho dự án. Tìm hiểu các dịch vụ bảo mật và quản lý danh tính AWS. Thực hành với AWS Security Hub. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Triển khai FSx trên Windows: + Tạo môi trường thực hành + Tạo một SSD Multi-AZ file system + Tạo một HDD Multi-AZ file system + Tạo file share + Kiểm tra hiệu năng +Giám sát hiệu năng 6/10/2025 6/10/2025 Triển khai FSx trên Windows 3 - Họp làm project : + Phân công nhiệm vụ + Quyết định dùng dịch vụ nào + Tra cuu giá tiền từng dịch vụ 7/10/2025 7/10/2025 4 - Tìm hiểu kiến trúc của dự án - Vẽ kiến trúc 8/10/2025 8/10/2025 https://drive.google.com/file/d/1ZIhwiHPyB1biXU1bg6HgXIaV6frzPs7K/view?usp=sharing 5 - Học lý thuyết: + Share Responsibility Model + Amazon Identity and access management + Amazon Cognito + AWS Organization + AWS Identity Center + Amazon Key Management Service + AWS Security Hub 9/10/2025 9/10/2025 - Share Responsibility Model - Amazon Identity and access management - Amazon Cognito\n- AWS Organization\n- AWS Identity Center\n- Amazon Key Management Service\n- AWS Security Hub\n6 - Get started with AWS Security Hub + Kích hoạt Security Hub + Điểm từng bộ tiêu chuẩn + Dọn dẹp tài nguyên + Kiểm tra hiệu năng +Giám sát hiệu năng 10/10/2025 10/10/2025 Bắt đầu với AWS Security Hub Kết quả đạt được tuần 5: Triển khai Amazon FSx for Windows thành công:\nTạo môi trường thực hành cho FSx Triển khai SSD Multi-AZ file system cho high performance Triển khai HDD Multi-AZ file system cho cost optimization Tạo và cấu hình file shares Kiểm tra và đánh giá hiệu năng hệ thống Giám sát hiệu năng và tối ưu hóa Làm việc nhóm và quản lý dự án:\nHọp nhóm và thảo luận kế hoạch dự án Phân công nhiệm vụ và vai trò cho từng thành viên Quyết định lựa chọn dịch vụ AWS phù hợp Tra cứu và ước tính giá tiền từng dịch vụ Xây dựng kế hoạch thực hiện chi tiết Thiết kế kiến trúc dự án:\nTìm hiểu và phân tích yêu cầu kiến trúc Vẽ sơ đồ kiến trúc tổng thể của dự án Xác định các thành phần và kết nối giữa chúng Đảm bảo kiến trúc tuân theo AWS best practices Tìm hiểu các dịch vụ bảo mật AWS:\nHiểu Shared Responsibility Model của AWS Học về Amazon Identity and Access Management (IAM) Tìm hiểu Amazon Cognito cho user authentication Học về AWS Organizations cho multi-account management Biết về AWS Identity Center (SSO) Hiểu Amazon Key Management Service (KMS) Tìm hiểu AWS Security Hub cho security monitoring Thực hành với AWS Security Hub:\nKích hoạt và cấu hình AWS Security Hub Điểm đánh giá từng bộ tiêu chuẩn bảo mật Phân tích các findings và recommendations Kiểm tra hiệu năng và tính năng giám sát Dọn dẹp tài nguyên và quản lý chi phí Hiểu cách tích hợp với các dịch vụ bảo mật khác "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.6-week6/","title":"Worklog Tuần 6","tags":[],"description":"","content":"Mục tiêu tuần 6: Tối ưu hóa chi phí EC2 sử dụng AWS Lambda và automation. Quản lý tài nguyên hiệu quả bằng Tag và Resource Groups. Tìm hiểu quản lý truy cập với AWS IAM và Resource Tags. Thực hành giới hạn quyền với IAM Permission Boundary. Triển khai mã hóa dữ liệu với AWS KMS. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tối ưu chi phí EC2 với Lambda: + Tạo VPC + Tạo Security Group + Tạo EC2 instance + Incoming Web-hooks Slack + Tạo Tag cho Instance + Tạo Role cho Lambda + Tạo Lambda Function + Kiểm tra kết quả 13/10/2025 13/10/2025 Tối ưu chi phí EC2 với Lambda 3 - Quản lý tài nguyên bằng Tag và Resource Groups: + Sử dụng Tag + Tạo một Resource Group + Dọn dẹp tài nguyên 14/10/2025 14/10/2025 Quản lý tài nguyên bằng Tag và Resource Groups 4 - Quản lý truy cập vào dịch vụ EC2 Resource Tag với AWS IAM: + Tạo IAM user + Tạo IAM Policy + Tạo IAM Role + Kiểm tra chính sách + Dọn dẹp tài nguyên 15/10/2025 15/10/2025 Quản lý truy cập vào dịch vụ EC2 Resource Tag với AWS IAM 5 - GIỚI HẠN QUYỀN CỦA USER VỚI IAM PERMISSION BOUNDARY: + Tạo Policy Giới hạn + Tạo IAM User Giới Hạn + Kiểm tra IAM User Giới Hạn + Dọn dẹp tài nguyên 16/10/2025 16/10/2025 GIỚI HẠN QUYỀN CỦA USER VỚI IAM PERMISSION BOUNDARY 6 - Mã hóa ở trạng thái lưu trữ với AWS KMS: + Tạo Policy và Role + Tạo Group và User + Tạo Key Management Service + Tạo Amazon S3 + Tạo AWS CloudTrail và Amazon Athena +Kiểm thử và chia sẻ dữ liệu mã hóa trên S3 + Dọn dẹp tài nguyên 17/10/2025 17/10/2025 Mã hóa ở trạng thái lưu trữ với AWS KMS Kết quả đạt được tuần 6: Tối ưu hóa chi phí EC2 với Lambda:\nTạo VPC và cấu hình môi trường mạng Thiết lập Security Group cho bảo mật Triển khai EC2 instance cho testing Cấu hình Incoming Web-hooks Slack cho thông báo Tạo và quản lý Tag cho EC2 instances Tạo IAM Role cho Lambda function Viết và triển khai Lambda Function tự động hóa Kiểm tra kết quả và xác minh tính năng cost optimization Quản lý tài nguyên với Tag và Resource Groups:\nHiểu và áp dụng chiến lược Tag cho tài nguyên Tạo và cấu hình Resource Groups Quản lý và tổ chức tài nguyên theo nhóm Dọn dẹp tài nguyên một cách hiệu quả Hiểu lợi ích của resource organization Quản lý truy cập EC2 với IAM và Resource Tags:\nTạo và cấu hình IAM users Viết và triển khai IAM Policies dựa trên tags Tạo và quản lý IAM Roles Kiểm tra và xác minh chính sách truy cập Dọn dẹp tài nguyên và IAM entities Hiểu fine-grained access control Giới hạn quyền với IAM Permission Boundary:\nTạo Policy giới hạn (Permission Boundary) Tạo IAM User với giới hạn quyền Kiểm tra và test IAM User giới hạn Hiểu cách Permission Boundary hoạt động Dọn dẹp tài nguyên và policies Tìm hiểu security best practices Mã hóa dữ liệu với AWS KMS:\nTạo IAM Policy và Role cho KMS Thiết lập IAM Group và User Tạo và quản lý AWS Key Management Service keys Cấu hình Amazon S3 với encryption Triển khai AWS CloudTrail và Amazon Athena cho monitoring Kiểm thử và chia sẻ dữ liệu mã hóa trên S3 Dọn dẹp tài nguyên và quản lý chi phí Hiểu encryption at rest và key management "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.7-week7/","title":"Worklog Tuần 7","tags":[],"description":"","content":"Mục tiêu tuần 7: Hoàn thiện và tối ưu hóa kiến trúc dự án cuối khóa. Tìm hiểu thêm về IAM Role và Condition policies. Ôn tập và nâng sâu kiến thức về Database services. Ôn lại các khái niệm cơ bản về Cloud Computing và IAM. Ôn tập EC2 và Storage concepts cho kỳ thi giữa kỳ. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tiếp tục chỉnh sửa kiến trúc và cân nhắc những dịch vụ sẽ dùng: + Thay thế Amazon RDS thành DynamoDB + Thay Amazon Lex thành Bedrock + Chỉnh sửa sơ đồ toàn diện + Chỉnh sửa kích thước đúng chuẩn 20/10/2025 20/10/2025 3 - IAM Role \u0026amp; Condition: + Giới thiệu về IAM + Tạo IAM Group + Tạo IAM User + Cấu hình Role Condition + Dọn dẹp tài nguyên 21/10/2025 21/10/2025 IAM Role \u0026amp; Condition 4 - Học lý thuyết: + Ôn tập Database Concepts + Amazon RDS \u0026amp; Amazon Aurora + Redshift - ElastiCache 22/10/2025 22/10/2025 - Database Concepts review - Amazon RDS \u0026amp; Amazon Aurora - Redshift - Elasticache\n5 - Ôn lý thuyết về cloud computing: + What is Cloud Computing? + AWS Global Infrastructure + AWS Shared Responsibility Model - Ôn lý thuyết về IAM: Identity Access \u0026amp; Management (IAM) + What Is IAM? +Multi-Factor Authentication (MFA) + CLI và SDK - Học Well Architected Framework 23/10/2025 23/10/2025 - Cloud computing - IAM: Identity Access \u0026amp; Management (IAM) - Well Architected Framework\n6 - Ôn lý thuyết về EC2: Virtual Machines: + What is Amazon EC2? + Security Groups + EC2 Instance Launch Types - Ôn lý thuyết về EC2 Instance Storage + EBS Volumes + EFS: Elastic File System và EFS Infrequent Access (EFS-IA) + Đặc điểm EC2 Instance Store - Học Well Architected Framework 24/10/2025 24/10/2025 - EC2: Virtual Machines - EC2 Instance Storage - Well Architected Framework\nKết quả đạt được tuần 7: Hoàn thiện kiến trúc dự án:\nTiếp tục chỉnh sửa và tối ưu hóa kiến trúc Cân nhắc và lựa chọn những dịch vụ phù hợp nhất Thay thế Amazon RDS bằng DynamoDB cho NoSQL solution Thay Amazon Lex bằng Amazon Bedrock cho AI capabilities Chỉnh sửa sơ đồ toàn diện và tổng thể Chỉnh sửa kích thước đúng chuẩn và professional Tìm hiểu thêm về IAM Role \u0026amp; Condition:\nÔn tập và giới thiệu sâu về IAM Tạo và quản lý IAM Groups Tạo và cấu hình IAM Users Cấu hình Role Conditions cho fine-grained access Dọn dẹp tài nguyên và best practices Hiểu conditional access và context-based permissions Ôn tập Database Concepts:\nÔn tập Database Concepts cơ bản Hiểu sâu về Amazon RDS và Amazon Aurora Tìm hiểu Amazon Redshift cho data warehousing Học về Amazon ElastiCache cho caching solutions So sánh các loại database và use cases Hiểu performance optimization strategies Ôn tập Cloud Computing Fundamentals:\nÔn lại \u0026ldquo;What is Cloud Computing?\u0026rdquo; và các khái niệm cơ bản Học AWS Global Infrastructure Hiểu rõ AWS Shared Responsibility Model Ôn tập IAM: Identity Access \u0026amp; Management Học Multi-Factor Authentication (MFA) Hiểu CLI và SDK usage Học Well Architected Framework Ôn tập EC2 và Storage:\nÔn tập \u0026ldquo;What is Amazon EC2?\u0026rdquo; và các concepts Hiểu Security Groups và network security Học EC2 Instance Launch Types Học về EBS Volumes và storage options Hiểu EFS: Elastic File System và EFS-IA Học đặc điểm EC2 Instance Store Tiếp tục học Well Architected Framework principles "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.8-week8/","title":"Worklog Tuần 8","tags":[],"description":"","content":"Mục tiêu tuần 8: Ôn tập toàn diện các dịch vụ AWS cho kỳ thi giữa kỳ. Ôn tập Load Balancing, Auto Scaling và S3 concepts. Ôn lại Global Infrastructure và Cloud Monitoring. Ôn tập VPC, Security \u0026amp; Compliance concepts. Chuẩn bị và tham gia kỳ thi giữa kỳ. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Ôn lý thuyết về Elastic Load Balancing \u0026amp; Auto Scaling Groups: + Scalability \u0026amp; High Availability + Vertical Scalability và Horizontal Scalability + Load Balancing và Auto Scaling Group - Ôn lý thuyết về Amazon S3 + S3 Use cases, security, Bucket Policies, Versioning + S3 Storage Classes + AWS Snow Family + AWS Storage Gateway Framework - Ôn lý thuyết về Databases \u0026amp; Analytics + AWS RDS, Amazon Aurora, Amazon ElastiCache, DynamoDB, Redshift - Ôn đề 27/10/2025 27/10/2025 - Elastic Load Balancing \u0026amp; Auto Scaling Groups - Amazon S3 - Databases \u0026amp; Analytics\n- Ôn đề\n3 - Ôn lý thuyết về Global Infrastructure: + Why make a global application? + Amazon Route 53 + AWS CloudFront + AWS Global Accelerator - Ôn lý thuyết về Cloud Monitoring: + Amazon CloudWatch + AWS CloudTrail + AWS X-Ray + AWS Status - Service Health Dashboard và AWS Personal Health Dashboard - Ôn đề 28/10/2025 28/10/2025 - Global Infrastructure - Cloud Monitoring - Ôn đề\n4 - Ôn lý thuyết về VPC: + VPC \u0026amp; Subnets Primer + Internet Gateway (IGW), NAT Gateway + Network ACL \u0026amp; Security Groups + VPC Peering, VPC Endpoints, Site to Site VPN \u0026amp; Direct Connect, Transit Gateway - Ôn lý thuyết về Security \u0026amp; Compliance + AWS Shared Responsibility Model + AWS Shield, AWS WAF, AWS KMS, AWS CloudHSM, AWS Certificate Manager (ACM), AWS Secrets Manager, AWS Artifact, AWS GuardDuty, AWS Inspector, AWS Config, AWS Security Hub - Ôn đề 29/10/2025 29/10/2025 - VPC - Security \u0026amp; Compliance - Ôn đề\n5 - Ôn lý thuyết về Other Compute: + ECS (Elastic Container Service) và ECR (Elastic Container Registry) + Fargate, AWS Lambda, Amazon API Gateway + Serverless - Ôn lý thuyết về Account Management, Billing \u0026amp; Support + AWS Organizations + Service Control Policies (SCP), AWS Organization - Consolidated Billing, Pricing Models in AWS + Savings Plan, Cost Allocation Tags, Tagging and Resource Groups, Cost and Usage Reports, Cost Explorer, AWS Budgets - Ôn đề 30/10/2025 30/10/2025 - Other Compute - Account Management \u0026amp; Billing - Ôn đề\n6 - Ôn đề + Đi thi ca 2 + Review lại bài đã làm 31/10/2025 31/10/2025 - Ôn đề Kết quả đạt được tuần 8: Ôn tập Load Balancing \u0026amp; Auto Scaling:\nHiểu Scalability \u0026amp; High Availability concepts Phân biệt Vertical Scalability và Horizontal Scalability Ôn tập Load Balancing và Auto Scaling Group Hiểu các loại Load Balancer và use cases Ôn tập Auto Scaling policies và strategies Ôn tập Amazon S3 \u0026amp; Storage:\nÔn tập S3 Use cases, security, Bucket Policies Hiểu S3 Versioning và lifecycle management Ôn tập S3 Storage Classes và cost optimization Tìm hiểu AWS Snow Family cho data migration Hiểu AWS Storage Gateway Framework Ôn tập Databases \u0026amp; Analytics:\nÔn tập AWS RDS và các tính năng Hiểu Amazon Aurora và performance benefits Tìm hiểu Amazon ElastiCache cho caching Ôn tập DynamoDB NoSQL concepts Hiểu Amazon Redshift cho data warehousing Ôn tập Global Infrastructure:\nHiểu tại sao cần global application Ôn tập Amazon Route 53 DNS service Hiểu AWS CloudFront CDN Tìm hiểu AWS Global Accelerator Ôn tập Cloud Monitoring:\nÔn tập Amazon CloudWatch monitoring Hiểu AWS CloudTrail cho auditing Tìm hiểu AWS X-Ray cho tracing Biết về AWS Status và Personal Health Dashboard Ôn tập VPC \u0026amp; Networking:\nHiểu VPC \u0026amp; Subnets concepts Ôn tập Internet Gateway và NAT Gateway Phân biệt Network ACL và Security Groups Hiểu VPC Peering, Endpoints, Site-to-Site VPN Tìm hiểu Direct Connect và Transit Gateway Ôn tập Security \u0026amp; Compliance:\nÔn tập AWS Shared Responsibility Model Hiểu AWS Shield, WAF, KMS, CloudHSM Tìm hiểu Certificate Manager và Secrets Manager Biết về AWS Artifact, GuardDuty, Inspector Hiểu AWS Config và Security Hub Ôn tập Other Compute Services:\nHiểu ECS và ECR cho containers Ôn tập AWS Fargate serverless containers Ôn lại AWS Lambda và API Gateway Hiểu Serverless architecture patterns Account Management \u0026amp; Billing:\nÔn tập AWS Organizations Hiểu Service Control Policies (SCP) Tìm hiểu Consolidated Billing và Pricing Models Hiểu Savings Plans và cost optimization Ôn tập Cost Explorer, Budgets và tagging Chuẩn bị thi giữa kỳ:\nLàm đề thi thử và ôn tập liên tục Tham gia kỳ thi giữa kỳ ca 2 Review lại các bài đã làm và rút kinh nghiệm Đánh giá kết quả và xác định điểm cần cải thiện "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.9-week9/","title":"Worklog Tuần 9","tags":[],"description":"","content":"Mục tiêu tuần 9: Tiếp tục ôn tập và củng cố kiến thức sau kỳ thi. Bắt đầu triển khai dự án cuối khóa với các dịch vụ serverless. Thiết lập Lambda Trigger và tích hợp với S3. Xây dựng API Gateway và kết nối với Lambda và DynamoDB. Bắt đầu làm workshop framework cho dự án. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tiếp tục review lại đề để ôn lại kiến thức 3/11/2025 3/11/2025 Ôn tập kiến thức AWS 3 - Bắt đầu làm project + Chuẩn bị \u0026amp; Cấu hình Lambda Trigger cho S3 + Tạo S3 Bucket + Cấu hình Lambda Trigger cho S3 + Upload dữ liệu CSV lên Bucket 4/11/2025 4/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tiếp tục làm project + Tạo API Gateway và tích hợp Lambda + Tạo IAM Role + Tạo Lambda Function DynamoDB_API_Handler - Tạo API Gateway và tích hợp với Lambda + Truy cập dịch vụ API Gateway + Tạo REST API + Tạo Resource, tạo Method cho từng bảng trong DynamoDB + Gắn Lambda cho từng method + Bị lỗi The final policy size (20588) is bigger than the limit (20480) 5/11/2025 5/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Tiếp tục hoàn thiện API Gateway: + Xóa Lambda DynamoDB_API_Handler cũ để xóa toàn bộ policies cũ và tạo DynamoDB_API_Handler mới với code được tinh chỉnh + Tạo proxy Resource và Method mới + Gắn Lambda + Bật CORS + Deploy API + Kiểm thử API bằng Postman 6/11/2025 6/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Sau khi hoàn thành phần API Gateway: + Bắt đầu làm workshop framework 7/11/2025 7/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 9: Ôn tập và củng cố kiến thức:\nTiếp tục review lại các đề thi để ôn lại kiến thức Củng cố các khái niệm AWS cơ bản sau kỳ thi Xác định các điểm yếu cần cải thiện Chuẩn bị kiến thức cho dự án thực tế Bắt đầu triển khai dự án:\nChuẩn bị và lên kế hoạch cho dự án cuối khóa Tìm hiểu kiến trúc serverless và các thành phần Xác định các dịch vụ AWS cần sử dụng Thiết kế data flow và architecture Thiết lập Lambda Trigger cho S3:\nTạo và cấu hình S3 Bucket cho dự án Cấu hình Lambda Trigger để xử lý sự kiện S3 Upload dữ liệu CSV lên S3 Bucket Kiểm tra trigger hoạt động và xử lý dữ liệu Hiểu event-driven architecture patterns Xây dựng API Gateway và tích hợp:\nTạo và cấu hình IAM Role cho Lambda Viết và triển khai Lambda Function DynamoDB_API_Handler Truy cập và thiết lập dịch vụ API Gateway Tạo REST API và cấu hình endpoints Tạo Resources và Methods cho từng bảng trong DynamoDB Gắn Lambda function cho từng HTTP method Giải quyết vấn đề và tối ưu hóa:\nGặp và giải quyết lỗi policy size limit (20588 \u0026gt; 20480) Xóa Lambda cũ và tạo DynamoDB_API_Handler mới Tối ưu hóa code và tinh chỉnh policies Tạo proxy Resource và Method mới Gắn Lambda và bật CORS cho API Deploy API và kiểm thử bằng Postman Hoàn thiện API và chuẩn bị workshop:\nHoàn thành phần API Gateway và kiểm tra tính năng Bắt đầu làm workshop framework cho dự án Chuẩn bị tài liệu và hướng dẫn Lên kế hoạch cho các bước tiếp theo Đánh giá tiến độ và chất lượng dự án "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/2-backend/2.1/","title":"Chuẩn bị &amp; Cấu hình Lambda Trigger cho S3","tags":[],"description":"","content":"Chuẩn bị \u0026amp; Cấu hình Lambda Trigger cho S3 Các bước thực hiện 1. Truy cập dịch vụ IAM Vào AWS Management Console → tìm IAM. Chọn Roles → Create Role. Chọn Trusted entity type: AWS service. Chọn Use case: Lambda, sau đó nhấn Next. 2. Gán quyền truy cập cho Role Gắn các policy sau:\nAmazonS3FullAccess AmazonDynamoDBFullAccess_v2 Nhấn Next, đặt tên role là LambdaS3DynamoDBRole.\nRole này giúp Lambda có thể đọc file từ S3 và ghi dữ liệu vào DynamoDB.\nTạo S3 Bucket Truy cập dịch vụ S3. Trong giao diện S3, chọn Create bucket. Trong màn hình Create bucket:\nBucket name: Nhập tên, ví dụ:\nflyora-bucket-database (Nếu tên đã tồn tại, hãy thêm số phía sau.)\nGiữ nguyên các thiết lập mặc định còn lại.\nXem lại cấu hình và chọn Create bucket để hoàn tất. Kết quả mong đợi Bucket flyora-bucket (hoặc tên bạn đã đặt) được tạo thành công. Role LambdaS3DynamoDBRole sẵn sàng để gán cho Lambda trong bước kế tiếp. Cấu hình Lambda Trigger cho S3 Trong bước này, bạn sẽ cấu hình AWS Lambda để tự động import file CSV vào DynamoDB mỗi khi có file mới trong S3 Bucket.\nTạo Lambda Function Truy cập Lambda → Create function. Chọn Author from scratch. Đặt tên: AutoImportCSVtoDynamoDB. Runtime: Python 3.13. Role: chọn LambdaS3DynamoDBRole đã tạo ở bước trước. Thêm Trigger Trong tab Configuration → Triggers, nhấn Add trigger. Chọn S3. Chọn Bucket flyora-bucket. Event type: All object create events. Nhấn Add để lưu. Dán code Lambda Dán đoạn code dưới đây: import boto3 import csv import io import json from botocore.exceptions import ClientError from decimal import Decimal dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) s3 = boto3.client(\u0026#39;s3\u0026#39;) # ------------------------- # Hàm kiểm tra kiểu dữ liệu của mẫu (Detect Type) # ------------------------- def detect_type(value): val_str = str(value).strip() # Check Int/Float try: float(val_str) return \u0026#39;N\u0026#39; # Number except ValueError: pass return \u0026#39;S\u0026#39; # String # ------------------------- # Hàm chuyển đổi dữ liệu (Convert) # ------------------------- def convert_value(value): if value is None: return None val_str = str(value).strip() if val_str == \u0026#34;\u0026#34;: return None # Int check try: if float(val_str).is_integer(): return int(float(val_str)) except ValueError: pass # Decimal check (cho Float) try: return Decimal(val_str) except Exception: pass # Boolean if val_str.lower() == \u0026#34;true\u0026#34;: return True if val_str.lower() == \u0026#34;false\u0026#34;: return False return val_str # ------------------------- # Tạo bảng Dynamic dựa trên kiểu dữ liệu phát hiện được # ------------------------- def create_table_if_not_exists(table_name, pk_name, pk_type): existing_tables = dynamodb.meta.client.list_tables()[\u0026#39;TableNames\u0026#39;] if table_name in existing_tables: print(f\u0026#34;Table \u0026#39;{table_name}\u0026#39; already exists.\u0026#34;) return print(f\u0026#34;Creating table: {table_name} | PK: {pk_name} | Type: {pk_type}\u0026#34;) table = dynamodb.create_table( TableName=table_name, KeySchema=[{\u0026#39;AttributeName\u0026#39;: pk_name, \u0026#39;KeyType\u0026#39;: \u0026#39;HASH\u0026#39;}], AttributeDefinitions=[{\u0026#39;AttributeName\u0026#39;: pk_name, \u0026#39;AttributeType\u0026#39;: pk_type}], BillingMode=\u0026#39;PAY_PER_REQUEST\u0026#39; ) table.wait_until_exists() print(\u0026#34;Table created successfully.\u0026#34;) # ------------------------- # Main Handler # ------------------------- def lambda_handler(event, context): try: for record in event[\u0026#39;Records\u0026#39;]: bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] print(f\u0026#34;Processing: {key}\u0026#34;) response = s3.get_object(Bucket=bucket, Key=key) # 1. QUAN TRỌNG: Dùng \u0026#39;utf-8-sig\u0026#39; để xóa BOM, giúp nhận diện số chính xác body = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8-sig\u0026#39;) reader = csv.DictReader(io.StringIO(body)) # Clean headers reader.fieldnames = [name.strip() for name in reader.fieldnames] items = list(reader) if not items: continue # Lấy thông tin Partition Key (PK) pk_name = reader.fieldnames[0] table_name = key.split(\u0026#39;.\u0026#39;)[0] # 2. QUAN TRỌNG: Phát hiện kiểu dữ liệu dựa trên dòng đầu tiên first_pk_val = items[0].get(pk_name) pk_type = detect_type(first_pk_val) # Sẽ trả về \u0026#39;N\u0026#39; nếu là số, \u0026#39;S\u0026#39; nếu là chữ # Tạo bảng đúng kiểu (N hoặc S) create_table_if_not_exists(table_name, pk_name, pk_type) table = dynamodb.Table(table_name) count = 0 with table.batch_writer() as batch: for row in items: clean_item = {} is_valid = True for k, v in row.items(): if not k or k.strip() == \u0026#34;\u0026#34;: continue clean_k = k.strip() val = convert_value(v) # Chuyển đổi sang Int/Decimal/Bool if val is None: continue # 3. QUAN TRỌNG: Xử lý Partition Key theo đúng kiểu của Bảng if clean_k == pk_name: if pk_type == \u0026#39;N\u0026#39;: # Nếu bảng là Number, bắt buộc Key phải là Number if not isinstance(val, (int, Decimal)): print(f\u0026#34;SKIPPING ROW: Key \u0026#39;{val}\u0026#39; is not a number but table requires Number.\u0026#34;) is_valid = False break else: # Nếu bảng là String, ép kiểu sang String val = str(val) clean_item[clean_k] = val if is_valid and pk_name in clean_item: batch.put_item(Item=clean_item) count += 1 print(f\u0026#34;Success: Imported {count} items into {table_name} (PK Type: {pk_type})\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;OK\u0026#39;)} except Exception as e: print(f\u0026#34;ERROR: {str(e)}\u0026#34;) import traceback traceback.print_exc() return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(str(e))} Nhấn Deploy và xác nhận Successfully. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Báo cáo sự kiện AWS Cloud Mastery Series #2 – DevOps on AWS Mục đích của sự kiện Xây dựng tư duy DevOps hiện đại và hiểu thêm văn hoá DevOps. Học và thực hành quy trình CI/CD triển khai trên AWS. Hướng dẫn sử dụng hạ tầng dưới dạng mã (Infrastructure as Code). Nắm được và Nâng cao khả năng quan sát hệ thống thông qua các kỹ thuật Monitoring \u0026amp; Observability trên AWS, gồm CloudWatch và AWS X-Ray. Tham gia workshop thực hành với các chuyên gia trong cộng đồng AWS Việt Nam. Danh sách diễn giả \u0026amp; chủ đề chia sẻ DevOps Mindset Truong Quang Tinh – AWS Community Builder, Platform Engineer – TymeX CI/CD Pipeline Văn Hoàng Kha – Chia sẻ về quy trình CI/CD và mô hình Pipeline Workflow (người trình bày nội dung của slide trong ảnh). DevOps on AWS – Workshop Bao Huynh – AWS Community Builder Thinh Nguyen – AWS Community Builder Vi Tran – AWS Community Builder Monitoring \u0026amp; Observability Long Huynh – AWS Community Builder Quy Pham – AWS Community Builder Nghiem Le – AWS Community Builder Nội dung nổi bật 1. DevOps Mindset Tư duy “collaboration-first”: tăng cường phối hợp giữa Development – Operations – QA Security để giảm silo và cải thiện chất lượng release.\nTập trung tự động hóa toàn vòng đời: build, test, deploy, monitoring nhằm giảm lỗi thủ công và tăng tốc độ triển khai.\n2. CI/CD Pipeline Anh Kha trình bày Pipeline Workflow trực quan trên Lucidchart, mô tả đầy đủ các bước từ commit đến production: GitHub Version Control với nhánh chuẩn và quy trình commit rõ ràng. Peer Review \u0026amp; Pull Request đảm bảo chất lượng code và tuân thủ coding standards. Automated Test \u0026amp; QA gồm unit test, integration test, security scan chạy tự động mỗi lần push. Multi-stage Deployment (Dev → QA → Pre-Prod → Prod) với kiểm soát approval và rollback rõ ràng. Quy trình này giúp tôi hình dung chính xác pipeline CI/CD chuẩn đang được áp dụng trong các dự án. 3. Infrastructure as Code (IaC) Các kien thức IaC : AWS CloudFormation: Template-based provisioning, quản lý stacks. Hỗ trợ drift detection giúp phát hiện thay đổi ngoài IaC. AWS CDK (Cloud Development Kit): IaC bằng lập trình. Reusable constructs, patterns, library hỗ trợ tốt cho microservices. Demo: Triển khai hạ tầng bằng CloudFormation và CDK để so sánh. Discussion: Khi nào dùng CloudFormation và khi nào dùng CDK 4. Monitoring \u0026amp; Observability Ví dụ thực tế với Amazon CloudWatchgiúp tôi thấy rõ cách xây dựng quan sát hệ thống: Logs tập trung, Metrics tùy chỉnh, Dashboards trực quan và Alarms phản ứng theo thời gian thực. Minh họa phân tích lỗi và tracing bằng AWS X-Ray cho thấy cách theo dõi toàn bộ dòng chảy request : Service map để nhận diện các điểm phụ thuộc giữa microservices. Bottleneck analysis để tìm ra thành phần gây chậm hoặc lỗi. Request flow visualization giúp truy vết chính xác nguyên nhân sự cố. Nhờ các demo này, tôi hiểu rõ giá trị của observability trong vận hành hệ thống hiện đại thay vì chỉ dừng lại ở việc xem log thủ công như trước đây. Trải nghiệm và bài học cá nhân Pipeline Workflow của anh Kha: Giúp tôi hiểu rõ cách một CI/CD pipeline hoạt động ngoài đời thực — từ commit, review, test đến deploy — và cách áp dụng vào quy trình dự án của team. Workshop DevOps on AWS: Giúp tôi kết nối lý thuyết với thực hành thông qua việc trực tiếp quan sát CodePipeline, CodeBuild, CodeDeploy hoạt động liền mạch trong một quy trình tự động hóa hoàn chỉnh. CloudWatch \u0026amp; X-Ray: Lần đầu tôi thấy rõ cách theo dõi hiệu năng end-to-end, xác định bottleneck và tìm root cause của sự cố mà không cần dò log thủ công. Điều này thay đổi hoàn toàn cách tôi tiếp cận troubleshooting. Tư duy DevOps: Tôi học được cách tối ưu hóa quy trình làm việc bằng tự động hóa, tăng tính cộng tác trong team và rút ngắn feedback-loop để nâng cao chất lượng sản phẩm. Kết quả đạt được Hiểu thêm DevOps mindset và nắm được bức tranh tổng thể của quy trình CI/CD, từ kiểm soát mã nguồn đến triển khai đa môi trường. Có thể tự thiết kế và xây dựng một pipeline chuẩn, dựa trên mô hình đã được minh họa trong workshop, để áp dụng trực tiếp vào dự án nhóm. Sử dụng thành thạo CloudWatch và X-Ray để theo dõi hệ thống, phân tích lỗi, xác định bottleneck và tối ưu hiệu năng dịch vụ. Hình ảnh tại sự kiện "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Báo cáo sự kiện chuyên đề về Edge Network Services Mục đích của sự kiện Workshop được tổ chức nhằm giúp người tham dự có cái nhìn toàn diện về cách tối ưu và bảo vệ hệ thống web thông qua các dịch vụ biên (edge services) của AWS. Các mục tiêu chính: Hiểu thêm thách thức khi vận hành ứng dụng Internet ở quy mô lớn. Giới thiệu cách Amazon CloudFront giải quyết vấn đề về hiệu năng, chi phí, và bảo mật. Tìm hiểu các tính năng cốt lõi của CloudFront và cơ chế hoạt động của nó. Giới thiệu các dịch vụ bao mat như AWS WAF, AWS Shield, Bot Control. Thực hành hands-on tối ưu web app và bảo vệ ứng dụng Internet. Danh sách diễn giả \u0026amp; chủ đề chia sẻ FROM EDGE TO ORIGIN: CloudFront as Your Foundation Nguyen Gia Hung – Head of Solutions Architect Amazon Web Services Viet Nam Defense from public threat: AWS WAF \u0026amp; Application Protection Julian Lu –Sr. Edge Specialist Solutions Architect at Amazon Web Services (AWS) Nội dung nổi bật 1. Thách thức khi vận hành ứng dụng Internet Workshop mở đầu bằng việc phân tích các vấn đề phổ biến mà doanh nghiệp thường gặp:\nĐộ trễ cao (latency) do người dùng ở xa origin server. Tải lớn lên hệ thống backend dẫn đến chi phí tăng cao. Tấn công từ internet như DDoS, bot traffic, injection, scraping. Khó mở rộng (scalability) khi lượng truy cập tăng đột biến. Thiếu lớp phòng thủ biên, khiến origin dễ bị lộ và tấn công trực tiếp. 2. CloudFront – Giải pháp nền tảng “From Edge to Origin” Diễn giả trình bày cách CloudFront giúp cải thiện hệ thống: Caching để giảm tải cho origin. Global Edge Network giúp người dùng truy cập từ vị trí gần nhất → tăng performance. Security at Edge với WAF, Shield, Bot Control tại biên. Cost Optimization nhờ giảm request trực tiếp về origin. Resiliency \u0026amp; Availability do phân tán toàn cầu và cơ chế failover. 3. CloudFront hoạt động như thế nào Một mô hình kiến trúc đơn giản được trình bày: Người dùng gửi request → đi đến Edge Location gần nhất. CloudFront kiểm tra cached content. Nếu không có → request được chuyển về origin. Edge cache nội dung để phục vụ các request tiếp theo.\n=\u0026gt; Quy trình này giúp vừa tăng tốc độ vừa giảm chi phí và tăng độ bảo vệ. 4. Bảo vệ ứng dụng khỏi các mối đe dọa AWS WAF – Web Application Firewall: Lọc và chặn các kiểu tấn công phổ biến: SQL injection, XSS, unauthorized access… Có thể cấu hình rules, rule groups, managed rules. Hoạt động ngay trên CloudFront → bảo vệ ở lớp biên, giảm gánh nặng cho origin. AWS WAF Bot Control: Nhận diện bot, phân biệt bot tốt vs bot xấu. Chặn scraping, brute force, bad automated traffic. Giảm tải và tăng độ an toàn cho hệ thống. AWS Shield: Bảo vệ chống DDoS ở nhiều cấp độ. Shield Standard (miễn phí) luôn bật cho CloudFront. Shield Advanced hỗ trợ monitor, incident response, và chi phí bảo vệ tối ưu. Trải nghiệm và bài học cá nhân Học hỏi từ chuyên gia AWS: -Diễn giả chia sẻ những khó khắn, thách thức trong triển khai hệ thống lớn và quản trị traffic. Các kiến thức về caching, bảo mật lớp biên, và tối ưu hiệu suất được giải thích rất dễ hiểu. Trải nghiệm kỹ thuật: Phân tích sơ đồ kiến trúc thật và mô phỏng quá trình xử lý request của CloudFront. Tìm hiểu cách xây dựng WAF rules và cơ chế Bot Control. Hiểu rõ cách CloudFront kết hợp với Shield để ngăn ngừa DDoS. Hands-on Workshop: Optimize Internet Web Application: Cấu hình CloudFront, thiết lập caching policies, kiểm tra latency improvements. Secure Internet Web Application: Tạo và áp dụng WAF rules. Quan sát bot traffic và tạo rule chặn. Tích hợp CloudFront + WAF + Shield. Kết quả đạt được Hiểu thêm vai trò quan trọng của edge layer trong kiến trúc ứng dụng Internet hiện đại — không chỉ để tăng tốc độ phân phối nội dung mà còn để tạo thành một lớp phòng thủ hiệu quả. Có thể giải thích và triển khai mô hình kiến trúc trong đó CloudFront đóng vai trò entry point cho mọi web application, giúp tối ưu performance và tăng mức độ an toàn. Hiểu thêm cách AWS Shield hoạt động và áp dụng hiệu quả để bảo vệ ứng dụng khỏi các loại tấn công DDoS mà không cần cấu hình phức tạp. Nắm được cơ chế phối hợp giữa CloudFront – WAF – Shield để tạo thành một lớp phòng thủ toàn diện, resilient và tiết kiệm chi phí. Hình ảnh tại sự kiện "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Báo cáo sự kiện AWS Cloud Mastery Series #3 - AWS Well-Architected Security Pillar Mục đích của sự kiện Workshop được tổ chức nhằm giúp người tham dự hiểu sâu về Security Pillar trong AWS Well-Architected Framework và cách xây dựng hệ thống cloud an toàn, kiểm soát được truy cập, giám sát liên tục, bảo vệ dữ liệu và phản ứng sự cố hiệu quả. Các mục tiêu chính: Hiểu thêm vai trò của Security trong kiến trúc cloud hiện đại. Nắm các nguyên tắc cốt lõi như Least Privilege, Zero Trust, Defense in Depth. Hiểu Shared Responsibility Model và top threats trong môi trường cloud tại Việt Nam. Tìm hiểu 5 Pillars quan trọng: IAM, Detection, Infrastructure Protection, Data Protection, Incident Response. Thực hành phân tích policy, xây dựng guardrails, ghi log, giám sát và mô phỏng tình huống sự cố. Danh sách diễn giả \u0026amp; chủ đề chia sẻ Mendel Grabski (Long) – ex Head of Security \u0026amp; DevOps, Cloud Security Solutions Architect Truong Quang Tinh – AWS Community Builder, Platform Engineer tại TymeX Thịnh Lâm – FCJ Cloud Engineer Việt Nguyễn – FCJ Cloud Engineer Danh Hoàng Hiếu Nghị – FCJ Cloud Engineer Nội dung nổi bật 1. Security Foundation \u0026amp; Thách thức trong môi trường cloud Workshop mở đầu với các vấn đề phổ biến mà môi trường cloud tại Việt Nam gặp phải:\nQuản lý IAM chưa đúng, cấp quá nhiều quyền (over-permission). Rò rỉ long-term credentials, key không được rotate. Diễn giả nhấn mạnh 3 nguyên tắc cốt lõi: Least Privilege – chỉ cấp quyền tối thiểu. Zero Trust – không tin cậy mặc định, luôn xác minh. Defense in Depth – bảo vệ theo nhiều lớp từ IAM → Network → Workload → Data. 2. Identity \u0026amp; Access Management (Pillar 1) Nội dung IAM tập trung vào mô hình kiến trúc hiện đại: Giảm tối đa việc tạo IAM Users, thay bằng IAM Roles và Identity Center (SSO). Permission Sets để quản lý quyền tập trung cho nhiều account. Service Control Policies (SCP) để đặt giới hạn quyền cấp độ Organization. Permission Boundaries đảm bảo quyền developers không vượt mức cho phép. MFA, credential rotation, Access Analyzer để giảm rủi ro lộ thông tin truy cập. 3. Detection \u0026amp; Continuous Monitoring (Pillar 2) CloudTrail org-level: ghi lại mọi API call trong toàn bộ Organization. GuardDuty: phát hiện bất thường như credential lộ, port scanning, malicious traffic. Security Hub: gom toàn bộ cảnh báo từ GuardDuty, IAM Analyzer, S3… Logging ở mọi lớp: VPC Flow Logs ALB \u0026amp; S3 Access Logs EventBridge + Lambda để tự động cảnh báo hoặc khóa tài khoản bị xâm phạm. Khái niệm Detection-as-Code giúp dễ dàng quản lý qua Git. 4. Infrastructure Protection (Pillar 3) Thiết kế VPC hợp lý: private subnet cho workloads, public subnet chỉ cho những gì thật sự cần thiết. Security Groups hướng inbound whitelist. NACLs cho subnet-level blocking. Edge security: AWS WAF AWS Shield (DDoS Protection) AWS Network Firewall 5. Data Protection (Pillar 4) KMS quản lý encryption keys: rotation, key policy, grant. Secrets Manager và SSM Parameter Store để quản lý secrets an toàn. Data classification giúp quyết định chính sách bảo vệ phù hợp 6. Incident Response (Pillar 5) Các bước IR lifecycle theo AWS:\nPreparation Detection \u0026amp; Analysis Containment Eradication Recovery Post-Incident Review Playbook được mô phỏng:\nXử lý compromised IAM key. S3 bucket public exposure. EC2 malware detection và cách cô lập instance. Snapshot, thu thập evidence, tự động phản ứng bằng Lambda/Step Functions. Trải nghiệm và bài học cá nhân Học hỏi từ chuyên gia AWS:\nDiễn giả giải thích rõ cách xây dựng kiến trúc bảo mật theo chuẩn Well-Architected. Các khái niệm IAM nâng cao như SCP, Permission Boundaries, Identity Center rất trực quan. Trải nghiệm kỹ thuật:\nThực hành validate IAM Policy. Quan sát cảnh báo GuardDuty và phân tích log. Tìm hiểu cách thiết lập CloudTrail org-level và kích hoạt Security Hub. Mô phỏng sự cố để hiểu quy trình Incident Response trong thực tế. Kết quả đạt được Hiểu thêm vai trò của Security trong AWS Well-Architected và cách áp dụng 5 pillars vào hệ thống thực tế. Nắm được mô hình IAM hiện đại với Identity Center, SCP và Permission Boundaries. Được xem cách sử dụng các dịch vụ phát hiện sự cố như GuardDuty, CloudTrail và Security Hub. Có hiểu biết thực tế về quy trình Incident Response và cách automation bằng Lambda/Step Functions. Hình ảnh tại sự kiện "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"Báo cáo sự kiện BUILDING AGENTIC AI: Context Optimization with Amazon Bedrock Mục đích của sự kiện Workshop tập trung vào Agentic AI và xây dựng AI agents trên Amazon Bedrock, bao gồm:\nHiểu khái niệm Agentic AI và Amazon Bedrock Agent Học kỹ thuật context optimization và agentic workflows Thực hành với CloudThinker orchestration framework Danh sách diễn giả \u0026amp; chủ đề chia sẻ Kien Nguyen – Solutions Architect\nChủ đề: AWS Bedrock Agent Core\nViet Pham – Founder cum CEO (Diaflow)\nChủ đề: [Use Case] Building Agentic Workflow on AWS\nThang Ton – Co-founder \u0026amp; COO (CloudThinker)\nChủ đề: CloudThinker Introduction\nHenry Bui – Head of Engineering (CloudThinker)\nChủ đề: CloudThinker Agentic Orchestration, Context Optimization on Amazon Bedrock (L300)\nKha Van – CloudThinker Engineer\nChủ đề: CloudThinker Hack: Hands-on Workshop\nNội dung chính 1. AWS Bedrock Agent Core (Kien Nguyen) Kiến trúc và tính năng cốt lõi của Amazon Bedrock Foundation Models, Agent architecture, Action groups Multi-step reasoning, Tool integration, Memory management 2. Building Agentic Workflow - Use Case Diaflow (Viet Pham) Case study thực tế từ Diaflow Thiết kế workflow với nhiều agents, tích hợp AWS services Giải quyết context switching, consistency, cost optimization 3. CloudThinker Platform (Thang Ton) Nền tảng AI orchestration đơn giản hóa việc xây dựng AI agents Agent orchestration, Context optimization, Multi-model support 4. Context Optimization Technical Deep-dive (Henry Bui) Orchestration Framework: Agent coordination, state management Context Optimization: Token usage, semantic chunking, dynamic injection Performance: Prompt engineering, model selection, monitoring Tích hợp Bedrock APIs, RAG, Guardrails 5. Hands-on Workshop (Kha Van) Thiết lập và kết nối CloudThinker với AWS Thực hành cost optimization và phân tích chi phí Trải nghiệm và bài học Hiểu thêm Amazon Bedrock và các kỹ thuật context optimization CloudThinker framework đơn giản hóa việc xây dựng agents Thực hành tối ưu chi phí với CloudThinker Case study Diaflow cho thấy challenges thực tế khi deploy agentic AI Context management và cost optimization quan trọng khi scale AI Kết quả đạt được Hiểu thêm Agentic AI và Amazon Bedrock Agent framework Học các kỹ thuật context optimization và agentic workflows Thực hành hands-on với CloudThinker và AWS integration Hiểu cost optimization và challenges thực tế từ use case Diaflow Hình ảnh tại sự kiện "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/1-introduction/","title":"Giới thiệu","tags":[],"description":"","content":"Giới thiệu Workshop Flyora là gì? Flyora là nền tảng thương mại điện tử hiện đại được thiết kế để minh họa kiến trúc cloud-native sử dụng các dịch vụ serverless của AWS. Workshop này hướng dẫn bạn xây dựng một cửa hàng trực tuyến hoàn chỉnh với tính năng duyệt sản phẩm, chatbot hỗ trợ AI, và quản lý dữ liệu tự động.\nMục tiêu Workshop Sau khi hoàn thành workshop, bạn sẽ:\nTriển khai hệ thống thương mại điện tử serverless trên AWS Xây dựng pipeline dữ liệu tự động sử dụng S3 và Lambda triggers Phát triển RESTful APIs với API Gateway và Lambda Tạo cơ sở dữ liệu có khả năng mở rộng bằng DynamoDB Tích hợp chatbot AI cho hỗ trợ khách hàng Triển khai frontend tĩnh với S3 và CloudFront Thiết lập CI/CD pipeline cho triển khai tự động Tổng quan Kiến trúc Nền tảng Flyora sử dụng kiến trúc serverless hoàn toàn:\nTầng Frontend:\nWebsite tĩnh được host trên Amazon S3 Phân phối nội dung toàn cầu qua Amazon CloudFront Giao diện responsive cho duyệt và mua sắm sản phẩm Tầng Backend:\nAPI Gateway cho các RESTful API endpoints AWS Lambda functions xử lý business logic Amazon DynamoDB lưu trữ dữ liệu sản phẩm và đơn hàng Amazon S3 cho import và lưu trữ dữ liệu Tầng AI:\nChatbot hỗ trợ AI cho gợi ý sản phẩm Tích hợp vào giao diện frontend Hỗ trợ khách hàng theo thời gian thực Bảo mật \u0026amp; Xác thực:\nIAM roles cho truy cập dịch vụ an toàn Cấu trúc Workshop Workshop được tổ chức theo các module theo nhóm:\nBackend Team - Phát triển API và data pipeline AI Team - Tích hợp chatbot và tính năng AI Frontend Team - Phát triển và triển khai UI CI/CD - Pipeline triển khai tự động Testing - Kiểm thử hệ thống và hiệu năng Cleanup - Quản lý tài nguyên và tối ưu chi phí Kết quả Mong đợi Sau khi hoàn thành workshop, bạn sẽ có:\nWebsite thương mại điện tử hoạt động hoàn chỉnh trên AWS Kinh nghiệm thực tế với kiến trúc serverless Hiểu biết về AWS best practices cho khả năng mở rộng và bảo mật Kiến thức triển khai CI/CD cho ứng dụng cloud Dự án portfolio thể hiện kỹ năng cloud engineering Chi phí Workshop này được thiết kế để chạy trong AWS Free Tier. Tất cả dịch vụ sử dụng đều có tùy chọn free tier, và kiến trúc tránh các tài nguyên tốn kém như EC2 instances. Chi phí ước tính: $0-5 USD nếu hoàn thành trong vài giờ.\nYêu cầu Trước khi Bắt đầu Trước khi bắt đầu, đảm bảo bạn có:\nTài khoản AWS với quyền quản trị Hiểu biết cơ bản về cloud computing Quen thuộc với REST APIs và JSON Kiến thức cơ bản về HTML/CSS/JavaScript (cho frontend) Git đã cài đặt trên máy local "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/4-frontend/4.1/","title":"Hosting website với S3","tags":[],"description":"","content":" Bước 1: Tạo một S3 Bucket Truy cập vào dịch vụ S3. Nhấn Create bucket. Nhập một Tên Bucket duy nhất (ví dụ: flyora-shop).\nBỏ chọn “Block all public access” Xác nhận cảnh báo về quyền truy cập công khai. Nhấn Create bucket. Bước 2: Tải lên các tệp website Mở bucket vừa tạo. Nhấn Upload → Add files → chọn các tệp website của bạn (ví dụ: index.html) Nhấn Upload. Bước 3: Bật tính năng Static Website Hosting Chuyển đến tab Properties của bucket. Kéo xuống Static website hosting. Nhấn Edit → Bật Static website hosting Nhập: Index document: index.html Error document: index.html (tùy chọn) Nhấn Save changes. Chuyển sang tab Permission của bucket. Chỉnh sửa Bucket Policy Dán JSON policy sau (thay flyora-shop bằng tên bucket của bạn): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34; } ] } Bước 4: Kiểm tra Website Nhấn vào Bucket website endpoint URL http://your-bucket-name.s3-website-ap-southeast-1.amazonaws.com Đảm bảo website hiển thị như hình dưới: "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/3-ai/3.1/","title":"Tạo VPC  &amp; Cấu hình Sercurity Group cho RDS và Lambda","tags":[],"description":"","content":"Tạo VPC \u0026amp; Cấu hình Sercurity Group cho RDS và Lambda Các bước thực hiện 1. Truy cập dịch vụ VPC Vào AWS Management Console → tìm VPC.\nChọn Your VPCs → Create VPC. Chọn VPC and more và đặt tên cho project Ở phần cấu hình cho VPC Number of Availability Zones chọn 1 Number of public subnets chọn 1 và Number of private subnets chọn 2 và NAT gateways chọn Zonal NAT gateways chọn In 1 Az VPC endpoints chọn None → Create VPC\nQuá trình này sẽ diễn ra vài phút để có thể tạo NAT gateway.\nSau khi tạo xong chúng ta có thể coi cách Vpc and more tạo ra các tài nguyên thông qua Resource map Chúng ta sẽ tạo thêm một private subnet đặt ở một AZ khác để có thể tạo RDS được 2. Thiết lập Security Groups Chúng ta sẽ tạo 2 Security Group (SG) riêng biệt để bảo mật tối đa.\nVào phần Security Groups bấm Create security group chúng ta sẽ tạo 2 security group cho Lambda và RDS\nĐặt tên cho security group và chọn VPC chúng ta đã tạo ở bước 1 sau đó bấm create Tiếp tục tạo SG cho RDS chọn VPC ở bước 1 ở phần inbound rules ở phần type chọn PostgreSQL ở phần Source chọn SG đã tạo cho Lambda "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/2-proposal/","title":"Bản đề xuất","tags":[],"description":"","content":"🐦 Đề xuất: Flyora – Nền tảng Thương mại Điện tử cho Người yêu Chim 📄 Tải Xuống Đề xuất PDF Đầy đủ\n1. Tóm tắt Điều hành Flyora là một ứng dụng web chuyên biệt được thiết kế để phục vụ những người đam mê chim cảnh trên khắp Việt Nam. Nền tảng cung cấp các sản phẩm được tuyển chọn như thức ăn chim, đồ chơi, lồng và phụ kiện trang trí phù hợp với các loài như Chào Mào, Vẹt, Yến Phụng và Chích Chòe. Được xây dựng với công nghệ web hiện đại và lưu trữ trên AWS, Flyora đảm bảo khả năng mở rộng, hiệu suất và truy cập an toàn. Nền tảng hướng tới trở thành điểm đến hàng đầu cho việc chăm sóc và trang trí chim, kết hợp thương mại điện tử với cá nhân hóa và tương tác cộng đồng.\n2. Phát biểu Vấn đề Thách thức Hiện tại:\nKhông có nền tảng tập trung cho các sản phẩm chuyên biệt về chim Các cửa hàng thú cưng chung thiếu khuyến nghị theo từng loài Giao diện người dùng lỗi thời và khả năng đáp ứng di động kém Khả năng mở rộng backend và tìm kiếm hạn chế Giải pháp Đề xuất: Flyora mang đến trải nghiệm mua sắm theo danh mục đáp ứng với xác thực người dùng an toàn, lọc sản phẩm thời gian thực và backend có thể mở rộng. Hỗ trợ cả người dùng desktop và di động, với kế hoạch tương lai cho khuyến nghị được hỗ trợ AI và chatbot.\n3. Kiến trúc Giải pháp 📄 Sơ đồ Kiến trúc Hệ thống 🧩 Frontend (Tầng Web) Amazon S3: Lưu trữ web tĩnh cho tài sản frontend CloudFront: CDN cho phân phối nội dung toàn cầu Thiết kế đáp ứng: Giao diện thân thiện với di động 🔐 Xác thực \u0026amp; Bảo mật IAM: Quản lý danh tính và truy cập CloudWatch \u0026amp; AWS X-Ray: Giám sát và theo dõi phân tán 🔄 Dịch vụ Backend (Tầng Ứng dụng) Amazon API Gateway: Quản lý API RESTful AWS Lambda Functions: Xử lý chatbot Xử lý API Tự động hóa nhập Amazon Bedrock: Mô hình nhúng và LLM cho các tính năng AI 📦 Dữ liệu \u0026amp; Lưu trữ (Tầng Dữ liệu) Amazon RDS for PostgreSQL: Cơ sở dữ liệu quan hệ DynamoDB: Cơ sở dữ liệu NoSQL Amazon S3: Lưu trữ dữ liệu 🔧 CI/CD \u0026amp; Phát triển GitLab: Kiểm soát phiên bản và kích hoạt pipeline CI/CD AWS CodeBuild: Quy trình xây dựng tự động AWS CodePipeline: Tích hợp và triển khai liên tục 4. Triển khai Kỹ thuật Các Giai đoạn: Học AWS \u0026amp; Thiết lập – Thành thạo các dịch vụ AWS và thiết kế kiến trúc Phát triển \u0026amp; Tích hợp – Xây dựng frontend và kết nối backend AWS Kiểm thử \u0026amp; Triển khai – Hoàn thành kiểm thử và phát hành sản xuất Tháng 1 - Tập trung Học AWS: Tuần 1-2: Cơ bản AWS (S3, Lambda, API Gateway, DynamoDB) Tuần 3: Dịch vụ nâng cao (Bedrock, OpenSearch) Tuần 4: Thiết kế kiến trúc và mô hình hóa cơ sở dữ liệu với MySQL Workbench Yêu cầu Kỹ thuật: Thành thạo dịch vụ AWS cho kiến trúc serverless Phát triển frontend với lưu trữ tĩnh S3 DynamoDB cho quản lý dữ liệu NoSQL GitHub cho kiểm soát phiên bản và tích hợp CI/CD 5. Lịch trình \u0026amp; Cột mốc Giai đoạn Thời gian Cột mốc Chính Tháng 1: Học AWS 4 tuần • Thành thạo cơ bản AWS\n• Thiết kế kiến trúc\n• Tạo lược đồ cơ sở dữ liệu Tháng 2: Phát triển 4 tuần • Hoàn thành UI frontend\n• Xây dựng hàm Lambda\n• Cấu hình API Gateway Tháng 3: Tích hợp 4 tuần • Tích hợp hệ thống đầy đủ\n• Hoàn thành kiểm thử\n• Triển khai sản xuất 6. Ước tính Chi phí Mục Chi phí Hàng tháng Chi phí Hàng năm Amazon S3 (Dịch vụ Lưu trữ Đơn giản) $0.15 $1.80 AWS Lambda (Tính toán Serverless) $0.00 $0.00 Amazon API Gateway (REST API Endpoints) $0.04 $0.48 DynamoDB (Cơ sở dữ liệu NoSQL On-demand) $0.00 $0.00 AWS X-Ray (Giám sát Ứng dụng) $0.01 $0.12 Amazon CloudWatch (Giám sát \u0026amp; Nhật ký) $0.00 $0.00 Amazon Bedrock (Dịch vụ AI/LLM) $3.49 $41.88 Amazon RDS for PostgreSQL (Cơ sở dữ liệu Quan hệ) $21.01 $252.12 AWS Data Transfer (Lưu lượng Mạng) $0.00 $0.00 Amazon CloudFront (Dịch vụ CDN) $0.10 $1.20 AWS CodePipeline (Tự động hóa CI/CD) $0.00 $0.00 AWS CodeBuild (Dịch vụ Build) $2.52 $30.24 Amazon VPC (Mạng riêng ảo) $43.07 $516.84 Tổng Ước tính $70.39 $844.68 Lưu ý: Chi phí phần cứng không áp dụng do Flyora là nền tảng web hoàn toàn.\n7. Đánh giá Rủi ro Rủi ro Tác động Xác suất Chiến lược Giảm thiểu Lambda cold starts Trung bình Trung bình Đồng thời được cung cấp cho các hàm quan trọng DynamoDB throttling Trung bình Thấp Tự động mở rộng và thiết kế khóa phân vùng phù hợp RDS downtime Trung bình Thấp Triển khai Multi-AZ, sao lưu tự động Vượt chi phí Thấp Thấp Giám sát với AWS Budgets và cảnh báo CloudWatch Giới hạn API Bedrock Trung bình Thấp Giám sát sử dụng, dự phòng kết quả cache 8. Kết quả Mong đợi Cải tiến Kỹ thuật: Giao diện người dùng đáp ứng, thân thiện với di động Xác thực người dùng an toàn và quản lý vai trò (IAM) Backend có thể mở rộng với Lambda/API Gateway Lọc sản phẩm thời gian thực và hỗ trợ chatbot Tính năng AI qua Bedrock (Nhúng/LLM) Lưu trữ dữ liệu mạnh mẽ với RDS, DynamoDB, S3 Giá trị Kinh doanh: Nền tảng tập trung cho người yêu chim ở Việt Nam Giảm phụ thuộc vào các cửa hàng thú cưng chung Nền tảng cho các tính năng AI và mở rộng cộng đồng tương lai Tiềm năng cho ứng dụng di động và tích hợp chatbot "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.10-week10/","title":"Worklog Tuần 10","tags":[],"description":"","content":"Mục tiêu tuần 10: Tiếp tục phát triển và hoàn thiện dự án cuối khóa. Tích hợp API vào frontend và giải quyết các vấn đề CORS. Chuyển đổi từ Node.js sang Java cho Lambda functions. Giải quyết các lỗi kỹ thuật và tối ưu hóa deployment. Tham dự sự kiện AWS Cloud Mastery Series. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tiếp tục làm project + Tạo API Gateway và tích hợp Lambda + Tạo IAM Role + Tạo Lambda Function DynamoDB_API_Handler - Tạo API Gateway và tích hợp với Lambda + Truy cập dịch vụ API Gateway + Tạo REST API + Tạo Resource, tạo Method cho từng bảng trong DynamoDB + Gắn Lambda cho từng method + Bị lỗi The final policy size (20588) is bigger than the limit (20480) 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Tiếp tục làm project + Lên văn phòng làm project cùng thành viên trong nhóm + Tích hợp API vào frontend + Gặp lỗi CORS policy: No \u0026lsquo;Access-Control-Allow-Origin\u0026rsquo; + Sửa method OPTIONS để sửa lỗi + Sửa Lambda - Gặp lỗi the server responded with a status of 400 + Tiếp tục sửa code và logic trong Lambda 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tiếp tục làm project + Làm lại từ đầu code Java để deploy lên Lambda + Chỉnh sửa code nhiều lần để đưa code từ local lên Lambda + Build file JAR + Deploy lên Lambda 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Tiếp tục làm project + Gặp lỗi ClassNotFoundException + Đổi từ Spring Cloud Function sang aws-serverless-java-container-springboot2 + Thêm maven-shade-plugin + Thêm core và event + Đã sửa được lỗi 13/11/2025 13/11/2025 Giải quyết lỗi ClassNotFoundException 6 - Tham dự sự kiện AWS Cloud Mastery Series #1 14/11/2025 14/11/2025 AWS Cloud Mastery Series #1 Kết quả đạt được tuần 10: Tiếp tục phát triển API Gateway và Lambda:\nTiếp tục xây dựng API Gateway và tích hợp Lambda Tạo và cấu hình IAM Role cho Lambda functions Triển khai Lambda Function DynamoDB_API_Handler Truy cập và thiết lập dịch vụ API Gateway Tạo REST API và cấu hình endpoints Tạo Resources và Methods cho từng bảng trong DynamoDB Gắn Lambda functions cho từng HTTP method Gặp và giải quyết lỗi policy size limit Làm việc nhóm và tích hợp frontend:\nLên văn phòng làm project cùng thành viên trong nhóm Tích hợp API vào frontend application Gặp và giải quyết lỗi CORS policy: No \u0026lsquo;Access-Control-Allow-Origin\u0026rsquo; Sửa method OPTIONS để giải quyết lỗi CORS Chỉnh sửa Lambda function để hỗ trợ CORS Gặp lỗi \u0026ldquo;the server responded with a status of 400\u0026rdquo; Tiếp tục sửa code và logic trong Lambda Chuyển đổi sang Java và tối ưu hóa deployment:\nLàm lại từ đầu code Java để deploy lên Lambda Chỉnh sửa code nhiều lần để đưa code từ local lên Lambda Build file JAR cho Java application Deploy ứng dụng Java lên AWS Lambda Hiểu các thách thức khi chuyển đổi ngôn ngữ Giải quyết lỗi ClassNotFoundException:\nGặp lỗi ClassNotFoundException khi chạy Java trên Lambda Đổi từ Spring Cloud Function sang aws-serverless-java-container-springboot2 Thêm maven-shade-plugin vào Maven configuration Thêm AWS Lambda core và event dependencies Đã sửa được lỗi và Lambda hoạt động ổn định Học cách package Java applications cho AWS Lambda Tham dự sự kiện và học hỏi:\nTham dự sự kiện AWS Cloud Mastery Series #1 Tìm hiểu các xu hướng mới trong AWS cloud services Kết nối với cộng đồng AWS và chia sẻ kinh nghiệm Cập nhật kiến thức về best practices và new features Ứng dụng kiến thức học được vào dự án thực tế "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.11-week11/","title":"Worklog Tuần 11","tags":[],"description":"","content":"Mục tiêu tuần 11: Tham dự các sự kiện AWS Cloud Mastery Series và Edge Network Services. Tiếp tục phát triển và hoàn thiện dự án cuối khóa. Giải quyết các lỗi kỹ thuật trong Lambda functions. Tích hợp monitoring và logging với X-Ray và CloudWatch. Phối hợp với team database để kết nối dữ liệu. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 -Tham dự sự kiện AWS Cloud Mastery Series #2 17/11/2025 17/11/2025 AWS Cloud Mastery Series #2 3 - Tiếp tục làm project + Gặp lỗi Sandbox Timeout + Tăng timeout lên 1 phút + Đã giải quyết được lỗi và hiện ra dữ liệu 18/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tham dự sự kiện chuyên đề về Edge Network Services và làm hands-on workshop 19/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Tiếp tục làm project + Nghiên cứu và tìm cách tích hợp x-ray và cloudwatch log vào project 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Tiếp tục làm project + Phối hợp với bên database để kết nối dữ liệu + Gặp lỗi java.lang.IllegalArgumentException: Boolean string was not \u0026rsquo;true\u0026rsquo; or \u0026lsquo;false\u0026rsquo;: 1\u0026quot; + Sửa các boolean thành integer Đã hết lỗi 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ Kết quả đạt được tuần 11: Tham dự các sự kiện và học hỏi:\nTham dự sự kiện AWS Cloud Mastery Series #2 Tham dự sự kiện chuyên đề về Edge Network Services Làm hands-on workshop về Edge Network Services Cập nhật kiến thức về các dịch vụ AWS mới Kết nối và học hỏi từ cộng đồng AWS Giải quyết lỗi Sandbox Timeout:\nGặp lỗi Sandbox Timeout khi chạy Lambda functions Tăng timeout lên 1 phút để giải quyết vấn đề Đã giải quyết được lỗi và hiện thị dữ liệu thành công Học cách cấu hình timeout cho AWS Lambda Nghiên cứu tích hợp monitoring và logging:\nNghiên cứu cách tích hợp AWS X-Ray vào project Tìm hiểu cách sử dụng CloudWatch Logs Lên kế hoạch implement distributed tracing Hiểu được tầm quan trọng của monitoring trong serverless architecture Phối hợp team và giải quyết lỗi dữ liệu:\nPhối hợp với bên database để kết nối dữ liệu Gặp lỗi java.lang.IllegalArgumentException: Boolean string was not \u0026rsquo;true\u0026rsquo; or \u0026lsquo;false\u0026rsquo;: 1 Sửa các boolean thành integer để giải quyết lỗi Đã hết lỗi và dữ liệu hoạt động ổn định Nâng cao kỹ năng debug và xử lý lỗi data type mismatch Cải thiện kỹ năng làm việc nhóm:\nLàm việc hiệu quả với nhiều team khác nhau Giải quyết các vấn đề tích hợp giữa các component Nâng cao kỹ năng giao tiếp kỹ thuật Hiểu được tầm quan trọng của data consistency "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/1-worklog/1.12-week12/","title":"Worklog Tuần 12","tags":[],"description":"","content":"Mục tiêu tuần 12: Tiếp tục phát triển và hoàn thiện dự án cuối khóa. Phối hợp với team database và frontend để tích hợp hệ thống. Giải quyết các lỗi kỹ thuật về data type và configuration. Xử lý các vấn đề CORS trong API integration. Viết workshop và chuẩn bị tài liệu dự án. Tham dự sự kiện AWS Cloud Mastery Series về Security Pillar. Các công việc cần triển khai trong tuần này: Thứ Công việc Ngày bắt đầu Ngày hoàn thành Nguồn tài liệu 2 - Tiếp tục làm project + Phối hợp với bên database để kết nối dữ liệu + Gặp lỗi java.lang.NumberFormatException: Character N is neither a decimal digit number, decimal point, nor \u0026quot;e\u0026quot; notation exponential mark\u0026quot; + Lỗi này là do có null trong integer + Sửa lại database, bỏ null Đã hết lỗi\n- Gặp lỗi java.lang.IllegalArgumentException: Could not resolve placeholder \u0026lsquo;app.jwt.secret\u0026rsquo; in value \u0026quot;${app.jwt.secret}\\ + Thêm environment variable vào Đã hết lỗi 24/11/2025 24/11/2025 3 - Tiếp tục làm project + Phối hợp với bên front-end để gọi api + Gặp lỗi java.lang.IllegalArgumentException: Boolean string was not \u0026rsquo;true\u0026rsquo; or \u0026lsquo;false\u0026rsquo;: 1\u0026quot; + Sửa các boolean thành integer Đã hết lỗi 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Tiếp tục làm project + Phối hợp với bên front end để gọi api + Gặp lỗi has been blocked by CORS policy: No \u0026lsquo;Access-Control-Allow-Origin\u0026rsquo; header is present on the requested resource + Sửa bên fe + Thêm vào method options, Access-Control-Allow-Methods -\u0026gt; \u0026lsquo;GET,POST,PUT,DELETE,OPTIONS\u0026rsquo; Đã hết lỗi 26/11/2025 26/11/2025 Lỗi has been blocked by CORS policy: No \u0026lsquo;Access-Control-Allow-Origin\u0026rsquo; header is present on the requested resource 5 - Tiếp tục làm project + Viết workshop + push backend lên github 27/11/2025 27/11/2025 workshop backend 6 - Tham dự sự kiện AWS Cloud Mastery Series #3 - AWS Well-Architected Security Pillar 28/11/2025 28/11/2025 AWS Cloud Mastery Series #3 Kết quả đạt được tuần 12: Giải quyết lỗi NumberFormatException và JWT configuration:\nPhối hợp với team database để kết nối dữ liệu Gặp lỗi java.lang.NumberFormatException: Character N is neither a decimal digit number Phát hiện lỗi do có giá trị null trong các trường integer Sửa lại database, loại bỏ các giá trị null Gặp lỗi Could not resolve placeholder \u0026lsquo;app.jwt.secret\u0026rsquo; Thêm environment variable để giải quyết vấn đề JWT configuration Đã giải quyết tất cả các lỗi và hệ thống hoạt động ổn định Phối hợp với frontend team và giải quyết lỗi API:\nPhối hợp với bên frontend để gọi API Gặp lại lỗi java.lang.IllegalArgumentException về Boolean data type Sửa các boolean thành integer để đảm bảo tính nhất quán Đã giải quyết vấn đề data type mismatch Nâng cao kỹ năng debug và xử lý lỗi integration Xử lý vấn đề CORS trong API Gateway:\nTiếp tục phối hợp với frontend team để gọi API Gặp lỗi CORS policy: No \u0026lsquo;Access-Control-Allow-Origin\u0026rsquo; header Sửa bên frontend để xử lý CORS Thêm vào method OPTIONS: Access-Control-Allow-Methods -\u0026gt; \u0026lsquo;GET,POST,PUT,DELETE,OPTIONS\u0026rsquo; Đã giải quyết hoàn toàn vấn đề CORS Học cách cấu hình CORS cho API Gateway Viết workshop và quản lý source code:\nViết workshop cho dự án để hướng dẫn người khác Push backend code lên GitHub để quản lý version Tổ chức và document hóa toàn bộ dự án Chuẩn bị tài liệu cho việc demo và bàn giao Nâng cao kỹ năng viết tài liệu kỹ thuật Tham dự sự kiện và học hỏi về Security:\nTham dự sự kiện AWS Cloud Mastery Series #3 Học về AWS Well-Architected Security Pillar Nắm được các best practices về bảo mật trên AWS Ứng dụng kiến thức security vào dự án thực tế Cập nhật kiến thức về security compliance và governance Cải thiện kỹ năng teamwork và problem-solving:\nLàm việc hiệu quả với nhiều team (database, frontend) Giải quyết nhanh chóng các vấn đề kỹ thuật phức tạp Nâng cao kỹ năng debug và troubleshooting Phát triển kỹ năng giao tiếp và điều phối dự án "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/2-backend/","title":"Backend Workshop (BE)","tags":[],"description":"","content":"Xây dựng Backend Trong workshop này, bạn sẽ:\nSử dụng Amazon S3 để lưu trữ file dữ liệu đầu vào (CSV). Cấu hình AWS Lambda Trigger để tự động import dữ liệu vào DynamoDB. Tạo Lambda Function (API Handler) và công khai qua API Gateway để truy cập dữ liệu. Kiểm thử API bằng Postman hoặc API Gateway Console. Dọn dẹp toàn bộ tài nguyên sau khi hoàn thành. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/3-ai/3.2/","title":"Cấu hình RDS và kết nối với Dbeaver","tags":[],"description":"","content":"Cấu hình RDS và kết nối với Dbeaver Các bước thực hiện 1. Truy cập dịch vụ RDS Vào AWS Management Console → tìm RDS. Trước khi tạo RDS chúng ta sẽ tạo subnet group Đặt tên cho subnet group và chọn VPC đã tạo AZ chọn ap-southeast-1a và ap-southeast-1b Subnet chọn 2 private subnet sau đó ấn Create 2. Tạo RDS Vào database → Create database Ở phần cấu hình database chọn Full configuration chọn PostgreSQL ờ phần Templates chọn Sandbox ở phần Settings đặt tên cho DB và đặt mật khẩu Cấu hình các phần còn lại như sau Ở phần Connectivity chọn VPC đã tạo và DB subnet group và Public access chọn No và phần VPC security group chọn Security group đã tạo cho RDS còn lại giữ nguyên và bấm Create 1. Lưu trữ dữ liệu và kết nối PostgreSQL bằng DBeaver Bạn có thể tải Dbeaver tại đây: https://dbeaver.io/ Tải file knowledge_base từ đây.\nĐể có thể kết nối từ máy local tới Dbeaver chúng ta cần tạo một EC2 để có thể làm cầu nối\nTruy cập vào EC2 → Launch instance\nĐặt tên cho EC2 và chọn Instance type t2.micro tạo một key pair và lưu trong máy Ở phần Network settings chọn VPC đã tạo chọn public subnet và tạo một Security group Inbound Security Group Rules chọn my ip xong Launch instance Tiếp theo chúng ta sẽ qua phần Security group của RDS chỉnh sửa phần inbound rules chúng ta sẽ add rule mới type: PostgreSQL và Source là EC2 vừa tạo Truy cập vào dbeaver ấn vào phần connection Chọn PostgreSQL ở phần Host copy port Endpoint của RDS và các thông tin bạn đã tạo RDS từ đầu Ấn vào dấu + SSH ở phần Host/IP copy public IP của EC2 User Name điền ec2-user ở phần Authentication Method chọn Public Key và chọn key pair đã tạo lúc tạo EC2 sau đó ấn Test connection rồi Finish Sau khi kết nối thành công mở sql script và dán dòng code này để tạo bảng knowledge_base sau khi tạo bảng refresh lại database để hiện bảng knowledge_base\n-- 1. Bật extension vector (chỉ chạy 1 lần) CREATE EXTENSION IF NOT EXISTS vector; -- 2. Tạo bảng kiến thức (Knowledge Base) CREATE TABLE knowledge_base ( id bigserial PRIMARY KEY, content text, -- Nội dung text gốc (đoạn văn đã chia nhỏ) metadata jsonb, -- Lưu thêm info: link ảnh, tên file, page number... embedding vector(1024) -- QUAN TRỌNG: Phải là 1024 cho Cohere Multilingual ); -- 3. Tạo index để tìm kiếm nhanh hơn CREATE INDEX ON knowledge_base USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64); Để có thể import dữ liệu vào Dbeaver bằng python chúng ta cần ssh thông qua cmd chúng ta sẽ mở cmd ở nơi lưu trữ keypair và copy câu lệnh này\nssh -i \u0026#34;my-key.pem\u0026#34; -L 5433:RDS endpoint port:5432 ec2-user@public IP EC2 -N Sau đó chúng ta chạy hàm python này để import dữ liệu vô Dbeaver\nimport pandas as pd import json import boto3 import psycopg2 import time import glob import os import numpy as np from dotenv import load_dotenv # ========================================== # 1. CẤU HÌNH \u0026amp; BẢO MẬT # ========================================== current_dir = os.path.dirname(os.path.abspath(__file__)) env_path = os.path.join(current_dir, \u0026#39;pass.env\u0026#39;) load_dotenv(env_path) CSV_FOLDER = \u0026#39;./database\u0026#39; DB_HOST = os.getenv(\u0026#34;DB_HOST\u0026#34;) DB_NAME = os.getenv(\u0026#34;DB_NAME\u0026#34;) DB_USER = os.getenv(\u0026#34;DB_USER\u0026#34;) DB_PASS = os.getenv(\u0026#34;DB_PASS\u0026#34;) # Kết nối AWS bedrock = boto3.client( service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;, aws_access_key_id=os.getenv(\u0026#34;aws_access_key_id\u0026#34;), aws_secret_access_key=os.getenv(\u0026#34;aws_secret_access_key\u0026#34;) ) # ========================================== # 2. TỪ ĐIỂN DỊCH (QUAN TRỌNG NHẤT) # ========================================== # A. Dịch Tên Cột (Cho AI hiểu ngữ cảnh) COLUMN_MAP = { \u0026#34;price\u0026#34;: \u0026#34;Giá bán\u0026#34;, \u0026#34;gia\u0026#34;: \u0026#34;Giá bán\u0026#34;, \u0026#34;cost\u0026#34;: \u0026#34;Chi phí\u0026#34;, \u0026#34;fee\u0026#34;: \u0026#34;Phí\u0026#34;, \u0026#34;stock\u0026#34;: \u0026#34;Tồn kho\u0026#34;, \u0026#34;so_luong\u0026#34;: \u0026#34;Tồn kho\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Mô tả\u0026#34;, \u0026#34;mo_ta\u0026#34;: \u0026#34;Mô tả\u0026#34;, \u0026#34;chi_tiet\u0026#34;: \u0026#34;Chi tiết\u0026#34;, \u0026#34;origin\u0026#34;: \u0026#34;Xuất xứ\u0026#34;, \u0026#34;xuat_xu\u0026#34;: \u0026#34;Xuất xứ\u0026#34;, \u0026#34;material\u0026#34;: \u0026#34;Chất liệu\u0026#34;, \u0026#34;chat_lieu\u0026#34;: \u0026#34;Chất liệu\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;Màu sắc\u0026#34;, \u0026#34;mau_sac\u0026#34;: \u0026#34;Màu sắc\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;Trọng lượng\u0026#34;, \u0026#34;trong_luong\u0026#34;: \u0026#34;Trọng lượng\u0026#34;, \u0026#34;food_type\u0026#34;: \u0026#34;Loại thức ăn\u0026#34;, \u0026#34;usage_target\u0026#34;: \u0026#34;Phù hợp cho loài chim\u0026#34;, \u0026#34;furniture_type\u0026#34;: \u0026#34;Loại nội thất\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;Thời gian xử lý\u0026#34;, \u0026#34;thoi_gian\u0026#34;: \u0026#34;Thời gian xử lý\u0026#34;, \u0026#34;method_name\u0026#34;: \u0026#34;Tên phương thức\u0026#34; } # B. Dịch Giá Trị (Cho Website hiển thị tiếng Việt) \u0026lt;--- PHẦN BẠN CẦN VALUE_TRANSLATIONS = { \u0026#34;FOODS\u0026#34;: \u0026#34;Đồ ăn\u0026#34;, \u0026#34;Foods\u0026#34;: \u0026#34;Đồ ăn\u0026#34;, \u0026#34;foods\u0026#34;: \u0026#34;Đồ ăn\u0026#34;, \u0026#34;TOYS\u0026#34;: \u0026#34;Đồ chơi\u0026#34;, \u0026#34;Toys\u0026#34;: \u0026#34;Đồ chơi\u0026#34;, \u0026#34;toys\u0026#34;: \u0026#34;Đồ chơi\u0026#34;, \u0026#34;FURNITURE\u0026#34;: \u0026#34;Nội thất\u0026#34;, \u0026#34;Furniture\u0026#34;: \u0026#34;Nội thất\u0026#34;, \u0026#34;furniture\u0026#34;: \u0026#34;Nội thất\u0026#34;, \u0026#34;Bird\u0026#34;: \u0026#34;Chim cảnh\u0026#34;, \u0026#34;bird\u0026#34;: \u0026#34;Chim cảnh\u0026#34; } # --- CÁC HÀM PHỤ TRỢ --- def get_embedding(text): try: if not text or len(str(text)) \u0026lt; 5: return None body = json.dumps({\u0026#34;texts\u0026#34;: [str(text)], \u0026#34;input_type\u0026#34;: \u0026#34;search_document\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34;}) response = bedrock.invoke_model(body=body, modelId=\u0026#34;cohere.embed-multilingual-v3\u0026#34;, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34;) return json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embeddings\u0026#39;][0] except: return None def clean(val): if pd.isna(val) or str(val).lower() in [\u0026#39;nan\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;null\u0026#39;]: return \u0026#34;\u0026#34; val_str = str(val).strip() # --- DỊCH TỰ ĐỘNG Ở ĐÂY --- # Nếu giá trị có trong từ điển dịch thì thay thế luôn if val_str in VALUE_TRANSLATIONS: return VALUE_TRANSLATIONS[val_str] return val_str def main(): try: conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS, port=5433) # Lưu ý port SSH 5433 cur = conn.cursor() print(\u0026#34;✅ Kết nối Database thành công!\u0026#34;) except Exception as e: print(f\u0026#34;❌ Lỗi kết nối DB: {e}\u0026#34;); return csv_files = glob.glob(os.path.join(CSV_FOLDER, \u0026#34;*.csv\u0026#34;)) print(f\u0026#34;📂 Tìm thấy {len(csv_files)} file CSV.\u0026#34;) # Biến thống kê stats = {\u0026#34;bird\u0026#34;: 0, \u0026#34;food\u0026#34;: 0, \u0026#34;toy\u0026#34;: 0, \u0026#34;furniture\u0026#34;: 0, \u0026#34;best_sellers\u0026#34;: []} total_success = 0 for file_path in csv_files: filename = os.path.basename(file_path).lower() print(f\u0026#34;\\n--- Đang xử lý file: {filename} ---\u0026#34;) try: df = pd.read_csv(file_path) df = df.replace({np.nan: None}) # Tự động nhận diện loại (Prefix) category_prefix = \u0026#34;Sản phẩm\u0026#34; if \u0026#34;bird\u0026#34; in filename: category_prefix = \u0026#34;Loài chim\u0026#34; elif \u0026#34;food\u0026#34; in filename: category_prefix = \u0026#34;Thức ăn chim\u0026#34; elif \u0026#34;toy\u0026#34; in filename or \u0026#34;do_choi\u0026#34; in filename: category_prefix = \u0026#34;Đồ chơi chim\u0026#34; elif \u0026#34;furniture\u0026#34; in filename: category_prefix = \u0026#34;Nội thất lồng chim\u0026#34; elif \u0026#34;ship\u0026#34; in filename or \u0026#34;delivery\u0026#34; in filename: category_prefix = \u0026#34;Phương thức vận chuyển\u0026#34; elif \u0026#34;payment\u0026#34; in filename: category_prefix = \u0026#34;Phương thức thanh toán\u0026#34; for index, row in df.iterrows(): # Thống kê if \u0026#34;bird\u0026#34; in filename: stats[\u0026#34;bird\u0026#34;] += 1 elif \u0026#34;food\u0026#34; in filename: stats[\u0026#34;food\u0026#34;] += 1 elif \u0026#34;toy\u0026#34; in filename: stats[\u0026#34;toy\u0026#34;] += 1 elif \u0026#34;furniture\u0026#34; in filename: stats[\u0026#34;furniture\u0026#34;] += 1 # A. ĐỊNH DANH p_id = clean(row.get(\u0026#39;id\u0026#39;) or row.get(\u0026#39;product_id\u0026#39;) or row.get(\u0026#39;payment_id\u0026#39;)) name = clean(row.get(\u0026#39;name\u0026#39;) or row.get(\u0026#39;product_name\u0026#39;) or row.get(\u0026#39;title\u0026#39;) or row.get(\u0026#39;method_name\u0026#39;)) if not name: if p_id: name = f\u0026#34;Mã {p_id}\u0026#34; else: continue # B. QUÉT CỘT TỰ ĐỘNG VÀ DỊCH content_parts = [f\u0026#34;{category_prefix}: {name}\u0026#34;] # Quét toàn bộ cột, nếu có trong COLUMN_MAP thì thêm vào for col_key, col_val in row.items(): val_clean = clean(col_val) # Hàm clean sẽ tự động dịch FOODS -\u0026gt; Đồ ăn if val_clean and col_key in COLUMN_MAP: content_parts.append(f\u0026#34;{COLUMN_MAP[col_key]}: {val_clean}\u0026#34;) # C. XỬ LÝ RIÊNG GIÁ \u0026amp; TỒN KHO \u0026amp; BÁN CHẠY price = clean(row.get(\u0026#39;price\u0026#39;) or row.get(\u0026#39;gia\u0026#39;) or row.get(\u0026#39;fee\u0026#39;)) if price: content_parts.append(f\u0026#34;Giá: {price}\u0026#34;) stock = clean(row.get(\u0026#39;stock\u0026#39;) or row.get(\u0026#39;so_luong\u0026#39;)) if stock: content_parts.append(f\u0026#34;Tồn kho: {stock}\u0026#34;) sold = clean(row.get(\u0026#39;sold\u0026#39;) or row.get(\u0026#39;da_ban\u0026#39;)) if sold: content_parts.append(f\u0026#34;Đã bán: {sold}\u0026#34;) try: if float(sold) \u0026gt; 0: stats[\u0026#34;best_sellers\u0026#34;].append((float(sold), name, category_prefix)) except: pass content_to_embed = \u0026#34;. \u0026#34;.join(content_parts) + \u0026#34;.\u0026#34; # D. TẠO METADATA (Cũng dùng giá trị đã dịch) # Lưu ý: Hàm clean() ở trên đã dịch rồi, nên ta gọi lại clean() cho từng field metadata = {} for k, v in row.items(): metadata[k] = clean(v) # Lưu vào metadata bản tiếng Việt luôn # Ghi đè các trường chuẩn metadata[\u0026#39;id\u0026#39;] = p_id metadata[\u0026#39;name\u0026#39;] = name metadata[\u0026#39;price\u0026#39;] = price if price else \u0026#34;Liên hệ\u0026#34; metadata[\u0026#39;type\u0026#39;] = category_prefix metadata[\u0026#39;image\u0026#39;] = clean(row.get(\u0026#39;image_url\u0026#39;) or row.get(\u0026#39;link_anh\u0026#39;)) metadata[\u0026#39;sold\u0026#39;] = sold # E. INSERT vector = get_embedding(content_to_embed) if vector: cur.execute( \u0026#34;INSERT INTO knowledge_base (content, embedding, metadata) VALUES (%s, %s, %s)\u0026#34;, (content_to_embed, json.dumps(vector), json.dumps(metadata, default=str)) ) total_success += 1 if total_success % 10 == 0: print(f\u0026#34; -\u0026gt; Đã nạp {total_success} dòng...\u0026#34;) conn.commit() time.sleep(0.1) except Exception as e: print(f\u0026#34;⚠️ Lỗi xử lý file {filename}: {e}\u0026#34;); continue # --- TẠO BẢN TIN THỐNG KÊ --- print(\u0026#34;\\n--- Đang tạo bản tin thống kê... ---\u0026#34;) top_products = sorted(stats[\u0026#34;best_sellers\u0026#34;], key=lambda x: x[0], reverse=True)[:5] top_names = \u0026#34;, \u0026#34;.join([f\u0026#34;{p[1]} ({int(p[0])} lượt mua)\u0026#34; for p in top_products]) summary_content = ( f\u0026#34;BÁO CÁO THỐNG KÊ SHOP CHIM: \u0026#34; f\u0026#34;Tổng số chim: {stats[\u0026#39;bird\u0026#39;]}. Đồ ăn: {stats[\u0026#39;food\u0026#39;]}. \u0026#34; f\u0026#34;Đồ chơi: {stats[\u0026#39;toy\u0026#39;]}. Nội thất: {stats[\u0026#39;furniture\u0026#39;]}. \u0026#34; f\u0026#34;TOP 5 SẢN PHẨM BÁN CHẠY NHẤT: {top_names}.\u0026#34; ) summary_vector = get_embedding(summary_content) if summary_vector: cur.execute(\u0026#34;INSERT INTO knowledge_base (content, embedding, metadata) VALUES (%s, %s, %s)\u0026#34;, (summary_content, json.dumps(summary_vector), json.dumps({\u0026#34;id\u0026#34;:\u0026#34;STATS\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Thống kê\u0026#34;}, default=str))) conn.commit() cur.close(); conn.close() print(f\u0026#34;\\n🎉 HOÀN TẤT! Tổng cộng đã import: {total_success + 1} dòng.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Sau khi import xong refresh lại bảng knowledge_base để thấy kết quả\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/2-backend/2.2/","title":"Kiểm tra dữ liệu trong DynamoDB","tags":[],"description":"","content":"Kiểm tra dữ liệu trong DynamoDB Trong bước này, bạn sẽ xác minh rằng dữ liệu từ file CSV đã được import thành công vào DynamoDB.\nCác bước thực hiện Upload dữ liệu CSV lên Bucket Tải file mẫu CSV từ đây.\nTrong Bucket vừa tạo:\nChọn tab Objects → Upload. Giải nén file zip. Kéo \u0026amp; thả các file, sau đó nhấn Upload. [!TIP] Sau khi upload xong hãy đợi khoảng 3-5 phút để lambda import dữ liệu\nTruy cập dịch vụ DynamoDB Vào AWS Management Console → tìm DynamoDB. Chọn Tables → nhấp vào bảng ví dụ: Products. Xem dữ liệu Chọn tab Explore items. Kiểm tra danh sách sản phẩm đã được import. Nếu không thấy dữ liệu, kiểm tra:\nTên bảng DynamoDB phải trùng với tên file CSV. File CSV phải có header hợp lệ. Lambda có đủ quyền truy cập S3 và DynamoDB. Tạo GSI cho DynamoDB\nTạo GSI (Global Secondary Index) cho DynamoDB\nĐể backend có thể truy vấn dữ liệu dựa trên các trường không phải khóa chính (ví dụ: tìm Account bằng username, tìm Order bằng customer_id), chúng ta cần tạo các Global Secondary Indexes (GSI) tương ứng với cấu trúc trong mã nguồn Java.\nChúng ta sẽ sử dụng AWS CloudShell để chạy script tự động tạo toàn bộ Index nhằm đảm bảo chính xác và tiết kiệm thời gian.\nTrên thanh điều hướng phía trên cùng (Top Navigation Bar), nhấp vào biểu tượng CloudShell (hình terminal \u0026gt;_). Đợi vài giây để môi trường dòng lệnh khởi động.\nTại dòng lệnh CloudShell, tạo một file script mới tên là create_gsi.sh:\nXóa file (nếu đã tạo trước đó): rm create_all_gsi.sh\nTạo File mới: nano create_all_gsi.sh\nTải file về mở lên rồi copy toàn bộ vào Cloudshell.\nCtrl + O (Để lưu) -\u0026gt; Nhấn Enter\nCtrl + X (Để thoát)\nCấp quyền chạy cho file: chmod +x create_all_gsi.sh\nKhởi chạy file: ./create_all_gsi.sh\nThời gian đợi sẽ tùy thuộc vào việc Indexs được tạo ra trong bao lâu\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/4-frontend/4.2/","title":"Phân phối với CloudFront","tags":[],"description":"","content":"Tạo CloudFront Distribution trỏ đến S3 bucket Truy cập dịch vụ CloudFront. Nhấn Create CloudFront Distribution (Tạo Phân phối CloudFront). Nhập một Tên CloudFront (ví dụ: flyora-shop). Chọn S3 bucket mà bạn đã tạo trước đó. Bật Website endpoint (Điểm cuối Website), đảm bảo bạn nhận được URL có dạng như sau: [http://Tên-S3-của-bạn.s3-website.ap-southeast-1.amazonaws.com/](http://Tên-S3-của-bạn.s3-website.ap-southeast-1.amazonaws.com/) Cấu hình: - Viewer protocol policy (Chính sách giao thức người xem): Redirect HTTP to HTTPS (Chuyển hướng HTTP sang HTTPS) - Allowed HTTP method (Phương thức HTTP được phép): GET, HEAD, OPTION - Cache policy (Chính sách cache): CachingOptimized (Tối ưu hóa cache) - Response headers policy (Chính sách tiêu đề phản hồi): CORS-with-preflight-and-SecurityHeadersPolicy Chọn do not enable security protections (không bật tính năng bảo vệ bảo mật). Review (Xem lại) và nhấn Create Distribution (Tạo Phân phối). Sau khi tạo Distribution, bạn cần chờ 5 đến 10 phút để quá trình triển khai (deploy) hoàn tất. Nếu triển khai thành công, nó sẽ hiển thị ngày bạn đã triển khai. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/3-blogstranslated/","title":"Các bài blogs đã dịch","tags":[],"description":"","content":"Tại đây sẽ là phần liệt kê, giới thiệu các blogs mà các bạn đã dịch. Ví dụ:\nBlog 1 - Centralized Multi-Account Application Resilience Assessment Using AWS Resilience Hub Blog này hướng dẫn cách triển khai giải pháp đánh giá khả năng phục hồi (resilience) tập trung cho các ứng dụng trên nhiều tài khoản AWS bằng AWS Resilience Hub. Bạn sẽ tìm hiểu về mô hình hub and spoke để quản lý resilience policies, cách thiết lập IAM roles cho cross-account assessment, tích hợp SNS để nhận thông báo drift detection, và cách phân tích báo cáo đánh giá để cải thiện RTO/RPO của ứng dụng theo chuẩn AWS Well-Architected Framework.\nBlog 2 - Implement a USDC bridge on AWS Blog này trình bày cách xây dựng cầu nối USDC (stablecoin) trên AWS sử dụng kiến trúc serverless với AWS Step Functions và Lambda. Bạn sẽ học cách triển khai Cross-Chain Transfer Protocol (CCTP) V2 của Circle để chuyển USDC giữa các blockchain như Ethereum, Base, Avalanche, tích hợp với Circle Attestation Service, quản lý private keys bằng AWS Secrets Manager, và xử lý workflow burn-mint token một cách tự động và an toàn.\nBlog 3 - Strengthen Your AWS Cloud Storage Security with Superna Defender Blog này giới thiệu giải pháp bảo mật lưu trữ đám mây AWS với Superna Defender, một công cụ Cyberstorage giúp bảo vệ Amazon S3 và FSx for Windows File Server khỏi ransomware và malware. Bạn sẽ tìm hiểu về khung NIST Cyberstorage Checklist (Identify, Detect, Protect, Respond, Recover), cách giám sát và phát hiện mối đe dọa theo thời gian thực, tự động cách ly và khôi phục dữ liệu bị ảnh hưởng, và tích hợp với các công cụ bảo mật như Splunk, Palo Alto Networks, AWS Security Hub.\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/3-ai/","title":"AI Workshop (Chatbot)","tags":[],"description":"","content":"Workshop này hướng dẫn chi tiết cách xây dựng một Chatbot tư vấn sản phẩm sử dụng kiến trúc RAG (Retrieval-Augmented Generation) trên nền tảng AWS.\n1. Kiến trúc hệ thống Hệ thống sử dụng các dịch vụ sau của AWS:\nAmazon Bedrock: Cung cấp các mô hình AI (LLM). Generation Model: Amazon Nova Lite (anthropic.claude-3-haiku-20240307-v1:0) để trả lời câu hỏi. Embedding Model: Cohere Embed Multilingual (cohere.embed-multilingual-v3) để vector hóa dữ liệu. Amazon RDS (PostgreSQL): Lưu trữ dữ liệu sản phẩm và vector (sử dụng extension Dbeaver). AWS Lambda: Hàm xử lý logic trung gian (Serverless backend). "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/4-frontend/4.3/","title":"Kết nối với API","tags":[],"description":"","content":"Bước 1: Lấy URL của API Gateway Trước hết, bạn phải nhận được URL của API Gateway: Mở thư mục dự án của bạn trong VS Code. Tạo một tệp mới có tên là api.js bên trong thư mục src. Thêm mã JavaScript sau: Bước 2: Kiểm tra API trong Postman Lấy dữ liệu từ Postman. Bạn sẽ nhận được phản hồi với mã trạng thái 200 kèm theo dữ liệu JSON: Bước 3: Triển khai và Xác minh trên S3 Truy cập trang web của bạn: http://your-bucket-name.s3-website-ap-southeast-1.amazonaws.com Sử dụng dữ liệu từ Postman với URL: Nếu bạn đăng nhập và thấy thông báo như thế này, thì việc Tích hợp API đã thành công: "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/2-backend/2.3/","title":"Tạo API Gateway và tích hợp Lambda","tags":[],"description":"","content":"Mục tiêu Tạo Lambda Function mới để xử lý các yêu cầu đến DynamoDB thông qua API Gateway.\nCác bước thực hiện Tải file backend từ đây.\nBước 1: Tạo IAM Role Mở IAM Console → Roles → Create role Chọn Trusted entity: AWS Service → Lambda Gán quyền: AmazonDynamoDBFullAccess CloudWatchLogsFullAccess Đặt tên role: LambdaAPIAccessRole Bước 2: Tạo Lambda Function Vào AWS Lambda → Create function Chọn Author from scratch Nhập tên: DynamoDB_API_Handler Runtime: Java 17 Chọn IAM Role: LambdaAPIAccessRole Bước 3: Deploy file jar Vào S3 → Upload → Add files: Chọn file jar. Sau đó copy object Url Vào AWS Lambda → Upload from: dan s3 url vua tai len Vào AWS Lambda → Code → Runtime settings -\u0026gt; Edit\nHandler: org.example.flyora_backend.handler.StreamLambdaHandler::handleRequest Vào AWS Lambda → Code -\u0026gt; Configuration → Edit:\nTimeout: 1 min Vào AWS Lambda → Configuration → Environment variables → Edit\nKey: APP_JWT_SECRET; Value: huntrotflyorateam!@ky5group5member Tương tự: Key: GHN_TOKEN; Value: 445c659d-5586-11f0-8c19-5aba781b9b65\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/3-ai/3.3/","title":"Tạo logic cho hàm lambda","tags":[],"description":"","content":"Tạo logic cho hàm lambda Các bước thực hiện 1. Truy cập dịch vụ lambda Vào AWS Management Console → tìm Lambda. Đầu tiên qua phần layer tạo một layer để hàm lambda có thư viện kết nối với PostgreSQL (psycopg2).\nTruy cập vào link github để có thể tải phiên bản python phù hợp ở đây là sử dụng psycopg2-3.11 https://github.com/jkehler/awslambda-psycopg2\nSau khi tải xong bỏ tất cả folder psycopg2-3.11 vào một folder đặt tên là (python) sau đó zip folder đó lại và đặt tên (postgres-layer-3.11)\nBấm create layer đặt tên cho layer xong tải file zip postgres-layer-3.11 lên → create 2. Tạo Lambda Functions → Create function Đặt tên cho hàm lambda và chọn runtime Python 3.11 Additional configurations tích chọn VPC chọn VPC đã được tạo Subnet chọn 1 private subnet Security group chọn Lambda-SG Sau khi hàm lambda được tạo chúng ta sẽ đến phần layer add layer đã được tạo Custom layer chọn layer đã tạo version chọn 1 → Add Qua phần Configuration ở phần General configuration chỉnh thời gian lên 1p Permissions thêm AmazonBedrockFullAccess và AWSLambdaVPCAccessExecutionRole Environment variables thêm các biến: DB_HOST, DB_NAME, DB_USER, DB_PASS (Điền thông tin RDS của bạn vào). Sau khi cấu hình xong dán dòng code python này vô hàm lambda và nhấn nút deploy import json import boto3 import psycopg2 import os # --- CẤU HÌNH --- DB_HOST = os.environ.get(\u0026#39;DB_HOST\u0026#39;) DB_NAME = os.environ.get(\u0026#39;DB_NAME\u0026#39;) DB_USER = os.environ.get(\u0026#39;DB_USER\u0026#39;) DB_PASS = os.environ.get(\u0026#39;DB_PASS\u0026#39;) # Dùng vùng US East 1 cho ổn định bedrock = boto3.client(service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) # --- 1. HÀM EMBEDDING (COHERE) --- def get_embedding(text): try: body = json.dumps({ \u0026#34;texts\u0026#34;: [text], \u0026#34;input_type\u0026#34;: \u0026#34;search_query\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34; }) response = bedrock.invoke_model( body=body, modelId=\u0026#34;cohere.embed-multilingual-v3\u0026#34;, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34; ) return json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embeddings\u0026#39;][0] except Exception as e: print(f\u0026#34;Lỗi Embed: {e}\u0026#34;) return None # --- 2. HÀM XỬ LÝ METADATA AN TOÀN --- def format_product_info(metadata): \u0026#34;\u0026#34;\u0026#34; Hàm này giúp chuẩn hóa dữ liệu dù file csv thiếu cột \u0026#34;\u0026#34;\u0026#34; if not metadata: return None # Lấy tên (Ưu tiên các key phổ biến) name = metadata.get(\u0026#39;name\u0026#39;) or metadata.get(\u0026#39;product_name\u0026#39;) or metadata.get(\u0026#39;title\u0026#39;) or \u0026#34;Sản phẩm không tên\u0026#34; # Lấy giá (Nếu không có thì để rỗng hoặc \u0026#39;Liên hệ\u0026#39;) price = metadata.get(\u0026#39;price\u0026#39;) or metadata.get(\u0026#39;gia\u0026#39;) or metadata.get(\u0026#39;display_price\u0026#39;) price_str = f\u0026#34;- Giá: {price}\u0026#34; if price else \u0026#34;\u0026#34; # Lấy loại (nếu có từ script import thông minh) category = metadata.get(\u0026#39;type\u0026#39;) or \u0026#34;Sản phẩm\u0026#34; # Tạo chuỗi mô tả cho AI đọc # Ví dụ: \u0026#34;Loài chim: Vẹt. - Giá: 500k\u0026#34; ai_context = f\u0026#34;Danh mục/Sản phẩm: {name} ({category}) {price_str}\u0026#34; # Tạo object cho Frontend hiển thị (Product Card) frontend_card = { \u0026#34;id\u0026#34;: metadata.get(\u0026#39;id\u0026#39;) or metadata.get(\u0026#39;product_id\u0026#39;), \u0026#34;name\u0026#34;: name, \u0026#34;price\u0026#34;: price if price else \u0026#34;Liên hệ\u0026#34;, # Frontend sẽ hiện chữ \u0026#34;Liên hệ\u0026#34; nếu không có giá \u0026#34;image\u0026#34;: metadata.get(\u0026#39;image_url\u0026#39;) or metadata.get(\u0026#39;link_anh\u0026#39;) or \u0026#34;\u0026#34;, # Ảnh có thể rỗng \u0026#34;type\u0026#34;: category # Để frontend biết đây là chim hay đồ chơi } return ai_context, frontend_card # --- 3. MAIN HANDLER --- def lambda_handler(event, context): print(\u0026#34;Event:\u0026#34;, event) try: # Parse Input if \u0026#39;body\u0026#39; in event: try: body_data = json.loads(event[\u0026#39;body\u0026#39;]) if isinstance(event[\u0026#39;body\u0026#39;], str) else event[\u0026#39;body\u0026#39;] except: body_data = {} else: body_data = event user_question = body_data.get(\u0026#39;question\u0026#39;, \u0026#39;\u0026#39;) if not user_question: return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Thiếu câu hỏi\u0026#39;)} # A. Tạo Vector q_vector = get_embedding(user_question) if not q_vector: return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Lỗi tạo vector\u0026#39;)} # B. Tìm kiếm DB conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS) cur = conn.cursor() # Lấy metadata lên để xử lý sql = \u0026#34;\u0026#34;\u0026#34; SELECT content, metadata FROM knowledge_base ORDER BY embedding \u0026lt;=\u0026gt; %s LIMIT 3 \u0026#34;\u0026#34;\u0026#34; cur.execute(sql, (json.dumps(q_vector),)) results = cur.fetchall() cur.close(); conn.close() # C. Xử lý kết quả (QUAN TRỌNG NHẤT) ai_contexts = [] frontend_products = [] for row in results: raw_content = row[0] # Nội dung gốc lúc import raw_metadata = row[1] # JSON metadata # Gọi hàm chuẩn hóa ai_text, card_data = format_product_info(raw_metadata) if ai_text: # Gộp nội dung gốc + nội dung định danh lại cho chắc ăn ai_contexts.append(f\u0026#34;{ai_text}. Chi tiết: {raw_content}\u0026#34;) frontend_products.append(card_data) # Nếu không tìm thấy gì if not ai_contexts: final_answer = \u0026#34;Xin lỗi, hiện tại shop chưa tìm thấy thông tin phù hợp trong kho dữ liệu ạ.\u0026#34; else: # D. Gửi cho LLM (Claude 3 Haiku) context_str = \u0026#34;\\n---\\n\u0026#34;.join(ai_contexts) system_prompt = ( \u0026#34;Bạn là nhân viên tư vấn của Shop Chim, tên là \u0026#39;Vẹt Tinh Thông\u0026#39;. \u0026#34; \u0026#34;Phong cách: Thân thiện, Lịch sự nhưng Ngắn gọn.\\n\\n\u0026#34; \u0026#34;HƯỚNG DẪN XỬ LÝ (ƯU TIÊN THEO THỨ TỰ):\\n\u0026#34; \u0026#34;1. GIAO TIẾP XÃ GIAO: Nếu khách chỉ chào hỏi (Xin chào, Hi...) hoặc cảm ơn: \u0026#34; \u0026#34;-\u0026gt; Hãy chào lại thân mật và hỏi khách cần tìm gì. TUYỆT ĐỐI KHÔNG tự ý liệt kê sản phẩm nếu khách chưa hỏi.\\n\u0026#34; \u0026#34; (Lưu ý: Đừng nhầm từ \u0026#39;Chào\u0026#39; trong \u0026#39;Xin chào\u0026#39; với chim \u0026#39;Chào mào\u0026#39;. Nếu khách chỉ chào, hãy lờ đi dữ liệu về chim Chào mào).\\n\\n\u0026#34; \u0026#34;2. TƯ VẤN SẢN PHẨM: Khi khách hỏi về hàng hóa:\\n\u0026#34; \u0026#34;-\u0026gt; Trả lời súc tích: Tên sản phẩm + Giá + Tình trạng kho (nếu có).\\n\u0026#34; \u0026#34;-\u0026gt; Không mô tả dài dòng văn hoa trừ khi khách hỏi chi tiết \u0026#39;như thế nào\u0026#39;, \u0026#39;ra sao\u0026#39;.\\n\u0026#34; \u0026#34;-\u0026gt; Nếu không có thông tin trong Context, hãy nói \u0026#39;Dạ shop chưa có món này ạ\u0026#39;.\\n\u0026#34; ) user_msg = f\u0026#34;Thông tin tham khảo:\\n{context_str}\\n\\nCâu hỏi: {user_question}\u0026#34; # Gọi Claude 3 Haiku claude_body = json.dumps({ \u0026#34;anthropic_version\u0026#34;: \u0026#34;bedrock-2023-05-31\u0026#34;, \u0026#34;max_tokens\u0026#34;: 300, \u0026#34;temperature\u0026#34;: 0.1, \u0026#34;system\u0026#34;: system_prompt, \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_msg}] }) try: model_res = bedrock.invoke_model( body=claude_body, modelId=\u0026#34;anthropic.claude-3-haiku-20240307-v1:0\u0026#34; ) res_json = json.loads(model_res[\u0026#39;body\u0026#39;].read()) final_answer = res_json[\u0026#39;content\u0026#39;][0][\u0026#39;text\u0026#39;] except Exception as e: print(f\u0026#34;Lỗi gọi LLM: {e}\u0026#34;) final_answer = \u0026#34;Xin lỗi, hệ thống AI đang bận, vui lòng thử lại sau.\u0026#34; # E. Trả về kết quả return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;POST\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#34;answer\u0026#34;: final_answer, \u0026#34;products\u0026#34;: frontend_products # Frontend dùng cái này để vẽ UI }) } except Exception as e: print(f\u0026#34;Lỗi System: {e}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(str(e))} 3. Tích hợp vào API Gateway của nhóm Truy cập dịch vụ API Gateway Chọn API mà Backend đã tạo Chọn Create resource Resource name chatbot và tích chọn vào CORS Chọn vào chatbot và tích chọn vào Create method Method type chọn POST tích chọn vào Lambda proxy integration Chọn VPC đã tạo xong ấn Create method Sau khi cấu hình xong bấm deploy API "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/4-eventparticipated/","title":"Các events đã tham gia","tags":[],"description":"","content":" Trong phần này, các bạn cần liệt kê và mô tả chi tiết các sự kiện (event) mà mình đã tham gia trong suốt quá trình thực tập hoặc làm việc.\nMỗi sự kiện nên được trình bày theo định dạng Event 1, Event 2, Event 3…, kèm theo các thông tin:\nTên sự kiện Thời gian tổ chức Địa điểm (nếu có) Vai trò của bạn trong sự kiện (người tham dự, hỗ trợ tổ chức, diễn giả, v.v.) Mô tả ngắn gọn nội dung và hoạt động chính trong sự kiện Kết quả hoặc giá trị đạt được (bài học, kỹ năng mới, đóng góp cho nhóm/dự án) Việc liệt kê này giúp thể hiện rõ sự tham gia thực tế của bạn, cũng như các kỹ năng mềm và kinh nghiệm bạn đã tích lũy qua từng sự kiện. Trong quá trình thực tập, em đã tham gia 6 events, với mỗi event là một trải nghiệm đáng nhớ với những kiến thức mới, hay và bổ ích, cùng với đó là nhứng món quà và những khoảnh khắc rất tuyệt vời.\nEvent 1 Tên sự kiện: [AWS GenAI Builder Club] AI-Driven Development Life Cycle – Reimagining Software Engineering\nThời gian: 14:00 ngày 3/10/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 2 Tên sự kiện: AI/ML/GenAI on AWS – Generative AI with Amazon Bedrock\nThời gian: 8:30 ngày 15/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 3 Tên sự kiện: AWS Cloud Mastery Series #2 – DevOps on AWS\nThời gian: 8:30 ngày 17/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 4 Tên sự kiện: chuyên đề về Edge Network Services\nThời gian: 9:00 ngày 19/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 5 Tên sự kiện: AWS Cloud Mastery Series #3 - AWS Well-Architected Security Pillar\nThời gian: 8:30 ngày 29/11/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\nEvent 6 Tên sự kiện: BUILDING AGENTIC AI - Context Optimization with Amazon Bedrock\nThời gian: 9:00 ngày 5/12/2025\nĐịa điểm: Tầng 26, tòa nhà Bitexco, số 02 đường Hải Triều, phường Sài Gòn, thành phố Hồ Chí Minh\nVai trò trong sự kiện: Người tham dự\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/4-frontend/","title":"Frontend Workshop (UI)","tags":[],"description":"","content":"Lưu trữ Frontend và Tích hợp API trên AWS Trong workshop này, bạn sẽ học cách triển khai một ứng dụng web frontend trên AWS và kết nối nó với API backend được lưu trữ thông qua Amazon API Gateway.\nHoạt động này là sự kết hợp giữa hosting frontend và tích hợp API, cho thấy cách các dịch vụ AWS có thể hỗ trợ các ứng dụng web serverless tương tác.\nAmazon S3 – Lưu trữ và phục vụ các tài nguyên web tĩnh của bạn (HTML, CSS, JS).\nAmazon CloudFront – Phân phối website của bạn trên toàn cầu với HTTPS và độ trễ thấp.\nAmazon API Gateway – Cung cấp các endpoint API backend mà frontend của bạn có thể gọi.\nWorkshop này minh họa cách kết nối một website tĩnh với backend API thông qua API Gateway, tạo nên một hoàn chỉnh kiến trúc web serverless cho phép tương tác dữ liệu thời gian thực giữa frontend và backend.\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/2-backend/2.4/","title":"Tạo API Gateway và tích hợp với Lambda","tags":[],"description":"","content":"Mục tiêu Kết nối AWS API Gateway với Lambda Function để tạo endpoint RESTful cho phép truy cập dữ liệu trong DynamoDB.\nCác bước thực hiện 1. Truy cập dịch vụ API Gateway Vào AWS Console → API Gateway Chọn Create API Chọn loại REST API (Build) Chọn: Create new API: New API API name: FlyoraAPI Endpoint Type: Regional Chọn Create API 2. Tạo Resource và Method Trong sidebar, chọn Actions → Create Resource Resource Name: api Chọn Create Resource Chọn /api → Actions → Create Resource\nTrong cấu hình resource:\nResource path: /api/ Resource Name: v1 Nhấn Create resource Trong cấu hình resource:\nTick Proxy resource Resource path: /api/ Resource Name: {proxy+} Nhấn Create resource Chọn /v1 → Actions → Create Resource\nTrong cấu hình resource:\nTick Proxy resource Resource path: /api/v1/ Resource Name: {myProxy+} Nhấn Create resource 6. Enable CORS cho tất cả resource Vào method OPTIONS → Integration response → Header Mappings, đảm bảo có cấu hình: Access-Control-Allow-Origin: \u0026lsquo;*\u0026rsquo; Access-Control-Allow-Headers: \u0026lsquo;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026rsquo; Access-Control-Allow-Methods: \u0026lsquo;DELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT\u0026rsquo; 3. Gắn Lambda Sau khi tạo thành công /api/vi/{myProxy+}, xuất hiện method ANY: Chọn ANY → Integration request → Edit Gắn Lambda: Integration type: Lambda Function Tick Lambda proxy integration Lambda Region: ap-southeast-1 (Singapore) Lambda Function: chọn hàm Lambda_API_Handler của bạn 4. Deploy API Chọn Actions → Deploy API Deployment stage: New stage Stage name: dev Description: Development stage for Lambda API Nhấn Deploy Sau khi deploy, bạn sẽ nhận được Invoke URL dạng: https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/5-cicd/","title":"CI/CD Automation","tags":[],"description":"","content":"Hướng dẫn thiết lập AWS CodeBuild cho Flyora Frontend Hướng dẫn này chỉ bao gồm thiết lập CI/CD cho Repository Frontend (React).\nHướng dẫn này sẽ giúp bạn thiết lập AWS CodeBuild và CodePipeline cho ứng dụng Flyora React frontend.\nYêu cầu Tài khoản AWS với quyền phù hợp GitHub repository: QuangHieu-lab/Flyora-shop File buildspec.yml trong thư mục gốc của repository (xem bên dưới) AWS Region: Singapore (ap-southeast-1) (hoặc region bạn chọn) S3 bucket để host static files CloudFront distribution (tùy chọn, cho CDN) Bắt buộc: File buildspec.yml Tạo file buildspec.yml trong thư mục gốc của repository với nội dung:\nversion: 0.2 phases: pre_build: commands: - echo \u0026#34;Installing dependencies on `date`\u0026#34; - npm ci - echo \u0026#34;Running linter...\u0026#34; - npm run lint --if-present || echo \u0026#34;No lint script configured\u0026#34; build: commands: - echo \u0026#34;Running tests on `date`\u0026#34; - npm test -- --watchAll=false --passWithNoTests --coverage || echo \u0026#34;Tests completed\u0026#34; - echo \u0026#34;Building application on `date`\u0026#34; - npm run build - echo \u0026#34;Build completed on `date`\u0026#34; post_build: commands: - echo \u0026#34;Post-build phase started on `date`\u0026#34; - echo \u0026#34;Checking if build directory exists...\u0026#34; - ls -la build/ - echo \u0026#34;Build artifacts created successfully\u0026#34; - echo \u0026#34;Build completed successfully - artifacts ready for deployment\u0026#34; - echo \u0026#34;Use CodePipeline Deploy stage for S3 deployment\u0026#34; - echo \u0026#34;CloudFront invalidation will be handled by CodePipeline\u0026#34; artifacts: files: - \u0026#39;**/*\u0026#39; base-directory: build name: BuildArtifact discard-paths: yes cache: paths: - \u0026#39;/root/.npm/**/*\u0026#39; - \u0026#39;node_modules/**/*\u0026#39; Điểm chính:\nSử dụng npm ci để cài đặt nhanh và đáng tin cậy hơn Chạy linter nếu được cấu hình Chạy tests với coverage Build ứng dụng React Deployment được xử lý bởi CodePipeline Deploy stage (không trong buildspec) Cache npm và node_modules Bước 1: Tạo S3 Bucket để Hosting Trước khi thiết lập CodeBuild, tạo S3 bucket để host ứng dụng React:\nVào S3 console Click \u0026ldquo;Create bucket\u0026rdquo; Bucket name: flyora-frontend-hosting Region: Singapore (ap-southeast-1) Bỏ chọn \u0026ldquo;Block all public access\u0026rdquo; (để host static website) Bật \u0026ldquo;Static website hosting\u0026rdquo; trong bucket properties Đặt Index document: index.html Đặt Error document: index.html Bước 2: Truy cập CodeBuild Mở trình duyệt và truy cập: https://console.aws.amazon.com/codesuite/codebuild/projects Đăng nhập vào tài khoản AWS Đảm bảo đang ở region Singapore (ap-southeast-1) Click \u0026ldquo;Create build project\u0026rdquo; Bước 3: Cấu hình Project Trường Giá trị Project name flyora-frontend-build Description Build project for Flyora React frontend Build badge Để trống Bước 4: Cấu hình Source Trường Giá trị Source provider GitHub Repository Repository in my GitHub account GitHub repository https://github.com/QuangHieu-lab/Flyora-shop | Source version | Để trống | | Git clone depth | 1 | | Primary source webhook events | ⚠️ BỎ CHỌN |\nBước 5: Cấu hình Environment Phần Trường Giá trị Provisioning Provisioning model On-demand Environment image Managed image Operating system Amazon Linux Runtime(s) Standard Image Latest (vd: aws/codebuild/amazonlinux2-x86_64-standard:5.0) Service role Service role New service role Bước 6: Biến môi trường Thêm các biến môi trường trong CodeBuild:\nTên Giá trị Loại AWS_S3_BUCKET flyora-frontend-hosting Plaintext CLOUDFRONT_DISTRIBUTION_ID CloudFront distribution ID của bạn (nếu dùng) Plaintext REACT_APP_API_URL URL backend API của bạn Plaintext Bước 7: Buildspec và Logs Buildspec:\nBuild specifications: Use a buildspec file Buildspec name: Để trống Logs:\nCloudWatch logs: ✅ Bật Group name: /aws/codebuild/flyora-frontend Bước 8: Quyền IAM CodeBuild service role cần quyền truy cập S3 và CloudFront. Thêm policy này:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::flyora-frontend-hosting\u0026#34;, \u0026#34;arn:aws:s3:::flyora-frontend-hosting/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateInvalidation\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Bước 9: Tạo CodePipeline Vào CodePipeline console Click \u0026ldquo;Create pipeline\u0026rdquo; Pipeline name: flyora-frontend-pipeline Source stage: GitHub (Version 2) Build stage: Chọn flyora-frontend-build Deploy stage: Thêm S3 deploy action Action provider: Amazon S3 Bucket: flyora-frontend-hosting Extract file before deploy: ✅ Chọn Kiểm tra Sau khi tạo pipeline:\nThực hiện thay đổi trong React repository Commit và push lên GitHub Pipeline tự động trigger Kiểm tra S3 bucket có files đã cập nhật Truy cập website qua S3 endpoint hoặc CloudFront URL Xử lý sự cố Vấn đề: Build thất bại với \u0026ldquo;npm: command not found\u0026rdquo; Giải pháp: Đảm bảo runtime-versions: nodejs: 18 được đặt trong buildspec.yml\nVấn đề: S3 sync permission denied Giải pháp: Kiểm tra IAM role có quyền S3 (xem Bước 8)\nVấn đề: Website hiển thị nội dung cũ Giải pháp:\nXóa browser cache Invalidate CloudFront cache nếu dùng CDN Kiểm tra S3 bucket có files mới nhất Tham khảo nhanh Tài nguyên Giá trị Pipeline Name flyora-frontend-pipeline Build Project Name flyora-frontend-build S3 Bucket flyora-frontend-hosting Region Singapore (ap-southeast-1) Source GitHub (QuangHieu-lab/Flyora-shop) Buildspec buildspec.yml (trong repo root) Logs CloudWatch: /aws/codebuild/flyora-frontend Ước tính chi phí Hạng mục Chi phí CodePipeline $1/tháng cho mỗi pipeline CodeBuild (Free Tier) 100 phút build/tháng trong 12 tháng S3 Storage ~$0.023/GB/tháng S3 Requests Tối thiểu cho static hosting CloudFront Free tier: 1TB data transfer/tháng Hàng tháng (ước tính) $1-3/tháng Tóm tắt ✅ Đã cấu hình Frontend CI/CD Pipeline!\nĐã hoàn thành:\n✅ CodeBuild project cho ứng dụng React ✅ CodePipeline cho quy trình tự động ✅ Tích hợp GitHub với trigger tự động ✅ Deploy tự động lên S3 ✅ Tích hợp CloudFront CDN (tùy chọn) Pipeline của bạn hiện tại:\nTự động phát hiện thay đổi code trong GitHub Build ứng dụng React với npm Deploy static files lên S3 Phục vụ website qua S3 hoặc CloudFront "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/2-backend/2.5/","title":"Kiểm thử API bằng Postman","tags":[],"description":"","content":"Mục tiêu Kiểm thử API Gateway REST endpoint tích hợp với Lambda Function để xác nhận hoạt động dữ liệu DynamoDB.\nTải và cài đặt Postman trước khi bắt đầu phần này.\nChỉnh lại Authorization Vào AWS Console → API Gateway Chọn FlyoraAPI Chọn /api/v1/{myProxy+} → ANY → Method request → Edit \u0026laquo;\u0026laquo;\u0026laquo;\u0026lt; Updated upstream Authorization: AWS_IAM ======= Authorization: AWS_IAM (Lưu ý chỉ bật khi dùng postman để kiểm thử, sau đó phải tắt đi) Stashed changes Tạo access key Vào AWS Console → IAM → Users Nhấn Create User Đặt tên: test Xác nhận tạo iam user Vào test → Security credentiala → Create access key Chọn Local code Copy access key và Secret access key Kiểm thử GET Mở Postman\nChọn GET\nNhập URL: https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev/api/v1/reviews/product/1\nTab Headers: Key: Content-Type | Value: application/json\nTab Authorization:\nType: AWS Signature Nhập AccessKey Nhập SecretKey AWS Region: ap-southeast-1 Service Name: execute-api Nhấn Send\nKết quả: Trả về danh sách Items trong bảng reviews\nKiểm thử POST Chọn POST\nURL: https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev/api/v1/reviews/submit\nBody → raw → JSON\n{ \u0026#34;customerId\u0026#34;: 2, \u0026#34;rating\u0026#34;: 4, \u0026#34;comment\u0026#34;: \u0026#34;Chim ăn ngon và vui vẻ!\u0026#34;, \u0026#34;customerName\u0026#34;: \u0026#34;Nguyễn Văn B\u0026#34; } Nhấn Send\nKết quả: Thêm Items trong bảng Review\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Triển khai hệ thống thương mại điện tử Flyora trên AWS Tổng quan Trong workshop này, bạn sẽ triển khai các thành phần cốt lõi của nền tảng Flyora theo mô hình Serverless trên AWS.\nMục tiêu là xây dựng hệ thống có khả năng mở rộng, tối ưu chi phí và dễ bảo trì.\nCác thành phần triển khai:\nFrontend: Lưu trữ \u0026amp; phân phối giao diện qua S3 + CloudFront Backend API: Xử lý logic nghiệp vụ qua API Gateway + AWS Lambda Cơ sở dữ liệu: Quản lý dữ liệu sản phẩm / đơn hàng bằng DynamoDB + S3 Chatbot: Hỗ trợ tư vấn sản phẩm, tích hợp vào UI (Nhóm AI thực hiện) Workshop được phân chia theo vai trò nhóm để dễ triển khai song song: Backend (BE), AI (Chatbot), và Frontend (FE).\nKiến trúc tổng thể Nội dung Workshop Giới thiệu mục tiêu \u0026amp; kết quả kỳ vọng\nBackend Workshop (BE) — Xây dựng API + Pipeline import dữ liệu tự động\nChuẩn bị \u0026amp; Cấu hình Lambda Trigger cho S3 Tạo Lambda tự động ghi dữ liệu CSV vào DynamoDB (S3 Trigger) Tạo API Gateway và tích hợp Lambda làm Backend API Kiểm thử API bằng Postman / API Gateway Console AI Workshop (Chatbot) — Hỗ trợ tư vấn sản phẩm\nTạo VPC \u0026amp; Cấu hình Sercurity Group cho RDS và Lambda Cấu hình RDS và kết nối với Dbeaver Tạo logic cho hàm lambda Frontend Workshop (FE) — Hiển thị dữ liệu \u0026amp; Hosting website\nHosting website với S3 Phân phối với CloudFront Kết nối với API Thiết lập CI/CD tự động deploy\nDọn dẹp tài nguyên để tránh phát sinh chi phí\nWorkshop này được thiết kế chạy trong phạm vi AWS Free Tier,\nkhông sử dụng EC2, không SSH, và không yêu cầu dịch vụ trả phí.\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/6-self-evaluation/","title":"Tự đánh giá","tags":[],"description":"","content":"Trong suốt thời gian thực tập tại AWS Vietnam từ 8/9 đến 28/11, tôi đã có cơ hội học hỏi, rèn luyện và áp dụng kiến thức đã được trang bị tại trường vào môi trường làm việc thực tế.\nTôi đã tham gia Một dự án xây dựng website thương mại điện tử chuyên bán các sản phẩm dành cho chim cảnh, qua đó cải thiện kỹ năng lập trình, phân tích, viết báo cáo, giao tiếp trong công việc, và kỹ năng tự học..\nVề tác phong, tôi luôn cố gắng hoàn thành tốt nhiệm vụ, tuân thủ nội quy, và tích cực trao đổi với đồng nghiệp để nâng cao hiệu quả công việc.\nĐể phản ánh một cách khách quan quá trình thực tập, tôi xin tự đánh giá bản thân dựa trên các tiêu chí dưới đây:\nSTT Tiêu chí Mô tả Tốt Khá Trung bình 1 Kiến thức và kỹ năng chuyên môn Hiểu biết về ngành, áp dụng kiến thức vào thực tế, kỹ năng sử dụng công cụ, chất lượng công việc ☐ ✅ ☐ 2 Khả năng học hỏi Tiếp thu kiến thức mới, học hỏi nhanh ☐ ✅ ☐ 3 Chủ động Tự tìm hiểu, nhận nhiệm vụ mà không chờ chỉ dẫn ✅ ☐ ☐ 4 Tinh thần trách nhiệm Hoàn thành công việc đúng hạn, đảm bảo chất lượng ✅ ☐ ☐ 5 Kỷ luật Tuân thủ giờ giấc, nội quy, quy trình làm việc ✅ ☐ ☐ 6 Tính cầu tiến Sẵn sàng nhận feedback và cải thiện bản thân ☐ ✅ ☐ 7 Giao tiếp Trình bày ý tưởng, báo cáo công việc rõ ràng ☐ ✅ ☐ 8 Hợp tác nhóm Làm việc hiệu quả với đồng nghiệp, tham gia nhóm ✅ ☐ ☐ 9 Ứng xử chuyên nghiệp Tôn trọng đồng nghiệp, đối tác, môi trường làm việc ✅ ☐ ☐ 10 Tư duy giải quyết vấn đề Nhận diện vấn đề, đề xuất giải pháp, sáng tạo ☐ ✅ ☐ 11 Đóng góp vào dự án/tổ chức Hiệu quả công việc, sáng kiến cải tiến, ghi nhận từ team ✅ ☐ ☐ 12 Tổng thể Đánh giá chung về toàn bộ quá trình thực tập ✅ ☐ ☐ Cần cải thiện Tôi cần học cách phân tích vấn đề từ nhiều góc độ hơn, hiểu sâu bản chất vấn đề để đưa ra giải pháp tối ưu và nhanh chóng. Tôi cần cải thiện khả năng trình bày ý tưởng, phản hồi trong các cuộc họp và giao tiếp trong môi trường công việc để đảm bảo trao đổi luôn rõ ràng và hiệu quả. Mặc dù đã chủ động học hỏi, tôi cần nâng cao hơn nữa khả năng tìm tài liệu, tiếp cận công nghệ mới và áp dụng vào công việc. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/6-cleanup/","title":"Dọn dẹp tài nguyên để tránh phát sinh chi phí","tags":[],"description":"","content":"Dọn dẹp tài nguyên AWS Để tránh phát sinh chi phí không mong muốn, bạn cần xóa các tài nguyên AWS đã tạo trong workshop theo thứ tự sau:\n1. Xóa EventBridge Rule Vào AWS Console → EventBridge Chọn Rules Chọn rule DatabaseBackup Chọn Delete Xác nhận xóa 2. Xóa API Gateway Vào AWS Console → API Gateway Chọn FlyoraAPI Chọn Actions → Delete API Xác nhận xóa bằng cách nhập tên API Nhấn Delete 3. Xóa Lambda Functions Vào AWS Console → Lambda Xóa các Lambda functions sau: DynamoDB_API_Handler AutoImportCSVtoDynamoDB DatabaseBackupFunction Với mỗi function: Chọn function → Actions → Delete Xác nhận xóa 4. Xóa DynamoDB Tables Vào AWS Console → DynamoDB Chọn Tables Xóa tất cả các bảng đã tạo từ CSV: Chọn tất cả bảng → Delete Xác nhận xóa bằng cách nhập delete 5. Xóa S3 Buckets và Objects 5.1. Xóa Bucket Database Vào AWS Console → S3 Chọn bucket flyora-bucket-database Xóa tất cả objects trong bucket: Chọn Empty bucket Xác nhận bằng cách nhập permanently delete Sau khi bucket rỗng: Chọn bucket → Delete bucket Xác nhận bằng cách nhập tên bucket 5.2. Xóa Bucket Backup Chọn bucket flyora-bucket-backup Xóa tất cả objects: Chọn Empty bucket Xác nhận xóa Xóa bucket: Chọn Delete bucket Xác nhận bằng cách nhập tên bucket 6. Xóa IAM User và Access Key Vào AWS Console → IAM → Users Chọn user test Vào tab Security credentials Xóa Access Key đã tạo: Chọn Access Key → Actions → Delete Quay lại danh sách Users Chọn user test → Delete user Xác nhận xóa 7. Xóa IAM Roles 7.1. Xóa LambdaAPIAccessRole Vào AWS Console → IAM → Roles Chọn LambdaAPIAccessRole Gỡ bỏ các policies đã gắn: AmazonDynamoDBFullAccess CloudWatchLogsFullAccess AWSXRayDaemonWriteAccess Chọn Delete role Xác nhận xóa 7.2. Xóa LambdaS3DynamoDBRole Chọn LambdaS3DynamoDBRole Gỡ bỏ các policies: AmazonS3FullAccess AmazonDynamoDBFullAccess_v2 Chọn Delete role Xác nhận xóa 7.3. Xóa LambdaDynamoDBBackupRole Chọn LambdaDynamoDBBackupRole Gỡ bỏ các policies: AmazonDynamoDBReadOnlyAccess AmazonS3FullAccess AWSLambdaBasicExecutionRole Chọn Delete role Xác nhận xóa 8. Xóa CloudWatch Logs Vào AWS Console → CloudWatch Chọn Logs → Log groups Tìm và xóa các log groups liên quan: /aws/lambda/DynamoDB_API_Handler /aws/lambda/AutoImportCSVtoDynamoDB /aws/lambda/DatabaseBackupFunction /aws/apigateway/FlyoraAPI Chọn log group → Actions → Delete log group(s) Xác nhận xóa 9. Xóa X-Ray Traces (Tùy chọn) X-Ray traces sẽ tự động hết hạn sau 30 ngày và không tính phí lưu trữ, nhưng bạn có thể xóa thủ công nếu muốn.\nVào AWS Console → X-Ray Chọn Traces Traces sẽ tự động bị xóa sau thời gian lưu trữ mặc định 10. Xóa RDS và Subnet groups Vào subnet group chọn subnet group đã tạo và ấn delete Vào database ấn vào database đã tạo → Action → Delete 11. Xóa Lambda BirdShopChatBot và layer Vào function chọn BirdShopChatBot → Action → Delete Vào Layers chọn layer đã tạo ấn delete 12. Xóa VPC và NAT gateway và Elastic IP, EC2 Vào VPC chọn NAT gateway đã tạo → Action → Delete NAT gateway Chọn Elastic IP → Action → Release Elastic IP addresses Sau khi xóa xong NAT gateway và Elastic IP qua phần Your VPCs ấn VPC đã tạo → Action → Delete VPC Qua phần EC2 chọn Instances chọn EC2 đã tạo → Instances state → Terminate instances 13. Xóa Cloudfront Vào CloudFront, chọn phân phối đã tạo → Actions → Disable. Chờ cho đến khi trạng thái chuyển sang Disabled. Select the checkbox for the disabled distribution again. Choose Delete and confirm the deletion. The distribution cannot be recovered once deleted. Kiểm tra lại Sau khi hoàn tất các bước trên, hãy kiểm tra lại các dịch vụ sau để đảm bảo không còn tài nguyên nào:\n✅ EventBridge: Không còn rule nào ✅ API Gateway: Không còn API nào ✅ Lambda: Không còn function nào (3 functions) ✅ DynamoDB: Không còn bảng nào ✅ S3: Không còn bucket nào (2 buckets) ✅ Cloudfront: Không còn cấu hình phân phối nội dung ✅ IAM Users: Không còn user test ✅ IAM Roles: Không còn 3 roles đã tạo ✅ CloudWatch Logs: Không còn log groups liên quan ✅ X-Ray: Traces sẽ tự động hết hạn ✅ RDS: Xóa thành công ✅ NAT gateway: Không còn nữa ✅ Elastic IP: Không còn nữa ✅ EC2 : Đã được terminate Hãy chắc chắn rằng bạn đã xóa tất cả các tài nguyên để tránh phát sinh chi phí không mong muốn. Đặc biệt chú ý xóa S3 buckets vì chúng có thể tích lũy dữ liệu theo thời gian.\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/2-backend/2.6/","title":"S3 CSV Backup","tags":[],"description":"","content":"S3 CSV Backup Tạo IAM Role cho Lambda Vào AWS Management Console → tìm IAM. Chọn Roles → Create Role. Chọn Trusted entity type: AWS service. Chọn Use case: Lambda, sau đó nhấn Next. Gán quyền truy cập cho Role Gắn các policy sau:\nAmazonDynamoDBReadOnlyAccess AmazonS3FullAccess AWSLambdaBasicExecutionRole Nhấn Next, đặt tên role là LambdaDynamoDBBackupRole.\nRole này cho phép Lambda quét toàn bộ bảng DynamoDB và lưu bản backup dưới dạng CSV vào S3 Bucket.\nTạo S3 Bucket Truy cập dịch vụ S3. Trong giao diện S3, chọn Create bucket. Trong màn hình Create bucket:\nBucket name: Nhập tên, ví dụ:\nflyora-bucket-backup (Nếu tên đã tồn tại, hãy thêm số phía sau.)\nGiữ nguyên các thiết lập mặc định còn lại.\nXem lại cấu hình và chọn Create bucket để hoàn tất. Cấu hình Lambda Trigger cho S3 Tạo Lambda Function Truy cập Lambda → Create function. Chọn Author from scratch. Đặt tên: DatabaseBackupFunction. Runtime: Python 3.14. Role: chọn LambdaDynamoDBBackupRole đã tạo ở bước trước. Vào Configuration → Environment variables Chọn Edit. Add environment variable Key: BUCKET_NAME Value: flyora-bucket-backup Chọn Save. Code\nimport boto3 import csv import io import os from datetime import datetime from boto3.dynamodb.conditions import Key s3 = boto3.client(\u0026#39;s3\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) def scan_all(table): \u0026#34;\u0026#34;\u0026#34;Scan toàn bộ bảng DynamoDB, xử lý paging.\u0026#34;\u0026#34;\u0026#34; items = [] response = table.scan() items.extend(response.get(\u0026#39;Items\u0026#39;, [])) while \u0026#39;LastEvaluatedKey\u0026#39; in response: response = table.scan(ExclusiveStartKey=response[\u0026#39;LastEvaluatedKey\u0026#39;]) items.extend(response.get(\u0026#39;Items\u0026#39;, [])) return items def lambda_handler(event, context): bucket = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] tables = [t.strip() for t in os.environ[\u0026#34;TABLE_LIST\u0026#34;].split(\u0026#34;,\u0026#34;)] timestamp = datetime.utcnow().strftime(\u0026#34;%Y-%m-%d-%H-%M-%S\u0026#34;) for table_name in tables: try: table = dynamodb.Table(table_name) data = scan_all(table) if not data: print(f\u0026#34;Table {table_name} EMPTY → skip\u0026#34;) continue # Lấy danh sách tất cả fields all_keys = sorted({key for item in data for key in item.keys()}) # Convert to CSV csv_buffer = io.StringIO() writer = csv.DictWriter(csv_buffer, fieldnames=all_keys) writer.writeheader() for item in data: writer.writerow({k: item.get(k, \u0026#34;\u0026#34;) for k in all_keys}) key = f\u0026#34;dynamo_backup/{table_name}/{table_name}_{timestamp}.csv\u0026#34; s3.put_object( Bucket=bucket, Key=key, Body=csv_buffer.getvalue().encode(\u0026#34;utf-8\u0026#34;) ) print(f\u0026#34;Backup xong bảng {table_name} → {key}\u0026#34;) except Exception as e: print(f\u0026#34;Lỗi khi backup bảng {table_name}: {e}\u0026#34;) return { \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tables\u0026#34;: tables } Chọn Deploy. Cấu hình chạy tự động (Schedule) Lambda → Triggers → Add Trigger → EventBridge (Schedule)\nRule name: DatabaseBackup\nRule description: AutoBackup in 4 days\nRule type: Schedule expression\nSchedule expression: rate(4 days)\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/7-feedback/","title":"Chia sẻ, đóng góp ý kiến","tags":[],"description":"","content":" Tại đây bạn có thể tự do đóng góp ý kiến cá nhân về những trải nghiệm khi tham gia chương trình First Cloud Journey, giúp team FCJ cải thiện những vấn đề còn thiếu sót dựa trên các hạng mục sau:\nĐánh giá chung 1. Môi trường làm việc\nMôi trường làm việc tại FCJ mang lại cảm giác chuyên nghiệp nhưng vẫn rất gần gũi. Không khí làm việc thoải mái, thân thiện, mọi người sẵn sàng hỗ trợ nhau, kể cả bên ngoài giờ làm việc. Không gian làm việc được bố trí gọn gàng, hiện đại, giúp mình dễ tập trung và làm việc hiệu quả hơn mỗi ngày.\n2. Sự hỗ trợ của mentor / team admin\nMentor luôn giải thích cặn kẽ từng vấn đề, kiên nhẫn hướng dẫn các bước thực hiện và đưa ra những gợi ý giúp mình tự tìm ra cách giải quyết. Team admin hỗ trợ rất nhanh chóng, đặc biệt trong các công việc liên quan đến tài liệu, quy trình và thông tin cần thiết, giúp mình không bị gián đoạn trong quá trình học hỏi.\n3. Sự phù hợp giữa công việc và chuyên ngành học\nCác nhiệm vụ được giao phù hợp với kiến thức căn bản mình đã học nhưng vẫn đủ thử thách để mình mở rộng kỹ năng. Nhiều công nghệ và quy trình mới mà ở trường chưa được tiếp cận đã giúp mình bổ sung thêm kiến thức thực tế, kết nối giữa lý thuyết và ứng dụng trong doanh nghiệp.\n4. Tổ chức sự kiện\nFCJ thường xuyên tổ chức các sự kiện, workshop và buổi chia sẻ nội bộ giúp tăng cường kiến thức cũng như kết nối giữa các thành viên. Những sự kiện này được chuẩn bị chu đáo, nội dung bổ ích, tạo không gian để mọi người giao lưu, học hỏi và thư giãn sau giờ làm việc. Đây là điểm cộng lớn vì giúp thực tập sinh hòa nhập nhanh hơn và cảm thấy mình là một phần của cộng đồng.\n5. Văn hóa \u0026amp; tinh thần đồng đội\nVăn hóa tại FCJ rất tích cực: tôn trọng – hỗ trợ – cùng phát triển. Mọi người luôn sẵn sàng chia sẻ kiến thức, góp ý một cách chân thành và giúp đỡ nhau khi dự án gấp. Tinh thần làm việc nhóm rất cao, tạo cảm giác mình không bị lạc lõng dù chỉ là một thực tập sinh.\nMột số câu hỏi khác Điều bạn hài lòng nhất trong thời gian thực tập? Mình hài lòng nhất về môi trường học hỏi năng động, mentor nhiệt tình,tài liệu nội bộ đầy đủ cho người mới và cơ hội tham gia các sự kiện và workshop đầy ý nghĩa . Điều bạn nghĩ công ty cần cải thiện cho các thực tập sinh sau? Hiện tại em chưa có đề xuất cải thiện nào. Quy trình thực tập của công ty đã hỗ trợ em rất tốt và em cảm thấy hài lòng. Nếu giới thiệu cho bạn bè, bạn có khuyên họ thực tập ở đây không? Vì sao? Có. Vì đây là môi trường phù hợp để phát triển kỹ năng thật, có một cộng đồng biết giúp đỡ lẫn nhau để tiến bộ hơn. Đề xuất \u0026amp; mong muốn Bạn có đề xuất gì để cải thiện trải nghiệm trong kỳ thực tập? Hiện tại em chưa có đề xuất gì thêm. Trải nghiệm thực tập của em khá tốt và em cảm thấy hài lòng với sự hỗ trợ từ các anh chị. Bạn có muốn tiếp tục chương trình này trong tương lai? Mình muốn tiếp tục tham gia hoặc đồng hành cùng chương trình trong tương lai vì đây là môi trường tốt để phát triển bản thân. Góp ý khác (tự do chia sẻ): "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/5-workshop/2-backend/2.7/","title":"Tích hợp aws x-ray","tags":[],"description":"","content":"Mục tiêu Sử dụng AWS X-Ray để theo dõi và kiểm thử toàn bộ luồng xử lý của API Gateway → Lambda → DynamoDB.\nX-Ray giúp quan sát trace, latency, lỗi, và các đoạn (segment/subsegment) để đảm bảo API hoạt động đúng và tối ưu.\nCác bước thực hiện 1. Truy cập dịch vụ IAM Vào AWS Console → IAM Chọn Roles → LambdaAPIAccessRole Chọn Add permissions → Attach policies → AWSXRayDaemonWriteAccess 2. Truy cập Lambda Vào AWS Console → Lambda Chọn Functions → DynamoDB_API_Handler -Configuration → Monitoring and operations tools → Additional monitoring tools → Edit Lambda service traces: Tick Enable 3. Truy cập dịch vụ Api gateway Vào AWS Console → Api gateway Chọn APIs → FlyoraAPI Stages → Logs and tracing → Edit Tick X-Ray tracing 4. Kiểm thử Vào AWS Console → Lambda Ở mục Test, Create new event\nEvent name: test\nDán mục dưới đây vào Event JSON:\n{ \u0026#34;resource\u0026#34;: \u0026#34;/{myProxy+}\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/api/v1/bird-types\u0026#34;, \u0026#34;httpMethod\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;headers\u0026#34;: {}, \u0026#34;multiValueHeaders\u0026#34;: {}, \u0026#34;queryStringParameters\u0026#34;: {}, \u0026#34;multiValueQueryStringParameters\u0026#34;: {}, \u0026#34;pathParameters\u0026#34;: {}, \u0026#34;stageVariables\u0026#34;: {}, \u0026#34;requestContext\u0026#34;: { \u0026#34;identity\u0026#34;: {} }, \u0026#34;body\u0026#34;: null, \u0026#34;isBase64Encoded\u0026#34;: false } Save -\u0026gt; Test\nTiếp theo vào AWS Console → X-ray Ở mục Traces, Xuất hiện id mới Bấm vào, xem chi tiết các trace đó\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/vi/tags/","title":"Tags","tags":[],"description":"","content":""}]