[{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Centralized Multi-Account Application Resilience Assessment Using AWS Resilience Hub By Venkata Kommuri, Venkata Moparthi, Santosh Vallurupalli, and Tracy Honeycutt on 05 AUG 2025 in Management Tools\nIntroduction As organizations scale their cloud environments across multiple AWS accounts and regions, managing and accessing resilience becomes increasingly complex. Traditional approaches of evaluating resilience separately for each workload, account, or region can lead to inefficiencies, inconsistencies, and coverage gaps. This challenge is particularly pronounced in distributed architectures utilizing various Infrastructure as Code (IaC) tools like AWS CloudFormation and Terraform.\nAWS Resilience Hub addresses these challenges by providing a centralized, managed service for monitoring and assessing resilience across multi-account cloud architectures. The service enables organizations to define, track, and enhance application resilience through unified oversight in a centralized hub master account. Integration with Amazon Simple Notification System (SNS) provides real-time drift detection notifications, helping maintain robust recovery capabilities across distributed deployments while supporting requirements for low latency, data residency compliance, and strengthened resilience against disruptions.\nUnderstanding AWS Resilience Hub AWS Resilience Hub is a central location in the AWS console for you to manage and improve the resilience posture of your applications on AWS. AWS Resilience Hub enables you to define your resilience goals, assess your resilience posture against those goals, and implement recommendations for improvement based on the AWS Well-Architected Framework.\nResiliency can be defined in terms of metrics called RTO (Recovery Time Objective) and RPO (Recovery Point Objective). RTO is a measure of how quickly your application can recover after an outage and RPO is a measure of the maximum amount of data loss that your application can tolerate. AWS Resilience Hub offers a unified view of an application\u0026rsquo;s resilience posture, identifies potential risks and provides actionable recommendations to enhance robustness against disruptions. By aligning with the AWS Well-Architected Framework\u0026rsquo;s best practices, Resilience Hub ensures that the applications can withstand and recover from various failure scenarios.\nChallenges with Disaggregated Resilience Assessments Below outlined are some of the key challenges in detail with disaggregated assessment approach:\nFragmented Resilience Evaluations Organizations managing workloads across multiple AWS accounts often conduct resilience assessments independently, leading to inconsistencies in evaluation criteria and risk identifications. Without a unified view, teams may struggle to correlate resilience findings across workloads leading to blind spots in overall application resilience.\nLack of Standardized Resilience Policies Without standardized resilience policies, different teams may define availability targets, RTO and RPO differently. This inconsistency can lead to misalignment between business continuity planning, and actual cloud infrastructure capabilities.\nDifficulty in Data Aggregation Aggregating resilience data from disparate sources poses another significant challenge. Without a unified platform, administrators must manually compile details of the infrastructure deployed from tools like AWS CloudFormation or Terraform state files across Regions and accounts. This labor-intensive process can introduce errors and delays, making it harder to spot failure points spanning multiple accounts, thus prolonging exposure to potential disruptions.\nIn this blog post, we will explore an architecture that addresses the challenges discussed above. This architecture will help you conduct resilience assessments for applications deployed across multiple AWS accounts and Regions.\nPrerequisites An AWS Account AWS CloudFormation templates or Terraform files defining your applications Pre-defined Recovery Time Objective (RTO) and Recovery Point Objective (RPO) guidelines as per your business requirements Solution Architecture The AWS Resilience Hub Solution employs a hub and spoke model to deliver centralized resilience management across an AWS environment, enabling organizations to assess and enhance application resilience effectively. This solution operates through a central AWS hub account, which acts as the administrative focal point for defining resilience policies, managing assessments, overseeing Applications across connected spoke accounts. The hub account connects to multiple spoke accounts through IAM roles and trust relationships. These spoke accounts contain the workloads and resources that need resilience evaluation.\nAWS Resilience Hub requires two IAM roles for secure cross-account resilience assessment. The primary role AWSResilienceHubAssessmentRole in the hub account manages assessment operations, while the secondary role AWSResilienceHubCrossAccountRole in spoke accounts executes workload evaluations. The hub account\u0026rsquo;s assessment role orchestrates resilience assessments by managing configurations, assuming cross-account roles, and coordinating workload evaluations.\nCustomers can leverage AWS Resilience Hub\u0026rsquo;s drift detection capabilities to automatically assess their workloads on a daily basis. This assessment occurs once every 24 hours. For enhanced monitoring and response, customers have the option to integrate these drift detection assessment results with an SNS topic. By doing so, relevant teams can subscribe to these notifications, allowing them to receive timely alerts and take corrective actions promptly when deviations are detected. This integration enables a proactive approach to maintaining application resilience and ensures that any changes affecting the resilience posture are addressed swiftly.\nFig 1 – Architectural diagram of the solution centralizes Multi-Account Application Resilience Assessment Using AWS Resilience Hub\nKey Components of the Solution Below are the building blocks of the solution:\nCentral Hub Account Serves as the administrative center for AWS Resilience Hub where administrators can manage resilience policies, view assessment results, coordinate resilience strategies across multiple accounts and applications. This account also includes a SNS topic that receives drift notifications, enabling real-time alerts for subscribed users.\nSpoke Accounts Contain the workloads and resources evaluated by AWS Resilience Hub, connecting to the central hub account through the IAM roles and trust relationships to enable resilience assessments.\nIAM Roles AWSResilienceHubAssessmentRole Resides in the central hub account; Manages overall configurations, creates resilience policies, sets up applications for assessments and assumes cross account roles in the spoke accounts to delegate tasks securely.\nAWSResilienceHubCrossAccountRole Located in spoke accounts; performs workload assessments, executes resource level operations, and reports findings back to the hub\u0026rsquo;s admin role, ensuring detailed resilience evaluation.\nTrust Relationships Enable the admin role in the hub account to delegate tasks to the executor role in spoke accounts, maintaining a clear separation of duties while facilitating efficient resilience management and assessment across the AWS environments.\nGetting Started Download the necessary CloudFormation files wget \\ https://d2908q01vomqb2.cloudfront.net/artifacts/MTBlog/cloudops-1962/ard.yaml \\ https://d2908q01vomqb2.cloudfront.net/artifacts/MTBlog/cloudops-1962/AWSResilienceHubAssessmentRole.yaml \\ https://d2908q01vomqb2.cloudfront.net/artifacts/MTBlog/cloudops-1962/AWSResilienceHubCrossAccountRole.yaml \\ https://d2908q01vomqb2.cloudfront.net/artifacts/MTBlog/cloudops-1962/AWSResilienceHubSNSTopic.yaml Deploy the IAM Roles for Central Hub Account and Spoke Accounts In the hub account run the below command:\naws cloudformation create-stack \\ --template-body AWSResilienceHubAssessmentRole.yaml \\ --stack-name AWSResilienceHubAssessmentRole \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM In each spoke account(s) run below commands:\naws cloudformation create-stack \\ --template-body AWSResilienceHubCrossAccountRole.yaml \\ --stack-name AWSResilienceHubCrossAccountRole \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \\ --parameter-overrides AWSResilienceHubAssessmentRoleARN=arn:aws:iam::\u0026lt;hubaccount_number\u0026gt;:role/AWSResilienceHubAssessmentRole Create an SNS topic to get notified when the application detects the drift This CloudFormation template sets up an SNS topic with its associated access permissions. The template configures both a new SNS topic and establishes the necessary access controls (Resource Policy) that define which services and users can interact with the topic. Specifically, this configuration allows AWS Resilience Hub to send notifications about drift detection assessment results to the SNS topic, helping monitor changes in your resilience policies.\naws cloudformation create-stack \\ --template-body AWSResilienceHubSNStopic.yaml \\ --stack-name AWSResilienceHubSNSTopic \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM \\ --parameter-overrides TopicName=dev-sample-topic EmailAddress=\u0026lt;Enter_Email_Address_of_Recipeint\u0026gt; Deploy Sample Workload in Spoke Account aws cloudformation create-stack \\ --template-body ard.yaml \\ --stack-name resilience-hub-workloads \\ --capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM Copy the ARN of the CloudFormation Stack that was just launched for later use:\naws cloudformation describe-stacks --stack-name resilience-hub-workloads --query \u0026#34;Stacks[0].StackId\u0026#34; --output text To illustrate the functionality of cross-account workload assessment using Resilience Hub, we will deploy a sample workload in a Spoke account and analyze its assessment results from a Central Account. The sample application we\u0026rsquo;ll use features a three-tier architecture with services deployed into a single AZ wherever applicable, comprising an Amazon CloudFront for content delivery and caching, Amazon Simple Storage Service (Amazon S3) bucket for object storage, an Amazon Elastic Compute Cloud (Amazon EC2) instance serving as a web server, and an Amazon Relational Database Service (Amazon RDS) instance for database operations. The core objective of this blog post is to demonstrate how Resilience Hub assesses the resilience of this architecture and to explore how we can enhance it based on the recommendations provided by the tools. Please note that the sample architecture does not follow AWS best practices and is used here to demonstrate how AWS Resilience Hub identifies resiliency gaps and generates recommendations for improvements.\nFig 2 – Architectural diagram of sample three-tier workload\nSetting Up Centralized Resilience Assessment in the Hub Account To start, login to the AWS hub account and navigate to the AWS Management Console.\nSelect AWS Resilience Hub and select Add application → Get started, choose Add application.\nSelect Add Application under Resilience management. Enter application details and select Resource collection. Add the AWS CloudFormation stack ARN of your sample workload that you have copied from Step 3 above (Deploy Sample Workload in Spoke Account).\nDefine the new resilience policy for the workload. For this sample workload, we will set \u0026ldquo;MissionCritical\u0026rdquo; policy with RTO and RPO set to 10 minutes and 4 minutes. If you already have pre-defined Resiliency Policies defined in your account, you can choose to use them by checking the option \u0026ldquo;Choose an existing resiliency policy\u0026rdquo;. For multi-account assessment, select AWSResilienceAssessmentRole from drop down and under Add cross account IAM role(s) ARN, add the ARN for AWSResilienceCrossAccountRole from the spoke account(s).\nNote: If deploying workloads across multiple spoke accounts, add the AWSResilienceCrossAccountRole ARN from each spoke account to enable cross-account resilience capabilities.\nBy selecting the \u0026ldquo;Automatically assess daily\u0026rdquo; option, you can set up daily automated evaluations of your workloads specified in CloudFormation and Terraform templates. Additionally, you have the option to receive email notifications through your subscribed SNS topic. These notifications will inform you of any drift detection assessment results, alerting you to changes in your applications\u0026rsquo; resilience posture over time. Checkbox the \u0026ldquo;Get Notified when the application drifts\u0026rdquo; and select the SNS topic that is created in Step 2. Now, select Add application. Once the application is successfully imported from AWS CloudFormation template, select Publish application. After publishing your application, it is now ready for assessment. Running a Resilience Assessment Navigate to the Resources tab to view deployed components from the AWS CloudFormation template for sample workload in spoke account. Now, you can select Assess application to start the application assessment. Name your assessment report and select Run. The assessment status will be in progress and will finish the assessment in a few minutes. Reviewing the Resilience Assessment Once the assessment is completed, the report is generated for review.\nYou can find assessment reports of your application in the Assessments section of your application. In the left navigation menu, choose Applications. In Applications, open an application. In Assessments tab, choose an assessment report from the Resiliency assessments section. When you open the report, you see the following:\nAn overall overview of the assessment report Recommendations to improve resiliency Recommendations to set up alarms, Standard Operating Procedures (SOPs), and tests Upon examining the report above, you\u0026rsquo;ll observe that the \u0026ldquo;Current Policy Compliance\u0026rdquo; section indicates that the existing infrastructure falls short of meeting the Recovery Time Objective (RTO) and Recovery Point Objective (RPO) defined in your resilience policy. To address this, AWS Resilience Hub offers valuable recommendations, enabling you to implement corrective measures and enhance your application\u0026rsquo;s resilience to align with your established objectives. For instance, selecting the RDS AppComponent will reveal a detailed set of recommendation assessments, as illustrated in the following image.\nWith these resilience recommendations in hand, application and infrastructure teams can now implement appropriate changes to their systems, enhancing their ability to withstand unexpected failures. This process demonstrates the power of AWS Resilience Hub\u0026rsquo;s cross-account functionality—while the sample workload is deployed in a spoke account, the assessment is conducted in the central hub account. This centralized approach allows for consolidated assessment reports to be available in the central hub account, providing a comprehensive view of resilience across multiple accounts and applications. This centralized oversight enables organizations to maintain consistent resilience standards and efficiently manage their multi-account AWS environments.\nConclusion The solution provides a centralized approach to assessing and improving workload resilience across multiple AWS accounts and regions, which is critical for maintaining business continuity and meeting business requirements. It integrates seamlessly with applications deployed via AWS CloudFormation and Terraform deployments, utilizing a hub and spoke model that enables organizations to efficiently evaluate resilience postures while maintaining security through IAM Role-based access controls.\nThe ability to assess stacks deployed outside of the AWS region of the central hub account, combined with cross-account IAM roles integration, enhances the service\u0026rsquo;s effectiveness by allowing organizations to gain visibility into global workloads and ensure secure, scalable assessments. By adopting AWS Resilience Hub, organizations can transform resilience management from a reactive process into a proactive strategy, leveraging automated assessments, detailed recommendations, and clear resilience metrics to build fault-tolerant, highly available applications that align with best practices for disaster recovery and operational resilience, making it an essential tool for maintaining a resilient, well-architected AWS environment as cloud infrastructures grow in complexity.\nTo get started with implementing this solution in your organization, download the CloudFormation templates provided in this post and follow our step-by-step deployment guide in the AWS Resilience Hub documentation. You can also explore additional resilience assessment strategies through the AWS Well-Architected Framework.\nAuthors Bio TAGS: AWS Resilience Hub, Resiliency\nVenkata Kommuri Venkata Kommuri, an Infrastructure Architect at AWS, specializes in designing and implementing complex cloud solutions across commercial and GovCloud environments. His expertise spans cloud migrations, containerization, networking, and Generative AI deployments, delivering scalable and secure enterprise architectures that drive digital transformation while maintaining operational excellence. Venkata Moparthi Venkata Moparthi is a Senior Solutions Architect at AWS, specializing in cloud migrations, generative AI, and resilience. As a trusted technology leader, he architects transformative cloud solutions for financial services and cross-industry enterprises, consistently delivering optimized outcomes that align technical innovation with business objectives. Santosh Vallurupalli Santosh Vallurupalli is a Sr. Solutions Architect at AWS. Santosh specializes in networking, containers, and migrations and enjoys helping customers in their journey of cloud adoption and building cloud native solutions for challenging issues. Tracy Honeycutt Tracy Honeycutt is a Solutions Architecture Manager based in Atlanta, GA. Tracy is focused on guiding Financial Services Industry (FSI) customers through their cloud journeys, accelerating migrations, modernizing workloads, and adopting new ways of working. Tracy has a specialty in networking and is passionate about networking, DNS, and helping customers who are early in their cloud journeys. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Implement a USDC bridge on AWS By Corey Cooper and Guillaume Goutaudier on 09 JUN 2025 in Advanced (300), AWS Step Functions, Blockchain, Technical How-to\nIntroduction Stablecoins offer significant advantages in the crypto space. They provide price stability and can serve as a reliable medium of exchange, store of value, or bridge between the fiat and crypto ecosystems. The ability to transfer stablecoins across multiple blockchains further enhances their utility by improving cross-chain interoperability and letting users take advantage of the strengths of different networks.\nCircle is a financial platform that offers regulated stablecoins, tokenized funds, developer services, and liquidity services to help businesses deliver the future of finance across multiple blockchains. Circle is widely known for its prominent dollar stablecoin, USDC.\nWith Circle\u0026rsquo;s recent release of its interoperability service, CCTP (Cross-Chain Transfer Protocol) V2, we wanted to explore how USDC could be transferred between multiple chains using a serverless architecture on AWS.\nIn this post, we describe and deploy such an architecture. We focus on the backend components, using the Quickstart: Cross-Chain USDC Transfer from the Circle developer documentation as a reference. We illustrate how to use serverless AWS services such as AWS Lambda and AWS Step Functions to implement onchain workflows.\nThis solution is particularly valuable for businesses building multi-chain DeFi platforms, NFT marketplaces, or cross-border payment systems. By using AWS serverless services, developers can reduce infrastructure complexity, speed up integration of cross-chain transfers, and maintain high availability and scalability with minimal operational overhead.\nSolution overview The high-level architecture of the solution is shown in the following diagram.\nThe workflow is implemented as a Step Functions state machine that orchestrates the execution of multiple Lambda functions. These Lambda functions interact with the source and target networks, the Circle Attestation Service (which observes and verifies the burn activity on the source chain), and use AWS Secrets Manager to store the private keys of the account from which we want to transfer USDC.\nStep Functions offers several benefits, including built-in error handling and timeouts, direct integration with AWS services, built-in state management and workflow progress tracking, and real-time and auditable workflow execution history.\nThe following figure illustrates the state machine workflow for a CCTP V2 Fast Transfer.\nThe workflow implements the required steps for using CCTP V2 (for a thorough understanding of these steps, refer to the CCTP developer documentation):\nValidate the input variables. Generate a bytes32 representation of the address where the USDC are to be sent. Approve the spending of the required amount of USDC on the TokenMessengerV2 smart contract. Burn USDC on the source network using the TokenMessengerV2 smart contract. Retrieve a message and attestation from the Circle Attestation Service. Mint USDC on the target network using the MessageTransmitterV2 smart contract (getting a new attestation if required). At the time of writing, CCTP V2 supports Ethereum, Avalanche, Base, Linea, and Arbitrum networks. We therefore focus the implementation on EVM-compatible blockchains.\nThe workflow is implemented on test networks using endpoints from Public Node. For production use, the following should be considered (at the minimum):\nThe management of the private keys should be given additional considerations. Refer to How to sign Ethereum EIP-1559 transactions using AWS KMS to learn how additional AWS security services such as AWS Key Management Service (AWS KMS) can improve your security posture. Multi-party authorization can also be implemented for higher-value transfers.\nYou might want to run your own nodes. You can refer to AWS Blockchain Node Runners for node deployment blueprints and to the AWS Nitro Enclaves for running Ethereum validators blog post series for detailed instructions on how to sign transactions inside a Nitro enclave.\nYou should attach the Lambda functions to a VPC. Refer to Giving Lambda functions access to resources in an Amazon VPC for more details.\nFor each step of the workflow, you should review all possible types of failure and confirm how you want to handle them. For example, the uptime and availability of the CCTP service are designed to be similar to how the existing minting services operate today, but you should still define how to handle delayed transactions or temporary unavailability of the Attestation Service.\nThe solution can also be integrated into a wider decentralized application. For example, you can use Amazon Simple Notification Service (Amazon SNS) to send notifications to end-users, and use Amazon EventBridge to trigger additional post-transfer actions.\nTo deploy this solution, we use the following AWS CloudFormation template. The template requires a private key as input parameter, so we generate one before deploying the template. We then test an actual USDC transfer from Ethereum to Base.\nPrerequisites To implement this solution, you should have an AWS account with the appropriate permissions.\nCreate a private key There are many different ways to generate a private key. For this post, we use the Python Web3 library from AWS CloudShell. To generate a private key and associated address, connect to CloudShell and run the following command:\npip install web3 \u0026amp;\u0026amp; \\ python -c \u0026#34;from web3 import Web3; w3 = Web3(); acc = w3.eth.account.create(); print(f\u0026#39;PRIVATE_KEY={acc._private_key.hex()}; ADDRESS={acc.address}\u0026#39;)\u0026#34; Execute the output of the previous command to record the PRIVATE_KEY and ADDRESS.\nDeploy the CloudFormation template From the same CloudShell session, run the following command to deploy the CloudFormation template:\naws cloudformation create-stack --region us-east-1 --stack-name \u0026#39;USDC-Bridge\u0026#39; --template-url \u0026#39;https://aws-blogs-artifacts-public.s3.us-east-1.amazonaws.com/artifacts/WEB3-2/USDC-Bridge-CloudFormation-Template.yml\u0026#39; --capabilities CAPABILITY_NAMED_IAM --parameters ParameterKey=PRIVATEKEY,ParameterValue=$PRIVATE_KEY Test the solution To be able to transfer USDC from one network to the other, you need native tokens (ETH for Ethereum, for example) from the source and target networks (so you can pay for the gas fees) and USDC on the source network (so you have USDC to transfer in the first place). Because we deployed the solution for testnets, those can be obtained from faucets. The following is a non-exhaustive list of faucets you may want to use:\nUSDC Testnet Faucet from Circle Sepolia Faucets from Chainlink Ethereum Sepolia Faucet from Alchemy Multi-Chain Faucet from QuickNode Chainstack multi-chain faucet Free testnet faucets from thirdweb In this section, we illustrate how we transferred USDC from Ethereum to Base, but you can experiment with other networks. To execute the Step Functions state machine, we can prepare the following input parameters:\n{ \u0026#34;source_address\u0026#34;: \u0026#34;\u0026lt;replace_with_public_address_from_previous_step\u0026gt;\u0026#34;, \u0026#34;target_address\u0026#34;: \u0026#34;\u0026lt;replace_with_public_address_from_previous_step\u0026gt;\u0026#34;, \u0026#34;amount\u0026#34;: 10000, \u0026#34;max_fee\u0026#34;: 100, \u0026#34;source_domain\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;target_domain\u0026#34;: \u0026#34;6\u0026#34; } We use the same source and target addresses, but you can use a different target address. Additionally, this solution is using Fast Transfer (for more details on Fast Transfers, refer to CCTP product fee schedule).\nTo execute the Step Functions state machine, complete the following steps:\nOn the Step Functions console, select the USDC_Bridge state machine. Choose Start execution. Insert the JSON document you previously prepared. Choose Start execution. If all goes well, you should see the different steps of the state machine being successfully executed until the end of the CCTP V2 Fast Transfer workflow.\nIf something goes wrong, you will be able to visually identify at which step the issue occurred, inspect the input and output of the step, and review the logs of the Lambda functions. Step Functions also provides the ability to resume the execution of the workflow after an initial failure, and integrates with Amazon CloudWatch for enhanced observability and alerting.\nAfter the workflow is successfully executed, you can confirm that USDC has been successfully transferred, and review the smart contracts executions using block explorers. In particular, you can note the transaction hash from the output of the Mint USDC step, and confirm that the logs of this transaction indicate that new USDC have been minted, as shown in the following screenshot.\nClean up From CloudShell, run the following command to clean up your resources:\naws cloudformation delete-stack --stack-name \u0026#39;USDC-Bridge\u0026#39; Conclusion In this post, we showed you how you can use CCTP V2, Lambda, Step Functions, and Secrets Manager to implement a serverless solution to transfer USDC on the backend of onchain applications. You can expand this solution and integrate it into a wider set of AWS tools and services, making it an ideal platform for running onchain workloads. We encourage you to experiment on your own and share your feedback.\nAbout the authors Corey Cooper Corey is a developer at Circle who loves basketball and is a lifelong Lakers fan from Atlanta, with a career spent building enterprise and payment technology. With over 15 years of experience as a solutions engineer, he brings a mix of technical depth, business insight, and hands-on leadership to product launches, go-lives, and scalable platform builds. Guillaume Goutaudier Guillaume is a Sr Enterprise Architect at AWS. He helps companies build strategic technical partnerships with AWS. He is also passionate about blockchain technologies, and is a member of the Technical Field Community for blockchain. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Strengthen Your AWS Cloud Storage Security with Superna Defender By Andrew Peng and Andrew MacKay on 07 JUL 2025 in Amazon FSx, Amazon FSx for Windows File Server, Amazon Simple Storage Service (S3), AWS Marketplace, Partner solutions, Software, Storage\nBy Andrew MacKay, CTO and Strategy Officer – Superna\nBy Andrew Peng, Sr. Specialist SA, FSx for Windows File Server – AWS\nIntroduction As customers migrate storage into Amazon Web Services (AWS), protecting file and object storage in the cloud is a security concern. Understanding and implementing a storage framework that includes robust security measures is critical to protect data from ransomware, unauthorized access, and exfiltration. Organizations are constantly looking for a solution that can provide protection and monitoring of their cloud based storage.\nIn this post, we will explore how Superna Defender can help to monitor and protect AWS storage resources at the object or file level in real-time, by providing the \u0026ldquo;who, what, and where\u0026rdquo; of every event. Superna Defender can provide a solution to help maintain alignment with Cyberstorage standards for object and file storage, and can also help to protect, monitor, and restore storage systems. Malware and ransomware threats to storage The threat of ransomware and malware to cloud storage is becoming increasingly complicated and sophisticated. Affected end user devices with authorized access to network shares can expose a large surface area to threats. Data loss, ransomware, and data exfiltration are possible if Amazon Simple Storage Service (Amazon S3) buckets have misconfigured security policies or infected devices access Amazon FSx for Windows File Server shares.\nThe damage from ransomware goes beyond immediate operational and data losses; it severely affects reputation long-term. A company\u0026rsquo;s failure to protect customer data may erode trust, causing customer attrition and affecting new business acquisition.\nMoving beyond Cloud Storage to Cyberstorage Malware events continue to be a constant threat—ransomware, zero-day exploits, and unintended access threats are lurking at every corner. Despite having traditional backup methodologies and tools in place, organizations still fall victim. This is where the concept of Cyberstorage comes into play.\nGartner created a new category called Cyberstorage to categorize products that actively defend storage and data against cyber events through prevention, early detection and event blocking. Superna has been recognized as a leader in this category, appearing in the Gartner unstructured data storage and endpoint protection hypercycle reports.\nOrganizations rely on a variety of security tools from endpoint protection, firewalls, identity access management, and threat detection. They also implement data management strategies like backups, snapshots, and immutability features. But with Cyberstorage, security can be taken a step further.\nWhen a cyber event occurs, Superna\u0026rsquo;s solution can detect threats in real-time on Amazon S3 and FSx for Windows File Server. Even stopping it immediately by cutting off access to at-risk users. Affected files are first isolated, then automatically recovered. Finally, a complete audit trail help provides the \u0026ldquo;who, what and where\u0026rdquo; for assisting in compliance and post-incident forensic investigations.\nThe Complex Security Landscape of Cyberstorage Figure 1 shows how Cyberstorage framework defines several layers for an overall storage architecture.\nFigure 1: Superna Defender operates at the Data layer\nConsider the typical enterprise security plan which includes components such as access controls, encryption, secure protocols, antivirus software, firewalls, and data loss prevention. These are often supplemented by auditing, penetration testing, backup and recovery mechanisms, and even physical security measures like security guards and locked doors.\nWhile this security landscape is robust, it\u0026rsquo;s also complex. A weakness at any point can have significant consequences. This leads us to a crucial question: it\u0026rsquo;s often not a matter of \u0026ldquo;if\u0026rdquo; data will be affected by a cyber event, but \u0026ldquo;when\u0026rdquo;. The effectiveness of an organization\u0026rsquo;s response is vital.\nSuperna Defender protects your object and file storage at the Data Layer, providing an additional \u0026ldquo;last line of defense\u0026rdquo; where production data resides. By integrating Cyberstorage framework concepts, organizations can bolster their security posture to make sure they are better equipped to help prevent and respond to unintended access.\nCriteria for the NIST Cyberstorage Checklist Superna Defender can also help customers comply with the points of the National Institute of Standards and Technology (NIST) Cyberstorage Checklist, as illustrated in Figure 2.\nFigure 2: NIST Cyberstorage Checklist, a part of the NIST Cybersecurity Framework\nIdentify To respond to suspicious activity, customers can use NIST cybersecurity models coupled with unique controls and rules for user and machine accounts.\nDetect Using machine learning, this product monitors customer\u0026rsquo;s offensive Cyberstorage, detects suspicious activity, and creates alerts and reports compatible with third-party software. It also provides simulated event capabilities to test incident response and security tool integrations.\nProtect Superna Defender offers protection against mass deletions, mass copies, and provides data loss prevention (DLP) and custom activity pattern detections with Active Auditor for AWS storage. Events can be forensically analyzed to provide the \u0026ldquo;who, what, and when\u0026rdquo; root-case analysis of Cyberstorage events. Suspicious activity exceeding defined limits can trigger automatic user account lockouts for immediate security or alert storage administrators or security teams for manual review to prevent unnecessary business disruption from potentially false positives. Learning mode avoids false positives and autoconfigures itself based on observing activity.\nRespond Central dashboards and alerts, powered by webhooks, a Zero Trust API, and integrations with ServiceNow, Splunk, Palo Alto Networks, and AWS Security Hub, provide seamless monitoring, reporting, and incident response. Automatically disabling user accounts, API keys, and access tokens can mitigate the effects of a Cyberstorage incident. Integrations with endpoint protection can extend responses to include host network isolation to further reduce the impact and spread.\nRecover Cyber Recovery Manager provides recovery through analytics of a cyber event to determine when an event has happened, and tracking which files were impacted during the cyber event. This information allows for precise cyber recovery, restoring only affected files while leaving others unharmed, which shortens recovery times and increases productivity. The advanced monitoring system tracks real-time and historical user activity to speed up and improve incident investigation accuracy. Users can immediately see relevant information about the incident to understand what happened and which users were involved. Users can also review affected files and prioritize which are restored first.\nArchitecture Overview of Superna Defender in AWS Superna Defender is a native AWS Marketplace deployment and leverages several AWS services as shown in Figure 3. These services run a real-time event processing engine in a pay-as-you-go based consumption model that can be stopped and restarted on demand. This solution uses Amazon EC2 Auto Scaling, VPCs, and security rules to monitor and protect Amazon S3 and Amazon FSx for Windows File Server storage.\nFigure 3: Superna Defender architecture diagram\nConclusion Superna Defender, integrating AWS and the NIST Cyberstorage Framework, provides a holistic cloud file and object storage security solution for customers. Superna Defender used with Amazon S3 or Amazon FSx for Windows File Server can provide real-time auditing and monitoring of object and file storage in AWS. This allows customers to see the \u0026ldquo;who, what, and where\u0026rdquo; of file system events, allowing surgical point-in-time recovery of affected data.\nGet started with Superna Defender on the AWS Marketplace, watch the video demonstration of Superna Defender, and read the Gartner Innovation Insight Report on how Cyberstorage mitigates the impact of Cyberattacks.\nCustomers should weigh the services they choose as their responsibilities vary depending on the services used, integrating those services into their IT environment, and applicable laws and regulations. Connect with Superna Superna – AWS Partner Spotlight Superna provides universal file and object auditing and threat mitigation for your hybrid cloud environments with a converged orchestration and security platform.\nContact Superna | Partner Overview | AWS Marketplace\nTAGS: Amazon FSx, AWS Partner Solution, Cloud Storage Security, Storage, Superna\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Event Report: \u0026ldquo;[AWS GenAI Builder Club] AI-Driven Development Life Cycle – Reimagining Software Engineering\u0026rdquo; Event Purpose The [AWS GenAI Builder Club] AI-Driven Development Life Cycle event was organized to help participants learn more about how artificial intelligence (AI) is transforming the entire modern software development process.\nSpecifically, the program aimed to:\nIntroduce the AI-DLC (AI-Driven Life Cycle) methodology – a new model that integrates AI into every phase of the software development lifecycle.\nClarify the role and benefits of AI in shortening time, optimizing processes, and improving software product quality.\nIntroduce the Kiro.dev platform – an AI IDE (Integrated Development Environment) supported by AWS, helping development teams transform from ideas to finished products more quickly.\nSpeaker List Toan Huynh - Specialist SA My Nguyen - Sr. Prototyping Architect, Amazon Web Services - ASEAN Highlighted Content AI-DLC Methodology – AI-Driven Software Development Approach The AI-DLC methodology is considered a breakthrough in software engineering, where AI not only supports programming but is integrated throughout the entire development lifecycle:\nFrom the ideation phase, AI helps suggest requirements and define project scope.\nDuring design and programming, AI assists in writing code, creating design documentation, and detecting errors early.\nDuring testing and deployment, AI can automate processes, predict risks, and propose improvements.\nKiro.dev – The AI IDE Kiro.dev was introduced as an AI-powered IDE (Integrated Development Environment with AI) that helps users create applications from design to working implementation using just prompts.\nNotable features include:\nAutomatic project structure generation including files like plan.md, /inception/ directory, and descriptive documents.\nTask management (AI tasks) according to plan, where each part can be assigned to AI for execution while humans supervise.\nFlexible interaction: if the prompt isn\u0026rsquo;t accurate, users can edit, regenerate, or switch to different AI models to achieve desired results.\nAmazon Q Developer Amazon Q Developer operates as an AI assistant integrated into AWS Console and VS Code with outstanding capabilities:\nNatural code generation and understanding: you can ask in natural language and Q Developer will generate complete code.\nProject context integration: Q Developer understands the entire codebase in the workspace, providing accurate context-specific answers.\nWhat Was Learned Understanding AI-DLC, Kiro.dev, and Amazon Q Developer AI-DLC is a methodology: a holistic approach to software development with AI at every stage. Kiro.dev is the AI-DLC execution platform: a tool supporting rapid application development based on ideas. Amazon Q Developer: is an AI assistant specifically for developers, helping accelerate development and deployment directly on AWS environments. Using Prompt Templates and Managing AI Output Learning to design effective prompts is a key skill:\nClearly define the role that AI will assume.\nWrite clear plans and AI tasks.\nManage AI results through the /inception/ directory for easy refinement.\nAlways review output because AI can generate incorrect or inappropriate content.\nHuman-AI Collaboration Mindset Speakers emphasized that AI is a collaborator, not a replacement.\nAI can generate ideas, write code, and test, but humans are responsible for verification, evaluation, and direction.\nThis opens up a new mindset in software development – where AI and humans co-create together.\nEvent Experience Learning from Highly Skilled Speakers Speakers from AWS and major technology organizations shared best practices in modern application design. Through real case studies, I learned more about how to apply Domain-Driven Design (DDD) and Event-Driven Architecture to large projects. Applying Modern Tools Directly explored Amazon Q Developer, an AI tool supporting SDLC from planning to maintenance. Learned how to automate code transformation and pilot serverless with AWS Lambda, thereby improving development productivity. Lessons Learned AI-DLC and tools like Kiro.dev or Amazon Q Developer are changing how we program.\nThanks to AI, the process from idea → design → deployment can be significantly shortened.\nLearning to communicate with and control AI will become a core skill for modern developers.\nThe future of software is not \u0026ldquo;AI replacing humans,\u0026rdquo; but \u0026ldquo;humans + AI = double productivity.\u0026rdquo;\nEvent Photos "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Event Report: \u0026ldquo;AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS – Generative AI with Amazon Bedrock\u0026rdquo; Event Purpose Introduce an overview of AWS\u0026rsquo;s AI/ML/GenAI ecosystem. Provide an overview of Managed AI Services on AWS, their uses, and pricing models. Learn Prompt Engineering techniques applied to real-world use cases. Explore Retrieval-Augmented Generation (RAG) architecture. Introduce Foundation Models and popular types of Foundation Models (FMs). Speaker List Lam Tuan Kiet – Sr DevOps Engineer, FPT Software Danh Hoang Hieu Nghi – AI Engineer, Renova Cloud Dinh Le Hoang Anh – Cloud Engineer Trainee, First Cloud AI Journey Highlighted Content Generative AI with Amazon Bedrock Introduction to popular Foundation Models (Claude, Llama, Titan) and selection criteria based on use cases: text analysis, chatbots, reasoning, content creation, and embedding. Practice with Prompting Techniques: Zero-shot: model responds directly without examples → suitable for simple questions. Few-shot: provide sample examples for the model to learn patterns → increases accuracy in specialized tasks. Chain-of-Thought (CoT): requires the model to reason step-by-step → improves logic problems, data analysis, and reasoning. RAG (Retrieval-Augmented Generation): combines LLM with enterprise data (S3, DynamoDB, Aurora, etc.) to reduce hallucination and create accurate context-based answers. Bedrock Agents: build multi-step workflows, call APIs, access data, and automate processes without writing complex backend code. Guardrails: establish content safety policies (filter toxic content, restrict topics, regex filtering, mandatory schemas) to ensure AI complies with enterprise standards. Managed AI Services on AWS Rekognition: analyzes images/videos (detect objects, faces, text, moderation). Transcribe: converts speech to text, suitable for call centers and captions. Polly: generates natural speech from text. Comprehend: natural language analysis (sentiment, key phrases, PII detection). Translate: real-time or batch machine translation. Textract: extracts data from scanned documents/PDFs. Personalize: builds recommendation systems like Netflix/Amazon. Mini Kahoot Review Mini quiz helps reinforce knowledge about prompting techniques (zero-shot, few-shot, CoT, structured output) through scenario-based questions. Increases interaction and creates a lively atmosphere, helping learners grasp content more easily than pure theory. Personal Experience and Lessons Learned Prompting Techniques: Most impressed with zero-shot, few-shot, and CoT techniques, helping me learn more about how to deploy GenAI in real projects. Hands-on Demo with Bedrock: Helped visualize the end-to-end process of building and deploying AI. Mini Kahoot: Reinforced knowledge on the spot, helping remember important steps and prompt techniques longer. Networking: Connected with the AWS community and learned from local experts, expanding collaboration opportunities. Event Experience Attending the workshop \u0026ldquo;AI/ML/GenAI on AWS – Generative AI with Amazon Bedrock\u0026rdquo; was an extremely valuable experience, giving me a systematic view of how to build and deploy GenAI solutions on AWS. Some highlights I gained:\nLearning from AWS Experts AWS speakers shared many practical perspectives on selecting Foundation Models, designing RAG architecture, and operating GenAI in enterprises. Clear illustrative examples helped me learn more about how to apply GenAI to real-world problems like document analysis, chatbot creation, or business process automation. Discovering AWS Tools \u0026amp; Services Direct experience with Amazon Bedrock and Guardrails helped me learn more about how AWS supports safe and efficient GenAI deployment. I also learned how to leverage available AI services like Rekognition, Comprehend, Transcribe, or Textract to quickly solve computer vision and NLP needs. Connecting and Exchanging The workshop was a great opportunity to exchange with the AWS user community and listen to practical experiences from teams that have deployed GenAI. Lessons Learned Using RAG, Bedrock Agents, and Prompting Techniques helps reduce dependency, increase scalability, and improve resilience for GenAI solutions. Managed AI Services and tools like Amazon Bedrock can significantly boost development productivity. Event Photos "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Luu Vi Khanh\nPhone Number: 0345158878\nEmail: karlpro812005@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 8/09/2025 to 28/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Learn foundational knowledge about AWS and cloud computing. Set up and configure AWS Free Tier account. Learn how to draw AWS architecture and use management tools. Get familiar with AWS networking concepts (VPC, networking). Practice creating and managing basic VPC. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch AWS architecture drawing tutorial on draw.io and practice - Download necessary tools 8/09/2025 8/09/2025 AWS Architecture Drawing Tutorial on draw.io 3 - Learn about AWS + Cloud Computing + What Makes AWS Different + AWS Global Infrastructure + AWS Services Management Tools + How to Start Your Cloud Journey + Cost Optimization on AWS and Working with AWS Support 9/09/2025 9/09/2025 Cloud Computing What Makes AWS Different AWS Global Infrastructure 4 - AWS Account Setup: + Create new AWS account + MFA for AWS Account + Create Admin Group and Admin User + Account Verification Support - Cost management with AWS Budget + Create Budget + Create Cost Budget + Create Usage Budget + Create RI Budget + Create Saving Plans Budget + Resource Cleanup 10/09/2025 10/09/2025 AWS Account Setup Cost Management with AWS Budget 5 - Learn about AWS networking services: + Amazon Virtual Private Cloud (VPC) + VPC peering \u0026amp; Transit gateway + VPN \u0026amp; Direct Connect + Elastic load balancing - SSH remote connection methods to EC2 - Learn about Elastic IP 11/09/2025 11/09/2025 AWS Virtual Private Cloud VPC Security and Multi-VPC features 6 - Getting started with Amazon Virtual Private Cloud (VPC) and AWS Site-to-Site VPN: + Firewall in VPC + Create VPC, subnet + Create Internet Gateway + Create Route table for Outbound Internet Routing through Internet Gateway + Create security groups 12/09/2025 12/09/2025 Getting started with Amazon VPC and AWS Site-to-Site VPN Week 1 Achievements: Learned foundational AWS knowledge:\nUnderstood cloud computing concepts and AWS benefits Learned about AWS global infrastructure (Regions, AZs) Learned about AWS Services management tools Understood how to start cloud journey and optimize costs Successfully set up AWS account:\nCreated new AWS Free Tier account Configured MFA for account security Created Admin Group and Admin User Completed account verification Cost management with AWS Budget:\nCreated and configured various Budget types: Cost, Usage, RI, Saving Plans Set up cost alerts for budget control Got familiar with AWS networking knowledge:\nUnderstood Amazon VPC and its components Learned about VPC peering, Transit Gateway Explored VPN, Direct Connect, and Elastic Load Balancing Learned SSH remote connection methods to EC2 and Elastic IP usage Hands-on VPC creation:\nCreated VPC and subnets Configured Internet Gateway Set up Route table for Outbound Internet Routing Created and configured Security Groups Understood firewall concepts in VPC Architecture drawing skills:\nLearned how to draw AWS architecture on draw.io Downloaded and used necessary design tools "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Deploy and manage Amazon EC2 instances within VPC environment. Set up hybrid DNS with Route 53 Resolver. Deploy Microsoft AD and configure DNS endpoints. Practice VPC Peering to connect multiple VPCs. Set up AWS Transit Gateway for multi-VPC connectivity. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deploy Amazon EC2 Instances: + Create EC2 server + Test connectivity + Create NAT Gateway + Use Reachability Analyzer + Create EC2 Instance Connect Endpoint + AWS Systems Manager Session Manager + CloudWatch Monitoring \u0026amp; Alerting 15/09/2025 15/09/2025 Getting started with Amazon VPC and AWS Site-to-Site VPN 3 - Set up hybrid DNS with Route 53 Resolver: + Create key pair + Initialize CloudFormation Template + Configure security group 16/09/2025 16/09/2025 Set up hybrid DNS with Route 53 Resolver 4 - Connect RDGW - Deploy Microsoft AD - Set up DNS: + Create Route 53 Outbound Endpoint + Create Route 53 Resolver Rules + Test results + Create Route 53 Inbound Endpoints 17/09/2025 17/09/2025 Set up hybrid DNS with Route 53 Resolver 5 - Set up VPC peering: + Initialize CloudFormation Template + Create Security Group + Create EC2 instance + Update Network ACL + Create Peering connection - Enable Cross-Peer DNS 18/09/2025 18/09/2025 Set up VPC Peering 6 - Set up AWS Transit Gateway + Create Key Pair + Initialize CloudFormation Template + Create Transit Gateway + Create Transit Gateway Attachments + Create Transit Gateway Route Tables + Add Transit Gateway Routes to VPC Route Tables 19/09/2025 19/09/2025 Set up AWS Transit Gateway Week 2 Achievements: Successfully deployed Amazon EC2 Instances:\nCreated and configured EC2 servers within VPC Tested connectivity and troubleshot network issues Created NAT Gateway for outbound internet access Used Reachability Analyzer to analyze connectivity Set up EC2 Instance Connect Endpoint Configured AWS Systems Manager Session Manager Set up CloudWatch Monitoring \u0026amp; Alerting Set up Hybrid DNS with Route 53 Resolver:\nCreated key pair for security Initialized and deployed CloudFormation Template Configured security group for DNS traffic Understood how DNS resolution works in hybrid environment Deployed Microsoft AD and DNS Configuration:\nConnected RDGW (Remote Desktop Gateway) Deployed Microsoft Active Directory Created Route 53 Outbound Endpoint Configured Route 53 Resolver Rules Created Route 53 Inbound Endpoints Tested and verified DNS resolution results Practiced VPC Peering:\nInitialized CloudFormation Template for multi-VPC setup Created Security Group for cross-VPC communication Created EC2 instance in different VPCs Updated Network ACL for peering traffic Created VPC Peering connection Enabled Cross-Peer DNS resolution Set up AWS Transit Gateway:\nCreated Key Pair for EC2 instances Deployed CloudFormation Template for Transit Gateway Created and configured Transit Gateway Set up Transit Gateway Attachments Created Transit Gateway Route Tables Added Transit Gateway Routes to VPC Route Tables Understood how Transit Gateway works as a central hub "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn knowledge of Amazon EC2 and related components. Deploy AWS Backup for system data protection. Practice File Storage Gateway to connect on-premises and cloud. Get started with Amazon S3 and basic features. Integrate S3 with CloudFront and manage lifecycle. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn theory: + Amazon Elastic Compute Cloud (EC2) - Instance type + Amazon Elastic Compute Cloud (EC2) - AMI / Backup / Key Pair + Amazon Elastic Compute Cloud (EC2) - Elastic block store + Amazon Elastic Compute Cloud (EC2) - Instance store + Amazon Elastic Compute Cloud (EC2) - User data + Amazon Elastic Compute Cloud (EC2) - Meta data + Amazon Elastic Compute Cloud (EC2) - EC2 auto scaling + EC2 Autoscaling - EFS/FSx - Lightsail - MGN 22/09/2025 22/09/2025 Amazon EC2 - Instance type Amazon EC2 - AMI / Backup / Key Pair Amazon EC2 - Elastic block store 3 - Deploy AWS Backup for system - Introduction + Deploy infrastructure + Create Backup plan + Test Restore + Clean up resources 23/09/2025 23/09/2025 Deploy AWS Backup for system 4 - Deploy File Storage Gateway + Create S3 Bucket + Create EC2 for Storage Gateway + Create Storage Gateway + Create File Shares 24/09/2025 24/09/2025 Deploy File Storage Gateway 5 - Getting Started With Amazon S3 + Create S3 bucket + Upload data + Enable static website + Configure public access block + Configure public objects - Check website 25/09/2025 25/09/2025 Getting Started With Amazon S3 6 - Getting Started With Amazon S3 + Block all public access + Configure Amazon CloudFront + Check Amazon CloudFront + Bucket Versioning + Move objects + Cross-region object replication + Delete resources 26/09/2025 26/09/2025 Getting Started With Amazon S3 Week 3 Achievements: Learned knowledge of Amazon EC2:\nUnderstood various Instance types and how to choose appropriately Learned about AMI, Backup strategies and Key Pair management Understood Elastic Block Store (EBS) and volume types Learned about Instance Store and differences with EBS Used User Data for configuration automation Accessed and utilized Meta Data service Understood EC2 Auto Scaling and scaling strategies Explored EFS/FSx, Lightsail and MGN Successfully deployed AWS Backup:\nDeployed infrastructure for backup system Created and configured Backup Plan Performed test restore to verify backup Cleaned up resources after completion Understood best practices for backup strategy Practiced File Storage Gateway:\nCreated S3 Bucket as backend storage Deployed EC2 instance for Storage Gateway Configured and activated Storage Gateway Created and managed File Shares Understood how hybrid storage works Getting started with Amazon S3 - Basics:\nCreated and configured S3 bucket Uploaded and managed data Enabled static website hosting Configured public access block for security Managed public objects and permissions Tested and verified website functionality Amazon S3 - Advanced and Integration:\nBlocked all public access for security Configured Amazon CloudFront distribution Tested and optimized CloudFront performance Set up Bucket Versioning for version control Used lifecycle policies to move objects Implemented cross-region object replication Cleaned up resources and managed costs "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn knowledge of AWS storage services. Practice deploying AWS Backup with advanced features. Learn how to use VM Import/Export for migration. Deploy File Storage Gateway for hybrid cloud. Participate in AI-Driven Development event. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn theory: + AWS Storage Services + Amazon Simple Storage Service (S3) - Access Point - Storage Class + S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier + Snow Family - Storage Gateway - Backup 29/09/2025 29/09/2025 AWS Storage Services Amazon S3 - Access Point - Storage Class S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier 3 - Deploy AWS Backup for system: + Create S3 Bucket + Deploy infrastructure + Create Backup plan + Set up notifications + Check operations + Clean up resources 30/09/2025 30/09/2025 Deploy AWS Backup for system 4 - VM Import/Export: + Prepare virtual machine + Export VM from On-premise + Upload VM to AWS + Import VM to AWS + Deploy EC2 Instance from AMI + Set up ACL for S3 Bucket + Export VM from EC2 Instance + Export VM from AMI + Clean up resources 1/10/2025 1/10/2025 VM Import/Export 5 - Deploy File Storage Gateway: + Create S3 Bucket + Create EC2 for Storage Gateway + Create Storage Gateway + Create File Shares + Connect File shares on On-premise machine + Clean up resources 2/10/2025 2/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Participate in AI-Driven Development Life Cycle: Reimagining Software Engineering event 3/10/2025 3/10/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Learned knowledge of Storage Services:\nUnderstood overview of AWS storage services Learned about Amazon S3 Access Points and Storage Classes Understood S3 Static Website, CORS, Control Access Learned about Object Key optimization and Performance tuning Explored Amazon Glacier for long-term storage Learned about Snow Family for data migration Got familiar with Storage Gateway and AWS Backup services Deployed advanced AWS Backup:\nCreated S3 Bucket for backup storage Deployed complete backup infrastructure Created and configured detailed Backup Plan Set up notification system Monitored and checked backup operations Cleaned up resources and managed costs Learned VM Import/Export:\nPrepared and packaged virtual machines from on-premises Exported virtual machines from on-premise environment Uploaded virtual machines to AWS S3 Imported virtual machines to AWS as AMI Deployed EC2 Instance from imported AMI Set up ACL for S3 Bucket security Exported virtual machines from EC2 Instance and AMI Cleaned up resources after migration Deployed complete File Storage Gateway:\nCreated S3 Bucket as backend storage Deployed EC2 instance for Storage Gateway Configured and activated Storage Gateway Created and managed File Shares Connected File Shares from on-premise machine Tested and verified hybrid storage connection Cleaned up resources and optimized Participated in AI-Driven Development event:\nLearned about AI-Driven Development Life Cycle Understood how AI is changing Software Engineering Learned about new trends in software development Connected with developer community and shared experiences "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Deploy and manage Amazon FSx for Windows File Server. Work in teams and plan for the final course project. Research and design architecture for the project. Learn about AWS security and identity management services. Practice with AWS Security Hub. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deploy FSx on Windows: + Create practice environment + Create an SSD Multi-AZ file system + Create an HDD Multi-AZ file system + Create file share + Check performance + Monitor performance 6/10/2025 6/10/2025 Deploy FSx on Windows 3 - Project meeting: + Assign tasks + Decide which services to use + Research pricing for each service 7/10/2025 7/10/2025 4 - Research project architecture - Draw architecture 8/10/2025 8/10/2025 https://drive.google.com/file/d/1ZIhwiHPyB1biXU1bg6HgXIaV6frzPs7K/view?usp=sharing 5 - Learn theory: + Shared Responsibility Model + Amazon Identity and Access Management + Amazon Cognito + AWS Organizations + AWS Identity Center + Amazon Key Management Service + AWS Security Hub 9/10/2025 9/10/2025 Shared Responsibility Model Amazon Identity and Access Management Amazon Cognito 6 - Get started with AWS Security Hub + Enable Security Hub + Score each standard set + Clean up resources + Check performance + Monitor performance 10/10/2025 10/10/2025 Getting started with AWS Security Hub Week 5 Achievements: Successfully deployed Amazon FSx for Windows:\nCreated practice environment for FSx Deployed SSD Multi-AZ file system for high performance Deployed HDD Multi-AZ file system for cost optimization Created and configured file shares Tested and evaluated system performance Monitored performance and optimized Team collaboration and project management:\nHeld team meetings and discussed project plans Assigned tasks and roles to each team member Decided on appropriate AWS services selection Researched and estimated pricing for each service Built detailed implementation plan Project architecture design:\nResearched and analyzed architecture requirements Drew overall project architecture diagram Identified components and connections between them Ensured architecture follows AWS best practices Learned about AWS security services:\nUnderstood AWS Shared Responsibility Model Learned about Amazon Identity and Access Management (IAM) Learned about Amazon Cognito for user authentication Studied AWS Organizations for multi-account management Learned about AWS Identity Center (SSO) Understood Amazon Key Management Service (KMS) Explored AWS Security Hub for security monitoring Practiced with AWS Security Hub:\nEnabled and configured AWS Security Hub Scored and evaluated each security standard set Analyzed findings and recommendations Tested performance and monitoring features Cleaned up resources and managed costs Understood integration with other security services "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Optimize EC2 costs using AWS Lambda and automation. Manage resources efficiently with Tags and Resource Groups. Learn about access management with AWS IAM and Resource Tags. Practice permission restrictions with IAM Permission Boundary. Deploy data encryption with AWS KMS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Optimize EC2 costs with Lambda: + Create VPC + Create Security Group + Create EC2 instance + Incoming Web-hooks Slack + Create Tag for Instance + Create Role for Lambda + Create Lambda Function + Check results 13/10/2025 13/10/2025 Optimize EC2 costs with Lambda 3 - Manage resources with Tags and Resource Groups: + Use Tags + Create a Resource Group + Clean up resources 14/10/2025 14/10/2025 Manage resources with Tags and Resource Groups 4 - Manage access to EC2 Resource Tag with AWS IAM: + Create IAM user + Create IAM Policy + Create IAM Role + Check policy + Clean up resources 15/10/2025 15/10/2025 Manage access to EC2 Resource Tag with AWS IAM 5 - LIMIT USER PERMISSIONS WITH IAM PERMISSION BOUNDARY: + Create Boundary Policy + Create Limited IAM User + Check Limited IAM User + Clean up resources 16/10/2025 16/10/2025 LIMIT USER PERMISSIONS WITH IAM PERMISSION BOUNDARY 6 - Encryption at rest with AWS KMS: + Create Policy and Role + Create Group and User + Create Key Management Service + Create Amazon S3 + Create AWS CloudTrail and Amazon Athena + Test and share encrypted data on S3 + Clean up resources 17/10/2025 17/10/2025 Encryption at rest with AWS KMS Week 6 Achievements: Optimized EC2 costs with Lambda:\nCreated VPC and configured network environment Set up Security Group for security Deployed EC2 instance for testing Configured Incoming Web-hooks Slack for notifications Created and managed Tags for EC2 instances Created IAM Role for Lambda function Wrote and deployed Lambda Function automation Checked results and verified cost optimization features Managed resources with Tags and Resource Groups:\nUnderstood and applied Tag strategy for resources Created and configured Resource Groups Managed and organized resources by groups Cleaned up resources efficiently Understood benefits of resource organization Managed EC2 access with IAM and Resource Tags:\nCreated and configured IAM users Wrote and deployed IAM Policies based on tags Created and managed IAM Roles Checked and verified access policies Cleaned up resources and IAM entities Understood fine-grained access control Limited permissions with IAM Permission Boundary:\nCreated Boundary Policy (Permission Boundary) Created IAM User with limited permissions Tested and checked limited IAM User Understood how Permission Boundary works Cleaned up resources and policies Learned about security best practices Encrypted data with AWS KMS:\nCreated IAM Policy and Role for KMS Set up IAM Group and User Created and managed AWS Key Management Service keys Configured Amazon S3 with encryption Deployed AWS CloudTrail and Amazon Athena for monitoring Tested and shared encrypted data on S3 Cleaned up resources and managed costs Understood encryption at rest and key management "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Complete and optimize the final course project architecture. Enhance knowledge of IAM Role and Condition policies. Review and deepen knowledge of Database services. Review basic concepts of Cloud Computing and IAM. Review EC2 and Storage concepts for midterm exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue editing architecture and consider services to use: + Replace Amazon RDS with DynamoDB + Replace Amazon Lex with Bedrock + Edit comprehensive diagram + Adjust to proper standard size 20/10/2025 20/10/2025 3 - IAM Role \u0026amp; Condition: + Introduction to IAM + Create IAM Group + Create IAM User + Configure Role Condition + Clean up resources 21/10/2025 21/10/2025 IAM Role \u0026amp; Condition 4 - Learn theory: + Review Database Concepts + Amazon RDS \u0026amp; Amazon Aurora + Redshift - ElastiCache 22/10/2025 22/10/2025 Database Concepts review Amazon RDS \u0026amp; Amazon Aurora Redshift - Elasticache 5 - Review cloud computing theory: + What is Cloud Computing? + AWS Global Infrastructure + AWS Shared Responsibility Model - Review IAM theory: Identity Access \u0026amp; Management (IAM) + What Is IAM? + Multi-Factor Authentication (MFA) + CLI and SDK - Learn and review Well Architected Framework 23/10/2025 23/10/2025 Cloud computing IAM: Identity Access \u0026amp; Management Well Architected Framework 6 - Review EC2 theory: Virtual Machines: + What is Amazon EC2? + Security Groups + EC2 Instance Launch Types - Review EC2 Instance Storage theory + EBS Volumes + EFS: Elastic File System and EFS Infrequent Access (EFS-IA) + EC2 Instance Store characteristics - Learn and review Well Architected Framework 24/10/2025 24/10/2025 EC2: Virtual Machines EC2 Instance Storage Well Architected Framework Week 7 Achievements: Completed project architecture:\nContinued editing and optimizing architecture Considered and selected the most suitable services Replaced Amazon RDS with DynamoDB for NoSQL solution Replaced Amazon Lex with Amazon Bedrock for AI capabilities Edited comprehensive and overall diagram Adjusted to proper standard and professional size Enhanced IAM Role \u0026amp; Condition knowledge:\nReviewed and introduced IAM in depth Created and managed IAM Groups Created and configured IAM Users Configured Role Conditions for fine-grained access Cleaned up resources and applied best practices Understood conditional access and context-based permissions Reviewed Database Concepts:\nReviewed basic Database Concepts Understood Amazon RDS and Amazon Aurora in depth Explored Amazon Redshift for data warehousing Learned about Amazon ElastiCache for caching solutions Compared different database types and use cases Understood performance optimization strategies Reviewed Cloud Computing Fundamentals:\nReviewed \u0026ldquo;What is Cloud Computing?\u0026rdquo; and basic concepts Reviewed AWS Global Infrastructure Clearly understood AWS Shared Responsibility Model Reviewed IAM: Identity Access \u0026amp; Management Reviewed Multi-Factor Authentication (MFA) Understood CLI and SDK usage Learned and reviewed Well Architected Framework Reviewed EC2 and Storage:\nReviewed \u0026ldquo;What is Amazon EC2?\u0026rdquo; and concepts Understood Security Groups and network security Reviewed EC2 Instance Launch Types Learned about EBS Volumes and storage options Understood EFS: Elastic File System and EFS-IA Reviewed EC2 Instance Store characteristics Continued learning Well Architected Framework principles "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Comprehensive review of AWS services for midterm exam. Review Load Balancing, Auto Scaling and S3 concepts. Review Global Infrastructure and Cloud Monitoring. Review VPC, Security \u0026amp; Compliance concepts. Prepare for and take AWS midterm exam. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review Elastic Load Balancing \u0026amp; Auto Scaling Groups theory: + Scalability \u0026amp; High Availability + Vertical Scalability and Horizontal Scalability + Load Balancing and Auto Scaling Group - Review Amazon S3 theory + S3 Use cases, security, Bucket Policies, Versioning + S3 Storage Classes + AWS Snow Family + AWS Storage Gateway Framework - Review Databases \u0026amp; Analytics theory + AWS RDS, Amazon Aurora, Amazon ElastiCache, DynamoDB, Redshift - Practice exams 27/10/2025 27/10/2025 Elastic Load Balancing \u0026amp; Auto Scaling Groups Amazon S3 Databases \u0026amp; Analytics Practice exams 3 - Review Global Infrastructure theory: + Why make a global application? + Amazon Route 53 + AWS CloudFront + AWS Global Accelerator - Review Cloud Monitoring theory: + Amazon CloudWatch + AWS CloudTrail + AWS X-Ray + AWS Status - Service Health Dashboard and AWS Personal Health Dashboard - Practice exams 28/10/2025 28/10/2025 Global Infrastructure Cloud Monitoring Practice exams 4 - Review VPC theory: + VPC \u0026amp; Subnets Primer + Internet Gateway (IGW), NAT Gateway + Network ACL \u0026amp; Security Groups + VPC Peering, VPC Endpoints, Site to Site VPN \u0026amp; Direct Connect, Transit Gateway - Review Security \u0026amp; Compliance theory + AWS Shared Responsibility Model + AWS Shield, AWS WAF, AWS KMS, AWS CloudHSM, AWS Certificate Manager (ACM), AWS Secrets Manager, AWS Artifact, AWS GuardDuty, AWS Inspector, AWS Config, AWS Security Hub - Practice exams 29/10/2025 29/10/2025 VPC Security \u0026amp; Compliance Practice exams 5 - Review Other Compute theory: + ECS (Elastic Container Service) and ECR (Elastic Container Registry) + Fargate, AWS Lambda, Amazon API Gateway + Serverless - Review Account Management, Billing \u0026amp; Support theory + AWS Organizations + Service Control Policies (SCP), AWS Organization - Consolidated Billing, Pricing Models in AWS + Savings Plan, Cost Allocation Tags, Tagging and Resource Groups, Cost and Usage Reports, Cost Explorer, AWS Budgets - Practice exams 30/10/2025 30/10/2025 Other Compute Account Management \u0026amp; Billing Practice exams 6 - Practice exams + Take exam session 2 + Review completed exercises 31/10/2025 31/10/2025 Practice exams Week 8 Achievements: Reviewed Load Balancing \u0026amp; Auto Scaling:\nUnderstood Scalability \u0026amp; High Availability concepts Distinguished between Vertical Scalability and Horizontal Scalability Reviewed Load Balancing and Auto Scaling Group Understood different Load Balancer types and use cases Reviewed Auto Scaling policies and strategies Reviewed Amazon S3 \u0026amp; Storage:\nReviewed S3 Use cases, security, Bucket Policies Understood S3 Versioning and lifecycle management Reviewed S3 Storage Classes and cost optimization Explored AWS Snow Family for data migration Understood AWS Storage Gateway Framework Reviewed Databases \u0026amp; Analytics:\nReviewed AWS RDS and its features Understood Amazon Aurora and performance benefits Explored Amazon ElastiCache for caching Reviewed DynamoDB NoSQL concepts Understood Amazon Redshift for data warehousing Reviewed Global Infrastructure:\nUnderstood why global applications are needed Reviewed Amazon Route 53 DNS service Understood AWS CloudFront CDN Explored AWS Global Accelerator Reviewed Cloud Monitoring:\nReviewed Amazon CloudWatch monitoring Understood AWS CloudTrail for auditing Explored AWS X-Ray for tracing Learned about AWS Status and Personal Health Dashboard Reviewed VPC \u0026amp; Networking:\nUnderstood VPC \u0026amp; Subnets concepts Reviewed Internet Gateway and NAT Gateway Distinguished between Network ACL and Security Groups Understood VPC Peering, Endpoints, Site-to-Site VPN Explored Direct Connect and Transit Gateway Reviewed Security \u0026amp; Compliance:\nReviewed AWS Shared Responsibility Model Understood AWS Shield, WAF, KMS, CloudHSM Explored Certificate Manager and Secrets Manager Learned about AWS Artifact, GuardDuty, Inspector Understood AWS Config and Security Hub Reviewed Other Compute Services:\nUnderstood ECS and ECR for containers Reviewed AWS Fargate serverless containers Reviewed AWS Lambda and API Gateway Understood Serverless architecture patterns Account Management \u0026amp; Billing:\nReviewed AWS Organizations Understood Service Control Policies (SCP) Explored Consolidated Billing and Pricing Models Understood Savings Plans and cost optimization Reviewed Cost Explorer, Budgets and tagging Midterm exam preparation:\nCompleted practice exams and continuous review Participated in midterm exam session 2 Reviewed completed exercises and learned from experience Evaluated results and identified areas for improvement "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Continue reviewing and consolidating knowledge after the midterm. Start implementing the final course project with serverless services. Set up Lambda Trigger and integrate with S3. Build API Gateway and connect with Lambda and DynamoDB. Start creating workshop framework for the project. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue reviewing exam questions to consolidate knowledge 3/11/2025 3/11/2025 AWS Knowledge Review 3 - Start working on project + Prepare \u0026amp; Configure Lambda Trigger for S3 + Create S3 Bucket + Configure Lambda Trigger for S3 + Upload CSV data to Bucket 4/11/2025 4/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Continue working on project + Create API Gateway and integrate Lambda + Create IAM Role + Create Lambda Function DynamoDB_API_Handler - Create API Gateway and integrate with Lambda + Access API Gateway service + Create REST API + Create Resource, create Method for each table in DynamoDB + Attach Lambda to each method + Encountered error: The final policy size (20588) is bigger than the limit (20480) 5/11/2025 5/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Continue completing API Gateway: + Delete old Lambda DynamoDB_API_Handler to remove all old policies and create new DynamoDB_API_Handler with refined code + Create new proxy Resource and Method + Attach Lambda + Enable CORS + Deploy API + Test API using Postman 6/11/2025 6/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - After completing API Gateway section: + Start creating workshop framework 7/11/2025 7/11/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Reviewed and consolidated knowledge:\nContinued reviewing midterm questions to consolidate knowledge Reinforced basic AWS concepts after the midterm Identified weak points that need improvement Prepared knowledge for practical project implementation Started project implementation:\nPrepared and planned for the final course project Explored serverless architecture and components Identified AWS services needed for the project Designed data flow and architecture Set up Lambda Trigger for S3:\nCreated and configured S3 Bucket for the project Configured Lambda Trigger to handle S3 events Uploaded CSV data to S3 Bucket Verified trigger functionality and data processing Understood event-driven architecture patterns Built API Gateway and integration:\nCreated and configured IAM Role for Lambda Wrote and deployed Lambda Function DynamoDB_API_Handler Accessed and set up API Gateway service Created REST API and configured endpoints Created Resources and Methods for each table in DynamoDB Attached Lambda function to each HTTP method Resolved issues and optimization:\nEncountered and resolved policy size limit error (20588 \u0026gt; 20480) Deleted old Lambda and created new DynamoDB_API_Handler Optimized code and refined policies Created new proxy Resource and Method Attached Lambda and enabled CORS for API Deployed API and tested using Postman Completed API and prepared workshop:\nCompleted API Gateway section and verified functionality Started creating workshop framework for the project Prepared documentation and guides Planned next steps Evaluated project progress and quality "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Deploying and managing Amazon EC2, Route 53 Resolver, VPC Peering, and Transit Gateway\nWeek 3: Learning EC2, implementing AWS Backup, File Storage Gateway, and getting started with Amazon S3\nWeek 4: Learning Storage Services, advanced AWS Backup, VM Import/Export, and attending AI-Driven Development event\nWeek 5: Deploying Amazon FSx, team collaboration, project architecture design, and learning AWS Security Services\nWeek 6: Optimizing EC2 costs with Lambda, managing resources with Tags, IAM Permission Boundary, and encryption with AWS KMS\nWeek 7: Finalizing project architecture, IAM Role \u0026amp; Condition, reviewing Database concepts, and preparing for midterm\nWeek 8: Comprehensive review of AWS services, Load Balancing, S3, VPC, Security, and taking the midterm exam\nWeek 9: Starting project implementation with Lambda Trigger, API Gateway, DynamoDB, and building workshop framework\nWeek 10: Integrating API with frontend, migrating to Java, resolving ClassNotFoundException, and attending AWS Cloud Mastery Series\nWeek 11: Attending AWS events, resolving Sandbox Timeout errors, integrating X-Ray and CloudWatch, coordinating with database team\nWeek 12: Completing the project, resolving CORS and JWT errors, writing workshop documentation, pushing code to GitHub, and attending Security Pillar event\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/3-ai/3.1/","title":"Create VPC &amp; Configure Security Groups for RDS and Lambda","tags":[],"description":"","content":"Create VPC \u0026amp; Configure Security Groups for RDS and Lambda Implementation Steps 1. Access the VPC service Go to AWS Management Console → search for VPC.\nSelect Your VPCs → Create VPC. Select VPC and more and name the project. In the VPC configuration section: for Number of Availability Zones select 1, for Number of public subnets select 1, for Number of private subnets select 2, and for NAT gateways select Zonal. NAT gateways choose In 1 AZ, VPC endpoints choose None → Create VPC.\nThis process will take a few minutes to create the NAT gateway.\nAfter creation is complete, we can view how \u0026lsquo;VPC and more\u0026rsquo; created the resources via the Resource map. We will create an additional private subnet located in a different AZ to be able to create the RDS instance. 2. Set up Security Groups We will create 2 separate Security Groups (SG) for maximum security.\nGo to Security Groups, click Create security group. We will create 2 security groups for Lambda and RDS.\nName the security group and select the VPC we created in step 1, then click create. Continue creating the SG for RDS, select the VPC from step 1. In the inbound rules section, for type select PostgreSQL, and for Source select the SG created for Lambda. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #2 – DevOps on AWS Event Purpose Build modern DevOps mindset and learn more about DevOps culture. Learn and practice CI/CD deployment processes on AWS. Guide on using Infrastructure as Code (IaC). Learn and enhance system observability through Monitoring \u0026amp; Observability techniques on AWS, including CloudWatch and AWS X-Ray. Participate in hands-on workshops with experts from the AWS Vietnam community. Speaker List \u0026amp; Topics DevOps Mindset Truong Quang Tinh – AWS Community Builder, Platform Engineer at TymeX CI/CD Pipeline Văn Hoàng Kha – Shared about CI/CD processes and Pipeline Workflow models DevOps on AWS – Workshop Bao Huynh – AWS Community Builder Thinh Nguyen – AWS Community Builder Vi Tran – AWS Community Builder Monitoring \u0026amp; Observability Long Huynh – AWS Community Builder Quy Pham – AWS Community Builder Nghiem Le – AWS Community Builder Highlighted Content 1. DevOps Mindset \u0026ldquo;Collaboration-first\u0026rdquo; thinking: enhancing coordination between Development – Operations – QA – Security to reduce silos and improve release quality.\nFocus on automating the entire lifecycle: build, test, deploy, and monitoring to reduce manual errors and accelerate deployment speed.\n2. CI/CD Pipeline Kha presented a visual Pipeline Workflow on Lucidchart, describing complete steps from commit to production: GitHub Version Control with standard branches and clear commit processes. Peer Review \u0026amp; Pull Request ensuring code quality and adherence to coding standards. Automated Test \u0026amp; QA including unit tests, integration tests, and security scans running automatically on each push. Multi-stage Deployment (Dev → QA → Pre-Prod → Prod) with clear approval controls and rollback mechanisms. This process helped me visualize exactly what a standard CI/CD pipeline looks like in real projects. 3. Infrastructure as Code (IaC) IaC knowledge covered: AWS CloudFormation: Template-based provisioning, stack management. Supports drift detection to identify changes outside IaC. AWS CDK (Cloud Development Kit): IaC through programming. Reusable constructs, patterns, and libraries with good support for microservices. Demo: Deployed infrastructure using both CloudFormation and CDK for comparison. Discussion: When to use CloudFormation versus when to use CDK. 4. Monitoring \u0026amp; Observability Real-world examples with Amazon CloudWatch showed me clearly how to build system observability: Centralized Logs, Custom Metrics, Visual Dashboards, and real-time reactive Alarms. Error analysis and tracing demonstration with AWS X-Ray showed how to track the entire request flow: Service map to identify dependencies between microservices. Bottleneck analysis to find components causing slowdowns or errors. Request flow visualization to accurately trace incident root causes. Thanks to these demos, I clearly understood the value of observability in modern system operations instead of just manually viewing logs as before. Personal Experience and Lessons Learned Kha\u0026rsquo;s Pipeline Workflow: Helped me clearly understand how a CI/CD pipeline works in real life — from commit, review, test to deploy — and how to apply it to my team\u0026rsquo;s project workflow. DevOps on AWS Workshop: Helped me connect theory with practice by directly observing CodePipeline, CodeBuild, and CodeDeploy working seamlessly in a complete automation process. CloudWatch \u0026amp; X-Ray: For the first time, I clearly saw how to monitor end-to-end performance, identify bottlenecks, and find root causes of incidents without manual log searching. This completely changed my approach to troubleshooting. DevOps Mindset: I learned how to optimize workflows through automation, increase team collaboration, and shorten feedback loops to improve product quality. Results Achieved Learned more about DevOps mindset and grasped the overall picture of CI/CD processes, from source control to multi-environment deployment. Can independently design and build a standard pipeline based on the model demonstrated in the workshop, to apply directly to team projects. Proficiently use CloudWatch and X-Ray to monitor systems, analyze errors, identify bottlenecks, and optimize service performance. Event Photos "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Event Report: Specialized Workshop on Edge Network Services Event Purpose The workshop was organized to provide participants with a comprehensive view of how to optimize and protect web systems through AWS edge services. Main objectives: Learn more about challenges when operating Internet applications at scale. Learn how Amazon CloudFront addresses performance, cost, and security issues. Explore CloudFront\u0026rsquo;s core features and operational mechanisms. Discover security services such as AWS WAF, AWS Shield, and Bot Control. Hands-on practice optimizing web apps and protecting Internet applications. Speaker List \u0026amp; Topics FROM EDGE TO ORIGIN: CloudFront as Your Foundation Nguyen Gia Hung – Head of Solutions Architect, Amazon Web Services Vietnam Defense from Public Threats: AWS WAF \u0026amp; Application Protection Julian Lu – Sr. Edge Specialist Solutions Architect at Amazon Web Services (AWS) Highlighted Content 1. Challenges in Operating Internet Applications The workshop opened by analyzing common issues businesses typically face:\nHigh latency due to users being far from the origin server. Heavy load on backend systems leading to increased costs. Internet attacks such as DDoS, bot traffic, injection, and scraping. Difficulty scaling when traffic suddenly spikes. Lack of edge defense layer, making the origin vulnerable to direct attacks. 2. CloudFront – The Foundation Solution \u0026ldquo;From Edge to Origin\u0026rdquo; Speakers presented how CloudFront helps improve systems: Caching to reduce load on origin. Global Edge Network allows users to access from the nearest location → increased performance. Security at Edge with WAF, Shield, and Bot Control at the edge. Cost Optimization by reducing direct requests to origin. Resiliency \u0026amp; Availability through global distribution and failover mechanisms. 3. How CloudFront Works A simple architecture model was presented: User sends request → goes to the nearest Edge Location. CloudFront checks for cached content. If not available → request is forwarded to origin. Edge caches content to serve subsequent requests. =\u0026gt; This process helps increase speed while reducing costs and enhancing protection. 4. Protecting Applications from Threats AWS WAF – Web Application Firewall:\nFilters and blocks common attack types: SQL injection, XSS, unauthorized access, etc. Can configure rules, rule groups, and managed rules. Operates directly on CloudFront → protects at the edge layer, reducing burden on origin. AWS WAF Bot Control:\nIdentifies bots, distinguishes good bots from bad bots. Blocks scraping, brute force, and bad automated traffic. Reduces load and increases system security. AWS Shield:\nProvides DDoS protection at multiple levels. Shield Standard (free) is always enabled for CloudFront. Shield Advanced supports monitoring, incident response, and optimized protection costs. Personal Experience and Lessons Learned Learning from AWS experts:\nSpeakers shared difficulties and challenges in deploying large-scale systems and managing traffic. Knowledge about caching, edge layer security, and performance optimization was explained very clearly. Technical experience:\nAnalyzed real architecture diagrams and simulated CloudFront\u0026rsquo;s request processing flow. Explored how to build WAF rules and Bot Control mechanisms. Clearly understood how CloudFront combines with Shield to prevent DDoS. Hands-on Workshop:\nOptimize Internet Web Application: Configured CloudFront, set up caching policies, and verified latency improvements. Secure Internet Web Application: Created and applied WAF rules. Observed bot traffic and created blocking rules. Integrated CloudFront + WAF + Shield. Results Achieved Learned more about the critical role of the edge layer in modern Internet application architecture — not only for accelerating content delivery but also for creating an effective defense layer. Able to explain and deploy architecture models where CloudFront serves as the entry point for all web applications, helping optimize performance and increase security levels. Learned more about how AWS Shield works and can effectively apply it to protect applications from DDoS attacks without complex configuration. Grasped the coordination mechanism between CloudFront – WAF – Shield to create a comprehensive, resilient, and cost-effective defense layer. Event Photos "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #3 - AWS Well-Architected Security Pillar Event Purpose The workshop was organized to help participants gain deep understanding of the Security Pillar in the AWS Well-Architected Framework and how to build secure cloud systems with controlled access, continuous monitoring, data protection, and effective incident response. Main objectives: Learn more about the role of Security in modern cloud architecture. Learn core principles such as Least Privilege, Zero Trust, and Defense in Depth. Understand the Shared Responsibility Model and top threats in Vietnam\u0026rsquo;s cloud environment. Explore 5 important Pillars: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response. Practice policy analysis, building guardrails, logging, monitoring, and simulating incident scenarios. Speaker List \u0026amp; Topics Mendel Grabski (Long) – ex Head of Security \u0026amp; DevOps, Cloud Security Solutions Architect Truong Quang Tinh – AWS Community Builder, Platform Engineer at TymeX Thịnh Lâm – FCJ Cloud Engineer Việt Nguyễn – FCJ Cloud Engineer Danh Hoàng Hiếu Nghị – FCJ Cloud Engineer Highlighted Content 1. Security Foundation \u0026amp; Challenges in Cloud Environment The workshop opened with common issues faced by cloud environments in Vietnam:\nImproper IAM management, granting excessive permissions (over-permission). Long-term credential leaks, keys not being rotated. Speakers emphasized 3 core principles:\nLeast Privilege – grant only minimum necessary permissions. Zero Trust – no default trust, always verify. Defense in Depth – multi-layered protection from IAM → Network → Workload → Data. 2. Identity \u0026amp; Access Management (Pillar 1) IAM content focused on modern architecture models: Minimize creating IAM Users, replace with IAM Roles and Identity Center (SSO). Permission Sets for centralized permission management across multiple accounts. Service Control Policies (SCP) to set permission limits at Organization level. Permission Boundaries to ensure developer permissions don\u0026rsquo;t exceed allowed limits. MFA, credential rotation, and Access Analyzer to reduce access information exposure risks. 3. Detection \u0026amp; Continuous Monitoring (Pillar 2) CloudTrail org-level: logs all API calls across the entire Organization. GuardDuty: detects anomalies such as credential exposure, port scanning, and malicious traffic. Security Hub: aggregates all alerts from GuardDuty, IAM Analyzer, S3, etc. Logging at every layer: VPC Flow Logs ALB \u0026amp; S3 Access Logs EventBridge + Lambda for automatic alerts or locking compromised accounts. Detection-as-Code concept for easy management through Git. 4. Infrastructure Protection (Pillar 3) Proper VPC design: private subnets for workloads, public subnets only for truly necessary resources. Security Groups with inbound whitelist approach. NACLs for subnet-level blocking. Edge security: AWS WAF AWS Shield (DDoS Protection) AWS Network Firewall 5. Data Protection (Pillar 4) KMS manages encryption keys: rotation, key policy, and grants. Secrets Manager and SSM Parameter Store for secure secrets management. Data classification helps determine appropriate protection policies. 6. Incident Response (Pillar 5) IR lifecycle steps according to AWS:\nPreparation Detection \u0026amp; Analysis Containment Eradication Recovery Post-Incident Review Simulated playbooks:\nHandling compromised IAM keys. S3 bucket public exposure. EC2 malware detection and instance isolation methods. Snapshot, evidence collection, and automated response using Lambda/Step Functions. Personal Experience and Lessons Learned Learning from AWS experts:\nSpeakers clearly explained how to build security architecture according to Well-Architected standards. Advanced IAM concepts like SCP, Permission Boundaries, and Identity Center were very intuitive. Technical experience:\nPracticed validating IAM Policies. Observed GuardDuty alerts and analyzed logs. Explored how to set up org-level CloudTrail and activate Security Hub. Simulated incidents to understand real-world Incident Response processes. Results Achieved Learned more about the role of Security in AWS Well-Architected and how to apply the 5 pillars to real systems. Grasped modern IAM models with Identity Center, SCP, and Permission Boundaries. Learned how to use incident detection services like GuardDuty, CloudTrail, and Security Hub. Gained practical understanding of Incident Response processes and automation using Lambda/Step Functions. Event Photos "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"Event Report: BUILDING AGENTIC AI: Context Optimization with Amazon Bedrock Event Purpose The workshop focused on Agentic AI and building AI agents on Amazon Bedrock, including:\nUnderstanding Agentic AI concepts and Amazon Bedrock Agent Learning context optimization techniques and agentic workflows Hands-on practice with CloudThinker orchestration framework Speakers \u0026amp; Topics Kien Nguyen – Solutions Architect\nTopic: AWS Bedrock Agent Core\nViet Pham – Founder cum CEO (Diaflow)\nTopic: [Use Case] Building Agentic Workflow on AWS\nThang Ton – Co-founder \u0026amp; COO (CloudThinker)\nTopic: CloudThinker Introduction\nHenry Bui – Head of Engineering (CloudThinker)\nTopic: CloudThinker Agentic Orchestration, Context Optimization on Amazon Bedrock (L300)\nKha Van – CloudThinker Engineer\nTopic: CloudThinker Hack: Hands-on Workshop\nKey Content 1. AWS Bedrock Agent Core (Kien Nguyen) Architecture and core features of Amazon Bedrock Foundation Models, Agent architecture, Action groups Multi-step reasoning, Tool integration, Memory management 2. Building Agentic Workflow - Diaflow Use Case (Viet Pham) Real-world case study from Diaflow Multi-agent workflow design and AWS services integration Addressing context switching, consistency, cost optimization 3. CloudThinker Platform (Thang Ton) AI orchestration platform simplifying AI agent development Agent orchestration, Context optimization, Multi-model support 4. Context Optimization Technical Deep-dive (Henry Bui) Orchestration Framework: Agent coordination, state management Context Optimization: Token usage, semantic chunking, dynamic injection Performance: Prompt engineering, model selection, monitoring Bedrock APIs, RAG, Guardrails integration 5. Hands-on Workshop (Kha Van) Setting up and connecting CloudThinker with AWS Practicing cost optimization and cost analysis Experience \u0026amp; Learnings Learned more about Amazon Bedrock and context optimization techniques CloudThinker framework simplifies agent development Hands-on experience with cost optimization using CloudThinker Diaflow case study revealed real-world challenges in deploying agentic AI Context management and cost optimization are crucial when scaling AI Key Outcomes Learned more about Agentic AI and Amazon Bedrock Agent framework Learned context optimization techniques and agentic workflows Gained hands-on experience with CloudThinker and AWS integration Understood cost optimization and real-world challenges from Diaflow use case Event Photos "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/4-frontend/4.1/","title":"Hosting website with S3","tags":[],"description":"","content":" Step 1: Create an S3 Bucket Go to the S3 service. Click Create bucket. Enter a unique Bucket name (for example: flyora-shop). Uncheck “Block all public access” Acknowledge the warning about public access. Click Create bucket. Step 2: Upload Website Files Open your newly created bucket. Click Upload → Add files → select your website files (e.g, index.html) Click Upload. Step 3: Enable Static Website Hosting Go to the Properties tab of your bucket. Scroll down to Static website hosting. Click Edit → Enable Static website hosting Enter: Index document: index.html Error document: index.html (optional) Click Save changes. *Go to the Permission tab of your bucket. Edit Bucket Policy Paste the following JSON policy (replace flyora-shop with your bucket name): { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::your-bucket-name/*\u0026#34; } ] } Step 4: Test Your Website Click the Bucket website endpoint URL. http://your-bucket-name.s3-website-ap-southeast-1.amazonaws.com\nMake sure it looks like this "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/1-introduction/","title":"Introduction","tags":[],"description":"","content":"Introduction What is Flyora? Flyora is a modern e-commerce platform designed to demonstrate cloud-native architecture using AWS serverless services. This workshop guides you through building a fully functional online store with product browsing, AI-powered chatbot assistance, and automated data management.\nWorkshop Objectives By completing this workshop, you will:\nDeploy a serverless e-commerce system on AWS Implement automated data pipelines using S3 and Lambda triggers Build RESTful APIs with API Gateway and Lambda Create a scalable database using DynamoDB Integrate AI chatbot functionality for customer support Deploy a static frontend with S3 and CloudFront Set up CI/CD pipelines for automated deployments Architecture Overview The Flyora platform uses a fully serverless architecture:\nFrontend Layer:\nStatic website hosted on Amazon S3 Global content delivery via Amazon CloudFront Responsive UI for product browsing and shopping Backend Layer:\nAPI Gateway for RESTful API endpoints AWS Lambda functions for business logic Amazon DynamoDB for product and order data Amazon S3 for data import and storage AI Layer:\nAI-powered chatbot for product recommendations Integrated into the frontend UI Provides real-time customer assistance Security \u0026amp; Authentication:\nIAM roles for secure service access Workshop Structure This workshop is organized into team-based modules:\nBackend Team - API development and data pipeline AI Team - Chatbot integration and AI features Frontend Team - UI development and deployment CI/CD - Automated deployment pipeline Testing - System validation and performance testing Cleanup - Resource management and cost optimization Expected Outcomes After completing this workshop, you will have:\nA fully functional e-commerce website running on AWS Hands-on experience with serverless architecture Understanding of AWS best practices for scalability and security Knowledge of CI/CD implementation for cloud applications A portfolio project demonstrating cloud engineering skills Cost Considerations This workshop is designed to run within the AWS Free Tier. All services used have free tier options, and the architecture avoids costly resources like EC2 instances. Estimated cost for running this workshop: $0-5 USD if completed within a few hours.\nPrerequisites Before starting, ensure you have:\nAn AWS account with administrative access Basic understanding of cloud computing concepts Familiarity with REST APIs and JSON Basic knowledge of HTML/CSS/JavaScript (for frontend work) Git installed on your local machine "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/2-backend/2.1/","title":"Prepare &amp; Configure Lambda Trigger for S3","tags":[],"description":"","content":"Prepare \u0026amp; Configure Lambda Trigger for S3 Steps to Perform 1. Access the IAM Service Go to the AWS Management Console → search for IAM. Select Roles → Create Role. Choose Trusted entity type: AWS service. Choose Use case: Lambda, then click Next. 2. Attach Permissions to the Role Attach the following policies:\nAmazonS3FullAccess AmazonDynamoDBFullAccess_v2 Click Next, then name the role LambdaS3DynamoDBRole.\nThis role allows Lambda to read files from S3 and write data to DynamoDB.\nCreate an S3 Bucket Go to the S3 service. In the S3 interface, select Create bucket. On the Create bucket screen:\nBucket name: Enter a name, for example:\nflyora-bucket-database (If the name already exists, add a number at the end.)\nKeep all other default settings unchanged.\nReview your configuration and click Create bucket to finish. Expected Results The flyora-bucket (or your chosen name) is successfully created. The LambdaS3DynamoDBRole role is ready to be assigned to Lambda in the next step. Configure Lambda Trigger for S3 In this step, you will configure AWS Lambda to automatically import CSV files into DynamoDB whenever a new file is uploaded to the S3 Bucket.\nCreate a Lambda Function Go to Lambda → Create function. Select Author from scratch. Function name: AutoImportCSVtoDynamoDB. Runtime: Python 3.13. Role: select LambdaS3DynamoDBRole created in the previous step. Add a Trigger In the Configuration → Triggers tab, click Add trigger. Choose S3. Select Bucket flyora-bucket. Event type: All object create events. Click Add to save. Paste the Lambda Code\nPaste the following code: import boto3 import csv import io import json from botocore.exceptions import ClientError from decimal import Decimal dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) s3 = boto3.client(\u0026#39;s3\u0026#39;) # ------------------------- # Hàm kiểm tra kiểu dữ liệu của mẫu (Detect Type) # ------------------------- def detect_type(value): val_str = str(value).strip() # Check Int/Float try: float(val_str) return \u0026#39;N\u0026#39; # Number except ValueError: pass return \u0026#39;S\u0026#39; # String # ------------------------- # Hàm chuyển đổi dữ liệu (Convert) # ------------------------- def convert_value(value): if value is None: return None val_str = str(value).strip() if val_str == \u0026#34;\u0026#34;: return None # Int check try: if float(val_str).is_integer(): return int(float(val_str)) except ValueError: pass # Decimal check (cho Float) try: return Decimal(val_str) except Exception: pass # Boolean if val_str.lower() == \u0026#34;true\u0026#34;: return True if val_str.lower() == \u0026#34;false\u0026#34;: return False return val_str # ------------------------- # Tạo bảng Dynamic dựa trên kiểu dữ liệu phát hiện được # ------------------------- def create_table_if_not_exists(table_name, pk_name, pk_type): existing_tables = dynamodb.meta.client.list_tables()[\u0026#39;TableNames\u0026#39;] if table_name in existing_tables: print(f\u0026#34;Table \u0026#39;{table_name}\u0026#39; already exists.\u0026#34;) return print(f\u0026#34;Creating table: {table_name} | PK: {pk_name} | Type: {pk_type}\u0026#34;) table = dynamodb.create_table( TableName=table_name, KeySchema=[{\u0026#39;AttributeName\u0026#39;: pk_name, \u0026#39;KeyType\u0026#39;: \u0026#39;HASH\u0026#39;}], AttributeDefinitions=[{\u0026#39;AttributeName\u0026#39;: pk_name, \u0026#39;AttributeType\u0026#39;: pk_type}], BillingMode=\u0026#39;PAY_PER_REQUEST\u0026#39; ) table.wait_until_exists() print(\u0026#34;Table created successfully.\u0026#34;) # ------------------------- # Main Handler # ------------------------- def lambda_handler(event, context): try: for record in event[\u0026#39;Records\u0026#39;]: bucket = record[\u0026#39;s3\u0026#39;][\u0026#39;bucket\u0026#39;][\u0026#39;name\u0026#39;] key = record[\u0026#39;s3\u0026#39;][\u0026#39;object\u0026#39;][\u0026#39;key\u0026#39;] print(f\u0026#34;Processing: {key}\u0026#34;) response = s3.get_object(Bucket=bucket, Key=key) # 1. QUAN TRỌNG: Dùng \u0026#39;utf-8-sig\u0026#39; để xóa BOM, giúp nhận diện số chính xác body = response[\u0026#39;Body\u0026#39;].read().decode(\u0026#39;utf-8-sig\u0026#39;) reader = csv.DictReader(io.StringIO(body)) # Clean headers reader.fieldnames = [name.strip() for name in reader.fieldnames] items = list(reader) if not items: continue # Lấy thông tin Partition Key (PK) pk_name = reader.fieldnames[0] table_name = key.split(\u0026#39;.\u0026#39;)[0] # 2. QUAN TRỌNG: Phát hiện kiểu dữ liệu dựa trên dòng đầu tiên first_pk_val = items[0].get(pk_name) pk_type = detect_type(first_pk_val) # Sẽ trả về \u0026#39;N\u0026#39; nếu là số, \u0026#39;S\u0026#39; nếu là chữ # Tạo bảng đúng kiểu (N hoặc S) create_table_if_not_exists(table_name, pk_name, pk_type) table = dynamodb.Table(table_name) count = 0 with table.batch_writer() as batch: for row in items: clean_item = {} is_valid = True for k, v in row.items(): if not k or k.strip() == \u0026#34;\u0026#34;: continue clean_k = k.strip() val = convert_value(v) # Chuyển đổi sang Int/Decimal/Bool if val is None: continue # 3. QUAN TRỌNG: Xử lý Partition Key theo đúng kiểu của Bảng if clean_k == pk_name: if pk_type == \u0026#39;N\u0026#39;: # Nếu bảng là Number, bắt buộc Key phải là Number if not isinstance(val, (int, Decimal)): print(f\u0026#34;SKIPPING ROW: Key \u0026#39;{val}\u0026#39; is not a number but table requires Number.\u0026#34;) is_valid = False break else: # Nếu bảng là String, ép kiểu sang String val = str(val) clean_item[clean_k] = val if is_valid and pk_name in clean_item: batch.put_item(Item=clean_item) count += 1 print(f\u0026#34;Success: Imported {count} items into {table_name} (PK Type: {pk_type})\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;OK\u0026#39;)} except Exception as e: print(f\u0026#34;ERROR: {str(e)}\u0026#34;) import traceback traceback.print_exc() return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(str(e))} Click Deploy and confirm it shows Successfully. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"🐦 Proposal: Flyora – E-commerce Platform for Bird Lovers 📄 Download Full Proposal PDF\n1. Executive Summary Flyora is a specialized web application designed to serve bird enthusiasts across Vietnam. It offers curated products such as bird food, toys, cages, and decorative accessories tailored to species like Chào Mào, Vẹt, Yến Phụng, and Chích Chòe. Built with modern web technologies and hosted on AWS, Flyora ensures scalability, performance, and secure access. The platform aims to become the go-to destination for bird care and ornamentation, combining e-commerce with personalization and community engagement.\n2. Problem Statement Current Challenges:\nNo centralized platform for bird-specific products Generic pet stores lack species-specific recommendations Poor mobile responsiveness and outdated UI in existing platforms Limited backend scalability and search capabilities Proposed Solution: Flyora delivers a responsive, category-driven shopping experience with secure user authentication, real-time product filtering, and a scalable backend. It supports both desktop and mobile users, with future plans for AI-powered recommendations and chatbot support.\n3. Solution Architecture 📄 System Architecture Diagram 🧩 Frontend (Web Tier) Amazon S3: Static web hosting for frontend assets CloudFront: CDN for global content delivery Responsive design: Mobile-friendly interface 🔐 Authentication \u0026amp; Security IAM: Identity and access management CloudWatch \u0026amp; AWS X-Ray: Monitoring and distributed tracing 🔄 Backend Services (App Tier) Technical Improvements: Responsive, mobile-friendly UI Secure user authentication and role management (IAM) Scalable backend with Lambda/API Gateway AWS Lambda Functions: Chatbot handler Import automation API handler Amazon Bedrock: Embedding Model and LLM Model for AI-powered features 📦 Data \u0026amp; Storage (Data Tier) Amazon RDS for PostgreSQL: Relational database DynamoDB: NoSQL database Amazon S3: Data storage 🔧 CI/CD \u0026amp; Development GitLab: Version control and CI/CD pipeline triggers AWS CodeBuild: Automated build process AWS CodePipeline: Continuous integration and deployment 4. Technical Implementation Phases: AWS Learning \u0026amp; Setup – Learn AWS services and architecture design Development \u0026amp; Integration – Build frontend and connect AWS backend Testing \u0026amp; Deployment – Complete testing and production release Month 1 - AWS Learning Focus: Week 1-2: AWS fundamentals (S3, Lambda, API Gateway, DynamoDB) Week 3: Advanced services (Bedrock, OpenSearch) Week 4: Architecture design and database modeling with MySQL Workbench Technical Requirements: AWS services proficiency for serverless architecture Frontend development with S3 static hosting DynamoDB for NoSQL data management GitHub for version control and CI/CD integration 5. Timeline \u0026amp; Milestones Phase Duration Key Milestones Month 1: AWS Learning 4 weeks • AWS fundamentals learned\n• Architecture designed\n• Database schema created Month 2: Development 4 weeks • Frontend UI completed\n• Lambda functions built\n• API Gateway configured Month 3: Integration 4 weeks • Full system integration\n• Testing completed\n• Production deployment 6. Budget Estimation Item Monthly Cost Annual Cost Amazon S3 (Simple Storage Service) $0.15 $1.80 AWS Lambda (Serverless Compute) $0.00 $0.00 Amazon API Gateway (REST API Endpoints) $0.04 $0.48 DynamoDB (On-demand NoSQL Database) $0.00 $0.00 AWS X-Ray (Application Monitoring) $0.01 $0.12 Amazon CloudWatch (Monitoring \u0026amp; Logs) $0.00 $0.00 Amazon Bedrock (AI/LLM Services) $3.49 $41.88 Amazon RDS for PostgreSQL (Relational DB) $21.01 $252.12 AWS Data Transfer (Network Traffic) $0.00 $0.00 Amazon CloudFront (CDN Service) $0.10 $1.20 AWS CodePipeline (CI/CD Automation) $0.00 $0.00 AWS CodeBuild (Build Service) $2.52 $30.24 Amazon VPC (Virtual Private Cloud) $43.07 $516.84 Total Estimate $70.39 $844.68 Hardware costs are not applicable as Flyora is a web-only platform.\n7. Risk Assessment Risk Impact Probability Mitigation Strategy Lambda cold starts Medium Medium Provisioned concurrency for critical functions DynamoDB throttling Medium Low Auto-scaling and proper partition key design RDS downtime Medium Low Multi-AZ deployment, automated backups Cost overruns Low Low Monitor with AWS Budgets and CloudWatch alerts Bedrock API limits Medium Low Monitor usage, fallback to cached results 8. Expected Outcomes Technical Improvements: Responsive, mobile-friendly, UI/UX Secure user authentication and role management (IAM) Scalable backend with Lambda/API Gateway Real-time product filtering and chatbot support AI-powered features via Bedrock (Embedding/LLM) Robust data storage with RDS, DynamoDB, and S3 Business Value: Centralized platform for bird lovers in Vietnam Reduced reliance on generic pet stores Foundation for future AI features and community expansion Potential for mobile app and chatbot integration "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Continue developing and perfecting the final course project. Integrate API into frontend and resolve CORS issues. Migrate from Node.js to Java for Lambda functions. Resolve technical errors and optimize deployment. Attend AWS Cloud Mastery Series event. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue working on project + Create API Gateway and integrate Lambda + Create IAM Role + Create Lambda Function DynamoDB_API_Handler - Create API Gateway and integrate with Lambda + Access API Gateway service + Create REST API + Create Resource, create Method for each table in DynamoDB + Attach Lambda to each method + Encountered error: The final policy size (20588) is bigger than the limit (20480) 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Continue working on project + Go to office to work on project with team members + Integrate API into frontend + Encountered CORS policy error: No \u0026lsquo;Access-Control-Allow-Origin\u0026rsquo; + Fix OPTIONS method to resolve error + Fix Lambda - Encountered error: the server responded with a status of 400 + Continue fixing code and logic in Lambda 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Continue working on project + Rewrite Java code from scratch to deploy to Lambda + Edit code multiple times to move code from local to Lambda + Build JAR file + Deploy to Lambda 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Continue working on project + Encountered ClassNotFoundException error + Changed from Spring Cloud Function to aws-serverless-java-container-springboot2 + Added maven-shade-plugin + Added core and event + Successfully resolved the error 13/11/2025 13/11/2025 Resolving ClassNotFoundException error 6 - Attend AWS Cloud Mastery Series #1 event 14/11/2025 14/11/2025 AWS Cloud Mastery Series #1 Week 10 Achievements: Continued developing API Gateway and Lambda:\nContinued building API Gateway and integrating Lambda Created and configured IAM Role for Lambda functions Deployed Lambda Function DynamoDB_API_Handler Accessed and set up API Gateway service Created REST API and configured endpoints Created Resources and Methods for each table in DynamoDB Attached Lambda functions to each HTTP method Encountered and resolved policy size limit error Team collaboration and frontend integration:\nWent to office to work on project with team members Integrated API into frontend application Encountered and resolved CORS policy error: No \u0026lsquo;Access-Control-Allow-Origin\u0026rsquo; Fixed OPTIONS method to resolve CORS error Modified Lambda function to support CORS Encountered \u0026ldquo;the server responded with a status of 400\u0026rdquo; error Continued fixing code and logic in Lambda Migration to Java and deployment optimization:\nRewrote Java code from scratch to deploy to Lambda Edited code multiple times to move code from local to Lambda Built JAR file for Java application Deployed Java application to AWS Lambda Understood challenges when migrating programming languages Resolved ClassNotFoundException error:\nEncountered ClassNotFoundException error when running Java on Lambda Changed from Spring Cloud Function to aws-serverless-java-container-springboot2 Added maven-shade-plugin to Maven configuration Added AWS Lambda core and event dependencies Successfully resolved the error and Lambda runs stably Learned how to package Java applications for AWS Lambda Attended event and learned:\nAttended AWS Cloud Mastery Series #1 event Explored new trends in AWS cloud services Connected with AWS community and shared experiences Updated knowledge about best practices and new features Applied learned knowledge to practical projects "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Attend AWS Cloud Mastery Series and Edge Network Services events. Continue developing and perfecting the final course project. Resolve technical errors in Lambda functions. Integrate monitoring and logging with X-Ray and CloudWatch. Coordinate with database team to connect data. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attend AWS Cloud Mastery Series #2 event 17/11/2025 17/11/2025 3 - Continue working on project + Encountered Sandbox Timeout error + Increased timeout to 1 minute + Successfully resolved error and displayed data 18/11/2025 18/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Attend specialized event on Edge Network Services and do hands-on workshop 19/11/2025 19/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Continue working on project + Research and find ways to integrate X-Ray and CloudWatch logs into project 20/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Continue working on project + Coordinate with database team to connect data + Encountered error: java.lang.IllegalArgumentException: Boolean string was not \u0026rsquo;true\u0026rsquo; or \u0026lsquo;false\u0026rsquo;: 1 + Fixed by changing boolean to integer Error resolved 21/11/2025 21/11/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Attended events and learned:\nAttended AWS Cloud Mastery Series #2 event Attended specialized event on Edge Network Services Completed hands-on workshop on Edge Network Services Updated knowledge about new AWS services Connected and learned from AWS community Resolved Sandbox Timeout error:\nEncountered Sandbox Timeout error when running Lambda functions Increased timeout to 1 minute to resolve the issue Successfully resolved error and displayed data Learned how to configure timeout for AWS Lambda Researched monitoring and logging integration:\nResearched how to integrate AWS X-Ray into project Explored how to use CloudWatch Logs Planned to implement distributed tracing Understood the importance of monitoring in serverless architecture Team coordination and data error resolution:\nCoordinated with database team to connect data Encountered error: java.lang.IllegalArgumentException: Boolean string was not \u0026rsquo;true\u0026rsquo; or \u0026lsquo;false\u0026rsquo;: 1 Fixed by changing boolean to integer to resolve error Error resolved and data operates stably Enhanced debugging skills and handling data type mismatch errors Improved teamwork skills:\nWorked effectively with multiple different teams Resolved integration issues between components Enhanced technical communication skills Understood the importance of data consistency "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Continue developing and perfecting the final course project. Coordinate with database and frontend teams to integrate the system. Resolve technical errors related to data types and configuration. Handle CORS issues in API integration. Write workshop and prepare project documentation. Attend AWS Cloud Mastery Series event on Security Pillar. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Continue working on project + Coordinate with database team to connect data + Encountered error: java.lang.NumberFormatException: Character N is neither a decimal digit number, decimal point, nor \u0026ldquo;e\u0026rdquo; notation exponential mark + This error was due to null values in integer fields + Fixed database by removing null values Error resolved - Encountered error: java.lang.IllegalArgumentException: Could not resolve placeholder \u0026lsquo;app.jwt.secret\u0026rsquo; in value \u0026ldquo;${app.jwt.secret}\u0026rdquo; + Added environment variable Error resolved 24/11/2025 24/11/2025 3 - Continue working on project + Coordinate with frontend team to call API + Encountered error: java.lang.IllegalArgumentException: Boolean string was not \u0026rsquo;true\u0026rsquo; or \u0026lsquo;false\u0026rsquo;: 1 + Fixed by changing boolean to integer Error resolved 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Continue working on project + Coordinate with frontend team to call API + Encountered error: has been blocked by CORS policy: No \u0026lsquo;Access-Control-Allow-Origin\u0026rsquo; header is present on the requested resource + Fixed on frontend side + Added to OPTIONS method, Access-Control-Allow-Methods -\u0026gt; \u0026lsquo;GET,POST,PUT,DELETE,OPTIONS\u0026rsquo; Error resolved 26/11/2025 26/11/2025 CORS policy error: No \u0026lsquo;Access-Control-Allow-Origin\u0026rsquo; header 5 - Continue working on project + Write workshop + Push backend to GitHub 27/11/2025 27/11/2025 workshop backend 6 - Attend AWS Cloud Mastery Series #3 - AWS Well-Architected Security Pillar event 28/11/2025 28/11/2025 AWS Cloud Mastery Series #3 Week 12 Achievements: Resolved NumberFormatException and JWT configuration errors:\nCoordinated with database team to connect data Encountered error: java.lang.NumberFormatException: Character N is neither a decimal digit number Discovered error was due to null values in integer fields Fixed database by removing null values Encountered error: Could not resolve placeholder \u0026lsquo;app.jwt.secret\u0026rsquo; Added environment variable to resolve JWT configuration issue Successfully resolved all errors and system operates stably Coordinated with frontend team and resolved API errors:\nCoordinated with frontend team to call API Encountered java.lang.IllegalArgumentException error regarding Boolean data type again Fixed by changing boolean to integer to ensure consistency Successfully resolved data type mismatch issue Enhanced debugging skills and integration error handling Handled CORS issues in API Gateway:\nContinued coordinating with frontend team to call API Encountered CORS policy error: No \u0026lsquo;Access-Control-Allow-Origin\u0026rsquo; header Fixed on frontend side to handle CORS Added to OPTIONS method: Access-Control-Allow-Methods -\u0026gt; \u0026lsquo;GET,POST,PUT,DELETE,OPTIONS\u0026rsquo; Completely resolved CORS issue Learned how to configure CORS for API Gateway Wrote workshop and managed source code:\nWrote workshop for the project to guide others Pushed backend code to GitHub for version management Organized and documented the entire project Prepared documentation for demo and handover Enhanced technical documentation writing skills Attended event and learned about Security:\nAttended AWS Cloud Mastery Series #3 event Learned about AWS Well-Architected Security Pillar Learned about best practices for security on AWS Applied security knowledge to practical projects Updated knowledge about security compliance and governance Improved teamwork and problem-solving skills:\nWorked effectively with multiple teams (database, frontend) Quickly resolved complex technical issues Enhanced debugging and troubleshooting skills Developed communication and project coordination skills "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/2-backend/","title":"Backend Workshop (API &amp; Data Layer)","tags":[],"description":"","content":"Backend: API \u0026amp; Data Pipeline In this workshop, you will:\nUse Amazon S3 to store input data (CSV files). Configure AWS Lambda Trigger to automatically import data into DynamoDB. Create Lambda (API Handler) and expose it through API Gateway to access DynamoDB data. Test REST API endpoints via Postman or API Gateway Console. Clean up all resources after completion. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/3-ai/3.2/","title":"Configure RDS and Connect with DBeaver","tags":[],"description":"","content":"Configure RDS and Connect with DBeaver Implementation Steps 1. Access RDS Service Go to AWS Management Console → search for RDS. Before creating the RDS instance, we will create a subnet group. Name the subnet group and select the VPC you created. For AZ select ap-southeast-1a and ap-southeast-1b. For Subnets, select the 2 private subnets, then click Create. 2. Create RDS Go to databases → Create database. In the database configuration section, select Full configuration, select PostgreSQL. In Templates select Sandbox. In Settings, name the DB and set a password. Configure the remaining parts as follows: In the Connectivity section, select the VPC created and the DB subnet group. Set Public access to No. In the VPC security group section, select the Security group created for RDS. Keep the rest as default and click Create. 1. Store Data and Connect to PostgreSQL using DBeaver You can download DBeaver here: https://dbeaver.io/ Download the knowledge_base file from here.\nTo connect from the local machine to DBeaver, we need to create an EC2 instance to act as a bridge.\nAccess EC2 → Launch instance.\nName the EC2, select Instance type t2.micro, create a key pair, and save it to your machine. In Network settings, select the VPC created, select a public subnet, and create a Security group. In Inbound Security Group Rules, select \u0026lsquo;My IP\u0026rsquo;, then Launch instance. Next, go to the RDS Security group section, edit the inbound rules, and add a new rule. Type: PostgreSQL and Source: the newly created EC2. Open DBeaver and click on the connection section. Select PostgreSQL. In the Host section, copy the RDS Endpoint. Fill in the information you used when creating the RDS. Click on the SSH tab. In Host/IP, copy the Public IP of the EC2. For User Name, enter ec2-user. In Authentication Method, select Public Key and choose the key pair created with the EC2. Then click Test connection and Finish. After successfully connecting, open an SQL script and paste this code to create the knowledge_base table. After creating the table, refresh the database to show the knowledge_base table.\n-- 1. Enable vector extension (run only once) CREATE EXTENSION IF NOT EXISTS vector; -- 2. Create knowledge base table (Knowledge Base) CREATE TABLE knowledge_base ( id bigserial PRIMARY KEY, content text, -- Original text content (chunked text) metadata jsonb, -- Store extra info: image link, filename, page number... embedding vector(1024) -- IMPORTANT: Must be 1024 for Cohere Multilingual ); -- 3. Create index for faster search CREATE INDEX ON knowledge_base USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64); To import data into DBeaver using Python, we need to SSH via CMD. Open CMD in the folder containing the keypair and copy this command:\nssh -i \u0026#34;my-key.pem\u0026#34; -L 5433:RDS endpoint port:5432 ec2-user@public IP EC2 -N Then, run this Python script to import data into DBeaver.\nimport pandas as pd import json import boto3 import psycopg2 import time import glob import os import numpy as np from dotenv import load_dotenv # ========================================== # 1. CONFIGURATION \u0026amp; SECURITY # ========================================== current_dir = os.path.dirname(os.path.abspath(__file__)) env_path = os.path.join(current_dir, \u0026#39;pass.env\u0026#39;) load_dotenv(env_path) CSV_FOLDER = \u0026#39;./database\u0026#39; DB_HOST = os.getenv(\u0026#34;DB_HOST\u0026#34;) DB_NAME = os.getenv(\u0026#34;DB_NAME\u0026#34;) DB_USER = os.getenv(\u0026#34;DB_USER\u0026#34;) DB_PASS = os.getenv(\u0026#34;DB_PASS\u0026#34;) # AWS Connection bedrock = boto3.client( service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;, aws_access_key_id=os.getenv(\u0026#34;aws_access_key_id\u0026#34;), aws_secret_access_key=os.getenv(\u0026#34;aws_secret_access_key\u0026#34;) ) # ========================================== # 2. TRANSLATION DICTIONARY (MOST IMPORTANT) # ========================================== # A. Translate Column Names (For AI context) COLUMN_MAP = { \u0026#34;price\u0026#34;: \u0026#34;Price\u0026#34;, \u0026#34;gia\u0026#34;: \u0026#34;Price\u0026#34;, \u0026#34;cost\u0026#34;: \u0026#34;Cost\u0026#34;, \u0026#34;fee\u0026#34;: \u0026#34;Fee\u0026#34;, \u0026#34;stock\u0026#34;: \u0026#34;Stock\u0026#34;, \u0026#34;so_luong\u0026#34;: \u0026#34;Stock\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Description\u0026#34;, \u0026#34;mo_ta\u0026#34;: \u0026#34;Description\u0026#34;, \u0026#34;chi_tiet\u0026#34;: \u0026#34;Details\u0026#34;, \u0026#34;origin\u0026#34;: \u0026#34;Origin\u0026#34;, \u0026#34;xuat_xu\u0026#34;: \u0026#34;Origin\u0026#34;, \u0026#34;material\u0026#34;: \u0026#34;Material\u0026#34;, \u0026#34;chat_lieu\u0026#34;: \u0026#34;Material\u0026#34;, \u0026#34;color\u0026#34;: \u0026#34;Color\u0026#34;, \u0026#34;mau_sac\u0026#34;: \u0026#34;Color\u0026#34;, \u0026#34;weight\u0026#34;: \u0026#34;Weight\u0026#34;, \u0026#34;trong_luong\u0026#34;: \u0026#34;Weight\u0026#34;, \u0026#34;food_type\u0026#34;: \u0026#34;Food Type\u0026#34;, \u0026#34;usage_target\u0026#34;: \u0026#34;Suitable for bird species\u0026#34;, \u0026#34;furniture_type\u0026#34;: \u0026#34;Furniture Type\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;Processing Time\u0026#34;, \u0026#34;thoi_gian\u0026#34;: \u0026#34;Processing Time\u0026#34;, \u0026#34;method_name\u0026#34;: \u0026#34;Method Name\u0026#34; } # B. Translate Values (For Website display) \u0026lt;--- PART YOU NEED VALUE_TRANSLATIONS = { \u0026#34;FOODS\u0026#34;: \u0026#34;Food\u0026#34;, \u0026#34;Foods\u0026#34;: \u0026#34;Food\u0026#34;, \u0026#34;foods\u0026#34;: \u0026#34;Food\u0026#34;, \u0026#34;TOYS\u0026#34;: \u0026#34;Toys\u0026#34;, \u0026#34;Toys\u0026#34;: \u0026#34;Toys\u0026#34;, \u0026#34;toys\u0026#34;: \u0026#34;Toys\u0026#34;, \u0026#34;FURNITURE\u0026#34;: \u0026#34;Furniture\u0026#34;, \u0026#34;Furniture\u0026#34;: \u0026#34;Furniture\u0026#34;, \u0026#34;furniture\u0026#34;: \u0026#34;Furniture\u0026#34;, \u0026#34;Bird\u0026#34;: \u0026#34;Pet Bird\u0026#34;, \u0026#34;bird\u0026#34;: \u0026#34;Pet Bird\u0026#34; } # --- AUXILIARY FUNCTIONS --- def get_embedding(text): try: if not text or len(str(text)) \u0026lt; 5: return None body = json.dumps({\u0026#34;texts\u0026#34;: [str(text)], \u0026#34;input_type\u0026#34;: \u0026#34;search_document\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34;}) response = bedrock.invoke_model(body=body, modelId=\u0026#34;cohere.embed-multilingual-v3\u0026#34;, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34;) return json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embeddings\u0026#39;][0] except: return None def clean(val): if pd.isna(val) or str(val).lower() in [\u0026#39;nan\u0026#39;, \u0026#39;none\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;null\u0026#39;]: return \u0026#34;\u0026#34; val_str = str(val).strip() # --- AUTO TRANSLATE HERE --- # If the value exists in the translation dictionary, replace it immediately if val_str in VALUE_TRANSLATIONS: return VALUE_TRANSLATIONS[val_str] return val_str def main(): try: conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS, port=5433) # Note the SSH port 5433 cur = conn.cursor() print(\u0026#34;✅ Database connection successful!\u0026#34;) except Exception as e: print(f\u0026#34;❌ DB Connection Error: {e}\u0026#34;); return csv_files = glob.glob(os.path.join(CSV_FOLDER, \u0026#34;*.csv\u0026#34;)) print(f\u0026#34;📂 Found {len(csv_files)} CSV files.\u0026#34;) # Statistics variables stats = {\u0026#34;bird\u0026#34;: 0, \u0026#34;food\u0026#34;: 0, \u0026#34;toy\u0026#34;: 0, \u0026#34;furniture\u0026#34;: 0, \u0026#34;best_sellers\u0026#34;: []} total_success = 0 for file_path in csv_files: filename = os.path.basename(file_path).lower() print(f\u0026#34;\\n--- Processing file: {filename} ---\u0026#34;) try: df = pd.read_csv(file_path) df = df.replace({np.nan: None}) # Auto-detect category (Prefix) category_prefix = \u0026#34;Product\u0026#34; if \u0026#34;bird\u0026#34; in filename: category_prefix = \u0026#34;Bird Species\u0026#34; elif \u0026#34;food\u0026#34; in filename: category_prefix = \u0026#34;Bird Food\u0026#34; elif \u0026#34;toy\u0026#34; in filename or \u0026#34;do_choi\u0026#34; in filename: category_prefix = \u0026#34;Bird Toy\u0026#34; elif \u0026#34;furniture\u0026#34; in filename: category_prefix = \u0026#34;Cage Furniture\u0026#34; elif \u0026#34;ship\u0026#34; in filename or \u0026#34;delivery\u0026#34; in filename: category_prefix = \u0026#34;Shipping Method\u0026#34; elif \u0026#34;payment\u0026#34; in filename: category_prefix = \u0026#34;Payment Method\u0026#34; for index, row in df.iterrows(): # Statistics if \u0026#34;bird\u0026#34; in filename: stats[\u0026#34;bird\u0026#34;] += 1 elif \u0026#34;food\u0026#34; in filename: stats[\u0026#34;food\u0026#34;] += 1 elif \u0026#34;toy\u0026#34; in filename: stats[\u0026#34;toy\u0026#34;] += 1 elif \u0026#34;furniture\u0026#34; in filename: stats[\u0026#34;furniture\u0026#34;] += 1 # A. IDENTITY p_id = clean(row.get(\u0026#39;id\u0026#39;) or row.get(\u0026#39;product_id\u0026#39;) or row.get(\u0026#39;payment_id\u0026#39;)) name = clean(row.get(\u0026#39;name\u0026#39;) or row.get(\u0026#39;product_name\u0026#39;) or row.get(\u0026#39;title\u0026#39;) or row.get(\u0026#39;method_name\u0026#39;)) if not name: if p_id: name = f\u0026#34;Code {p_id}\u0026#34; else: continue # B. AUTO SCAN COLUMNS AND TRANSLATE content_parts = [f\u0026#34;{category_prefix}: {name}\u0026#34;] # Scan all columns, if exists in COLUMN_MAP then add for col_key, col_val in row.items(): val_clean = clean(col_val) # clean function will auto translate FOODS -\u0026gt; Food if val_clean and col_key in COLUMN_MAP: content_parts.append(f\u0026#34;{COLUMN_MAP[col_key]}: {val_clean}\u0026#34;) # C. HANDLE PRICE \u0026amp; STOCK \u0026amp; BEST SELLERS SEPARATELY price = clean(row.get(\u0026#39;price\u0026#39;) or row.get(\u0026#39;gia\u0026#39;) or row.get(\u0026#39;fee\u0026#39;)) if price: content_parts.append(f\u0026#34;Price: {price}\u0026#34;) stock = clean(row.get(\u0026#39;stock\u0026#39;) or row.get(\u0026#39;so_luong\u0026#39;)) if stock: content_parts.append(f\u0026#34;Stock: {stock}\u0026#34;) sold = clean(row.get(\u0026#39;sold\u0026#39;) or row.get(\u0026#39;da_ban\u0026#39;)) if sold: content_parts.append(f\u0026#34;Sold: {sold}\u0026#34;) try: if float(sold) \u0026gt; 0: stats[\u0026#34;best_sellers\u0026#34;].append((float(sold), name, category_prefix)) except: pass content_to_embed = \u0026#34;. \u0026#34;.join(content_parts) + \u0026#34;.\u0026#34; # D. CREATE METADATA (Also use translated values) # Note: The clean() function above already translated, so we call clean() again for each field metadata = {} for k, v in row.items(): metadata[k] = clean(v) # Save to metadata in English # Overwrite standard fields metadata[\u0026#39;id\u0026#39;] = p_id metadata[\u0026#39;name\u0026#39;] = name metadata[\u0026#39;price\u0026#39;] = price if price else \u0026#34;Contact\u0026#34; metadata[\u0026#39;type\u0026#39;] = category_prefix metadata[\u0026#39;image\u0026#39;] = clean(row.get(\u0026#39;image_url\u0026#39;) or row.get(\u0026#39;link_anh\u0026#39;)) metadata[\u0026#39;sold\u0026#39;] = sold # E. INSERT vector = get_embedding(content_to_embed) if vector: cur.execute( \u0026#34;INSERT INTO knowledge_base (content, embedding, metadata) VALUES (%s, %s, %s)\u0026#34;, (content_to_embed, json.dumps(vector), json.dumps(metadata, default=str)) ) total_success += 1 if total_success % 10 == 0: print(f\u0026#34; -\u0026gt; Loaded {total_success} rows...\u0026#34;) conn.commit() time.sleep(0.1) except Exception as e: print(f\u0026#34;⚠️ Error processing file {filename}: {e}\u0026#34;); continue # --- CREATE STATISTICS REPORT --- print(\u0026#34;\\n--- Creating statistics report... ---\u0026#34;) top_products = sorted(stats[\u0026#34;best_sellers\u0026#34;], key=lambda x: x[0], reverse=True)[:5] top_names = \u0026#34;, \u0026#34;.join([f\u0026#34;{p[1]} ({int(p[0])} purchases)\u0026#34; for p in top_products]) summary_content = ( f\u0026#34;BIRD SHOP STATISTICS REPORT: \u0026#34; f\u0026#34;Total birds: {stats[\u0026#39;bird\u0026#39;]}. Food: {stats[\u0026#39;food\u0026#39;]}. \u0026#34; f\u0026#34;Toys: {stats[\u0026#39;toy\u0026#39;]}. Furniture: {stats[\u0026#39;furniture\u0026#39;]}. \u0026#34; f\u0026#34;TOP 5 BEST SELLING PRODUCTS: {top_names}.\u0026#34; ) summary_vector = get_embedding(summary_content) if summary_vector: cur.execute(\u0026#34;INSERT INTO knowledge_base (content, embedding, metadata) VALUES (%s, %s, %s)\u0026#34;, (summary_content, json.dumps(summary_vector), json.dumps({\u0026#34;id\u0026#34;:\u0026#34;STATS\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Statistics\u0026#34;}, default=str))) conn.commit() cur.close(); conn.close() print(f\u0026#34;\\n🎉 COMPLETED! Total imported: {total_success + 1} rows.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() After importing, refresh the knowledge_base table to see the results.\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/4-frontend/4.2/","title":"Distribute with CloudFront ","tags":[],"description":"","content":" Create a CloudFront Distribution pointing to the S3 bucket Go to the Cloudfront service. Create Cloudfront Distribution Enter a Cloudfront name (for example: flyora-shop). Select your S3 you have created before Turn on Website endpoint make sure you get a url like this http://Your-S3-name.s3-website.ap-southeast-1.amazonaws.com/ Configure: Viewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP method: GET, HEAD , OPTION Cache policy: CatchingOptimized Response headers policy: CORS-with-preflight-and-SecurityHeadersPolicy Choose do not enable securiy protections Review and click Create Distribution After Create Distribution you have to wait for 5 or 10 minutes to deploy and if it deploy successfully it will show you date you have deploy "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/2-backend/2.2/","title":"Verify Data in DynamoDB","tags":[],"description":"","content":"Verify Data in DynamoDB In this step, you will verify that the data from the CSV file has been successfully imported into DynamoDB.\nSteps to Perform Upload the CSV File to the Bucket Download the sample CSV file from here.\nIn the newly created Bucket:\nGo to the Objects tab → click Upload. Extract file zip. Drag and drop the files, then click Upload. [!TIP] After uploading, please wait about 3–5 minutes for the Lambda function to import the data.\nAccess the DynamoDB Service Go to AWS Management Console → search for DynamoDB. Select Tables → click on the products table. View the Data Open the Explore items tab. Check the list of products that have been imported. If you don\u0026rsquo;t see any data, please check the following:\nThe DynamoDB table name must match the CSV file name. The CSV file must have a valid header. The Lambda function must have sufficient permissions to access both S3 and DynamoDB. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Centralized Multi-Account Application Resilience Assessment Using AWS Resilience Hub This blog demonstrates how to implement a centralized resilience assessment solution for applications across multiple AWS accounts using AWS Resilience Hub. You\u0026rsquo;ll explore the hub and spoke model for managing resilience policies, learn to configure IAM roles for cross-account assessments, integrate SNS for drift detection notifications, and analyze assessment reports to improve your application\u0026rsquo;s RTO/RPO in alignment with the AWS Well-Architected Framework.\nBlog 2 - Implement a USDC bridge on AWS This blog walks you through building a USDC (stablecoin) bridge on AWS using a serverless architecture with AWS Step Functions and Lambda. You\u0026rsquo;ll discover how to implement Circle\u0026rsquo;s Cross-Chain Transfer Protocol (CCTP) V2 to transfer USDC between blockchains like Ethereum, Base, and Avalanche, integrate with the Circle Attestation Service, manage private keys securely with AWS Secrets Manager, and automate the burn-mint token workflow safely and efficiently.\nBlog 3 - Strengthen Your AWS Cloud Storage Security with Superna Defender This blog introduces a comprehensive cloud storage security solution for AWS using Superna Defender, a Cyberstorage tool that protects Amazon S3 and FSx for Windows File Server from ransomware and malware threats. You\u0026rsquo;ll learn about the NIST Cyberstorage Checklist framework (Identify, Detect, Protect, Respond, Recover), real-time threat monitoring and detection capabilities, automated isolation and recovery of affected data, and seamless integration with security tools like Splunk, Palo Alto Networks, and AWS Security Hub.\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/3-ai/","title":"AI Workshop (Chatbot)","tags":[],"description":"","content":"This workshop provides a detailed guide on how to build a Product Consultation Chatbot using RAG (Retrieval-Augmented Generation) architecture on the AWS platform.\n1. System Architecture The system utilizes the following AWS services:\nAmazon Bedrock: Provides AI models (LLMs). Generation Model: Amazon Nova Lite (anthropic.claude-3-haiku-20240307-v1:0) for answering questions. Embedding Model: Cohere Embed Multilingual (cohere.embed-multilingual-v3) for data vectorization. Amazon RDS (PostgreSQL): Stores product data and vectors (using the Dbeaver extension). AWS Lambda: Intermediate logic processing function (Serverless backend). "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/4-frontend/4.3/","title":"API Integration","tags":[],"description":"","content":"Step 1: Get Your API Gateway URL First of all you have to receive API GateWay url https://uwbxj9wfq6.execute-api.ap-southeast-1.amazonaws.com/dev Open your project folder in VS Code. Create a new file named api.js inside your src directory. Add the following JavaScript code: Step 2: Test the API in Postman Take the data from postman You should receive a status 200 response with JSON data Step 3: Deploy and Verify on S3 Visit your http://your-bucket-name.s3-website-ap-southeast-1.amazonaws.com Using data from Postman with https://uwbxj9wfq6.execute-api.ap-southeast-1.amazonaws.com/dev If you login and it show a notification like this the API Integration was successfully "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/2-backend/2.3/","title":"Create Lambda Function to Handle DynamoDB Requests","tags":[],"description":"","content":"Objective Create a new Lambda Function to process requests to DynamoDB through API Gateway.\nSteps Download the backend file from here.\nStep 1: Create IAM Role Open IAM Console → Roles → Create role Select\nTrusted entity: AWS Service → Lambda Attach the following permissions:\nAmazonDynamoDBFullAccess CloudWatchLogsFullAccess Set the role name: LambdaAPIAccessRole\nStep 2: Create Lambda Function Go to AWS Lambda → Create function Select Author from scratch Function name: DynamoDB_API_Handler Runtime: Java 17 Choose IAM Role: LambdaAPIAccessRole Step 3: Deploy the JAR File Go to S3 → Upload → Add files\nUpload the jar file, then copy the Object URL Go to AWS Lambda → Upload from S3\nPaste the object URL you copied Go to AWS Lambda → Code → Runtime settings → Edit Set Handler:\norg.example.flyora_backend.handler.StreamLambdaHandler::handleRequest Step 4: Configure Lambda Function Go to AWS Lambda → Configuration → General configuration → Edit Set Timeout: 1 min Go to AWS Lambda → Configuration → Environment variables → Edit Add: Key: APP_JWT_SECRET Value: huntrotflyorateam!@ky5group5member Key: GHN_TOKEN; Value: 445c659d-5586-11f0-8c19-5aba781b9b65 "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/3-ai/3.3/","title":"Create Logic for Lambda Function","tags":[],"description":"","content":"Create Logic for Lambda Function Implementation Steps 1. Access Lambda Service Go to AWS Management Console → search for Lambda. First, go to the Layers section to create a layer so the Lambda function has the library to connect to PostgreSQL (psycopg2).\nAccess the GitHub link to download the appropriate Python version, here we use psycopg2-3.11: https://github.com/jkehler/awslambda-psycopg2\nAfter downloading, put the entire psycopg2-3.11 folder into a folder named (python), then zip that folder and name it (postgres-layer-3.11).\nClick Create layer, name the layer, upload the postgres-layer-3.11 zip file → Create. 2. Create Lambda Functions → Create function. Name the Lambda function and select runtime Python 3.11. In Additional configurations, check VPC, select the created VPC, select 1 private subnet for Subnet, and select Lambda-SG for Security group. After the Lambda function is created, we will go to the layer section to add the created layer. Select Custom layers, choose the created layer, select version 1 → Add. Go to the Configuration tab, in General configuration, increase the timeout to 1 minute. In Permissions, add AmazonBedrockFullAccess and AWSLambdaVPCAccessExecutionRole. In Environment variables, add the following variables: DB_HOST, DB_NAME, DB_USER, DB_PASS (Enter your RDS information). After configuring, paste this Python code into the Lambda function and click Deploy.\nimport json import boto3 import psycopg2 import os # --- CONFIGURATION --- DB_HOST = os.environ.get(\u0026#39;DB_HOST\u0026#39;) DB_NAME = os.environ.get(\u0026#39;DB_NAME\u0026#39;) DB_USER = os.environ.get(\u0026#39;DB_USER\u0026#39;) DB_PASS = os.environ.get(\u0026#39;DB_PASS\u0026#39;) # Use ap-southeast-1 region bedrock = boto3.client(service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) # --- 1. EMBEDDING FUNCTION (COHERE) --- def get_embedding(text): try: body = json.dumps({ \u0026#34;texts\u0026#34;: [text], \u0026#34;input_type\u0026#34;: \u0026#34;search_query\u0026#34;, \u0026#34;truncate\u0026#34;: \u0026#34;END\u0026#34; }) response = bedrock.invoke_model( body=body, modelId=\u0026#34;cohere.embed-multilingual-v3\u0026#34;, accept=\u0026#34;application/json\u0026#34;, contentType=\u0026#34;application/json\u0026#34; ) return json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embeddings\u0026#39;][0] except Exception as e: print(f\u0026#34;Embed Error: {e}\u0026#34;) return None # --- 2. SAFE METADATA HANDLING FUNCTION --- def format_product_info(metadata): \u0026#34;\u0026#34;\u0026#34; This function helps normalize data even if the CSV file is missing columns \u0026#34;\u0026#34;\u0026#34; if not metadata: return None # Get name (Prioritize common keys) name = metadata.get(\u0026#39;name\u0026#39;) or metadata.get(\u0026#39;product_name\u0026#39;) or metadata.get(\u0026#39;title\u0026#39;) or \u0026#34;Unnamed Product\u0026#34; # Get price (If not present, leave empty or \u0026#39;Contact\u0026#39;) price = metadata.get(\u0026#39;price\u0026#39;) or metadata.get(\u0026#39;gia\u0026#39;) or metadata.get(\u0026#39;display_price\u0026#39;) price_str = f\u0026#34;- Price: {price}\u0026#34; if price else \u0026#34;\u0026#34; # Get type (if available from smart import script) category = metadata.get(\u0026#39;type\u0026#39;) or \u0026#34;Product\u0026#34; # Create description string for AI to read # Example: \u0026#34;Bird Species: Parrot. - Price: 500k\u0026#34; ai_context = f\u0026#34;Category/Product: {name} ({category}) {price_str}\u0026#34; # Create object for Frontend display (Product Card) frontend_card = { \u0026#34;id\u0026#34;: metadata.get(\u0026#39;id\u0026#39;) or metadata.get(\u0026#39;product_id\u0026#39;), \u0026#34;name\u0026#34;: name, \u0026#34;price\u0026#34;: price if price else \u0026#34;Contact\u0026#34;, # Frontend will show \u0026#34;Contact\u0026#34; if no price \u0026#34;image\u0026#34;: metadata.get(\u0026#39;image_url\u0026#39;) or metadata.get(\u0026#39;link_anh\u0026#39;) or \u0026#34;\u0026#34;, # Image can be empty \u0026#34;type\u0026#34;: category # To let frontend know if this is a bird or a toy } return ai_context, frontend_card # --- 3. MAIN HANDLER --- def lambda_handler(event, context): print(\u0026#34;Event:\u0026#34;, event) try: # Parse Input if \u0026#39;body\u0026#39; in event: try: body_data = json.loads(event[\u0026#39;body\u0026#39;]) if isinstance(event[\u0026#39;body\u0026#39;], str) else event[\u0026#39;body\u0026#39;] except: body_data = {} else: body_data = event user_question = body_data.get(\u0026#39;question\u0026#39;, \u0026#39;\u0026#39;) if not user_question: return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Missing question\u0026#39;)} # A. Create Vector q_vector = get_embedding(user_question) if not q_vector: return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Vector creation error\u0026#39;)} # B. Search DB conn = psycopg2.connect(host=DB_HOST, database=DB_NAME, user=DB_USER, password=DB_PASS) cur = conn.cursor() # Retrieve metadata for processing sql = \u0026#34;\u0026#34;\u0026#34; SELECT content, metadata FROM knowledge_base ORDER BY embedding \u0026lt;=\u0026gt; %s LIMIT 3 \u0026#34;\u0026#34;\u0026#34; cur.execute(sql, (json.dumps(q_vector),)) results = cur.fetchall() cur.close(); conn.close() # C. Process Results (MOST IMPORTANT) ai_contexts = [] frontend_products = [] for row in results: raw_content = row[0] # Original content at import raw_metadata = row[1] # JSON metadata # Call normalization function ai_text, card_data = format_product_info(raw_metadata) if ai_text: # Merge original content + identification content to be sure ai_contexts.append(f\u0026#34;{ai_text}. Details: {raw_content}\u0026#34;) frontend_products.append(card_data) # If nothing is found if not ai_contexts: final_answer = \u0026#34;Sorry, the shop currently cannot find matching information in the database.\u0026#34; else: # D. Send to LLM (Claude 3 Haiku) context_str = \u0026#34;\\n---\\n\u0026#34;.join(ai_contexts) system_prompt = ( \u0026#34;You are a consultant for the Bird Shop, named \u0026#39;Smart Parrot\u0026#39;. \u0026#34; \u0026#34;Style: Friendly, Polite but Concise.\\n\\n\u0026#34; \u0026#34;HANDLING INSTRUCTIONS (PRIORITIZE IN ORDER):\\n\u0026#34; \u0026#34;1. SOCIAL INTERACTION: If the customer just says hello (Hello, Hi...) or thanks: \u0026#34; \u0026#34;-\u0026gt; Greeting back warmly and ask what they are looking for. DO NOT list products unless asked.\\n\u0026#34; \u0026#34;2. PRODUCT CONSULTATION: When customer asks about goods:\\n\u0026#34; \u0026#34;-\u0026gt; Answer concisely: Product Name + Price + Stock status (if any).\\n\u0026#34; \u0026#34;-\u0026gt; Do not describe in flowery detail unless asked specifically \u0026#39;how is it\u0026#39;.\\n\u0026#34; \u0026#34;-\u0026gt; If information is not in Context, say \u0026#39;Sorry, the shop doesn\u0026#39;t have this item yet\u0026#39;.\\n\u0026#34; ) user_msg = f\u0026#34;Reference Information:\\n{context_str}\\n\\nQuestion: {user_question}\u0026#34; # Call Claude 3 Haiku claude_body = json.dumps({ \u0026#34;anthropic_version\u0026#34;: \u0026#34;bedrock-2023-05-31\u0026#34;, \u0026#34;max_tokens\u0026#34;: 300, \u0026#34;temperature\u0026#34;: 0.1, \u0026#34;system\u0026#34;: system_prompt, \u0026#34;messages\u0026#34;: [{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: user_msg}] }) try: model_res = bedrock.invoke_model( body=claude_body, modelId=\u0026#34;anthropic.claude-3-haiku-20240307-v1:0\u0026#34; ) res_json = json.loads(model_res[\u0026#39;body\u0026#39;].read()) final_answer = res_json[\u0026#39;content\u0026#39;][0][\u0026#39;text\u0026#39;] except Exception as e: print(f\u0026#34;LLM Call Error: {e}\u0026#34;) final_answer = \u0026#34;Sorry, the AI system is busy, please try again later.\u0026#34; # E. Return Result return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;POST\u0026#39;, \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#34;answer\u0026#34;: final_answer, \u0026#34;products\u0026#34;: frontend_products # Frontend uses this to draw UI }) } except Exception as e: print(f\u0026#34;System Error: {e}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(str(e))} 3. Integrate into Team\u0026rsquo;s API Gateway Access API Gateway Service. Select the API created by the Backend. Select Create resource. Resource name chatbot and check CORS. Select the chatbot resource and click Create method. Method type select POST, check Lambda proxy integration. Select the created Lambda Function (or VPC based on original context), then click Create method. After configuration is complete, click Deploy API. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in six events. Each one was a memorable experience that provided new, interesting, and valuable knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: [AWS GenAI Builder Club] AI-Driven Development Life Cycle – Reimagining Software Engineering\nDate \u0026amp; Time: 2:00 PM, October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AI/ML/GenAI on AWS – Generative AI with Amazon Bedrock\nDate \u0026amp; Time: 8:30 AM, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS\nDate \u0026amp; Time: 8:30 AM, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: Edge Network Services Workshop\nDate \u0026amp; Time: 9:00 AM, November 19, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #3 - AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 8:30 AM, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: BUILDING AGENTIC AI - Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 9:00 AM, December 5, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/2-backend/2.4/","title":"Create API Gateway and Integrate with Lambda","tags":[],"description":"","content":"Objective Connect AWS API Gateway with a Lambda Function to create a RESTful endpoint that allows accessing data stored in DynamoDB.\nImplementation Steps 1. Access API Gateway Go to AWS Console → API Gateway Click Create API Select REST API (Build) Configure: Create new API: New API API name: FlyoraAPI Endpoint type: Regional Click Create API 2. Create Resources and Methods In the sidebar, select\nActions → Create Resource Resource Name: api Click Create Resource Select /api → Actions → Create Resource Configure the resource: Resource path: /api/ Resource Name: v1 Click Create Resource Create a proxy resource under /api Check Proxy resource Resource path: /api/ Resource Name: {proxy+} Click Create Resource Select /v1 → Actions → Create Resource Configure:\nCheck Proxy resource Resource path: /api/v1/ Resource Name: {myProxy+} Click Create Resource Enable CORS for all resources Under OPTIONS → Integration response → Header Mappings, ensure the headers below exist:\nAccess-Control-Allow-Origin: * Access-Control-Allow-Headers:\nContent-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token Access-Control-Allow-Methods:\nDELETE,GET,HEAD,OPTIONS,PATCH,POST,PUT 3. Integrate with Lambda After creating /api/v1/{myProxy+}, the ANY method appears: Select ANY → Integration request → Edit Attach Lambda: Integration type: Lambda Function Check Lambda proxy integration Lambda Region: ap-southeast-1 (Singapore) Lambda Function: select your Lambda_API_Handler 4. Deploy API Select Actions → Deploy API Deployment stage: New stage Stage name: dev Description: Development stage for Lambda API Click Deploy After deployment, you will receive an Invoke URL in the format:\nhttps://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/4-frontend/","title":"Frontend Workshop (UI)","tags":[],"description":"","content":"Frontend Hosting and API Integration on AWS In this workshop, you will learn how to deploy a frontend web application on AWS and connect it to a backend API hosted via Amazon API Gateway.\nThis activity combines frontend hosting and API integration, showing how AWS services can support interactive, serverless web applications.\nAmazon S3 – Stores and serves your static web assets (HTML, CSS, JS).\nAmazon CloudFront – Distributes your website globally with HTTPS and low latency.\nAmazon API Gateway – Exposes backend API endpoints that your frontend can call.\nThis workshop demonstrates how to connect a static website to a backend API through API Gateway, creating a complete serverless web architecture that enables real-time data interaction between frontend and backend.\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/5-cicd/","title":"CI/CD Automation","tags":[],"description":"","content":"AWS CodeBuild Setup Guide for Flyora Frontend This guide covers CI/CD setup for the Frontend Repository (React).\nThis guide walks you through setting up AWS CodeBuild and CodePipeline for the Flyora React frontend application.\nPrerequisites AWS Account with appropriate permissions GitHub repository: QuangHieu-lab/Flyora-shop buildspec.yml file in the repository root (see below) AWS Region: Singapore (ap-southeast-1) (or your preferred region) S3 bucket for hosting static files CloudFront distribution (optional, for CDN) Required: buildspec.yml File Create a buildspec.yml file in the root of your repository with this content:\nversion: 0.2 phases: pre_build: commands: - echo \u0026#34;Installing dependencies on `date`\u0026#34; - npm ci - echo \u0026#34;Running linter...\u0026#34; - npm run lint --if-present || echo \u0026#34;No lint script configured\u0026#34; build: commands: - echo \u0026#34;Running tests on `date`\u0026#34; - npm test -- --watchAll=false --passWithNoTests --coverage || echo \u0026#34;Tests completed\u0026#34; - echo \u0026#34;Building application on `date`\u0026#34; - npm run build - echo \u0026#34;Build completed on `date`\u0026#34; post_build: commands: - echo \u0026#34;Post-build phase started on `date`\u0026#34; - echo \u0026#34;Checking if build directory exists...\u0026#34; - ls -la build/ - echo \u0026#34;Build artifacts created successfully\u0026#34; - echo \u0026#34;Build completed successfully - artifacts ready for deployment\u0026#34; - echo \u0026#34;Use CodePipeline Deploy stage for S3 deployment\u0026#34; - echo \u0026#34;CloudFront invalidation will be handled by CodePipeline\u0026#34; artifacts: files: - \u0026#39;**/*\u0026#39; base-directory: build name: BuildArtifact discard-paths: yes cache: paths: - \u0026#39;/root/.npm/**/*\u0026#39; - \u0026#39;node_modules/**/*\u0026#39; Key Points:\nUses npm ci for faster, reliable installs Runs linter if configured Runs tests with coverage Builds React application Deployment handled by CodePipeline Deploy stage (not in buildspec) Caches npm and node_modules Step 1: Create S3 Bucket for Hosting Before setting up CodeBuild, create an S3 bucket to host your React application:\nGo to S3 console Click \u0026ldquo;Create bucket\u0026rdquo; Bucket name: flyora-frontend-hosting Region: Singapore (ap-southeast-1) Uncheck \u0026ldquo;Block all public access\u0026rdquo; (for static website hosting) Enable \u0026ldquo;Static website hosting\u0026rdquo; in bucket properties Set Index document: index.html Set Error document: index.html Step 2: Navigate to CodeBuild Open your browser and go to: https://console.aws.amazon.com/codesuite/codebuild/projects Sign in to your AWS account Ensure you\u0026rsquo;re in the Singapore (ap-southeast-1) region Click \u0026ldquo;Create build project\u0026rdquo; Step 3: Project Configuration Field Value Project name flyora-frontend-build Description Build project for Flyora React frontend Build badge Leave unchecked Step 4: Source Configuration Field Value Source provider GitHub Repository Repository in my GitHub account GitHub repository https://github.com/QuangHieu-lab/Flyora-shop | Source version | Leave blank | | Git clone depth | 1 | | Primary source webhook events | ⚠️ UNCHECK this |\nStep 5: Environment Configuration Section Field Value Provisioning Provisioning model On-demand Environment image Managed image Operating system Amazon Linux Runtime(s) Standard Image Latest (e.g., aws/codebuild/amazonlinux2-x86_64-standard:5.0) Service role Service role New service role Step 6: Environment Variables Add these environment variables in CodeBuild:\nName Value Type AWS_S3_BUCKET flyora-frontend-hosting Plaintext CLOUDFRONT_DISTRIBUTION_ID Your CloudFront distribution ID (if using) Plaintext REACT_APP_API_URL Your backend API URL Plaintext Step 7: Buildspec and Logs Buildspec:\nBuild specifications: Use a buildspec file Buildspec name: Leave blank Logs:\nCloudWatch logs: ✅ Enable Group name: /aws/codebuild/flyora-frontend Step 8: IAM Permissions The CodeBuild service role needs permissions to access S3 and CloudFront. Add this policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::flyora-frontend-hosting\u0026#34;, \u0026#34;arn:aws:s3:::flyora-frontend-hosting/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateInvalidation\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Step 9: Create CodePipeline Go to CodePipeline console Click \u0026ldquo;Create pipeline\u0026rdquo; Pipeline name: flyora-frontend-pipeline Source stage: GitHub (Version 2) Build stage: Select flyora-frontend-build Deploy stage: Add S3 deploy action Action provider: Amazon S3 Bucket: flyora-frontend-hosting Extract file before deploy: ✅ Checked Testing After pipeline creation:\nMake a change in your React repository Commit and push to GitHub Pipeline automatically triggers Check S3 bucket for updated files Access your website via S3 endpoint or CloudFront URL Troubleshooting Issue: Build fails with \u0026ldquo;npm: command not found\u0026rdquo; Solution: Ensure runtime-versions: nodejs: 18 is set in buildspec.yml\nIssue: S3 sync permission denied Solution: Check IAM role has S3 permissions (see Step 8)\nIssue: Website shows old content Solution:\nClear browser cache Invalidate CloudFront cache if using CDN Check S3 bucket has latest files Quick Reference Resource Value Pipeline Name flyora-frontend-pipeline Build Project Name flyora-frontend-build S3 Bucket flyora-frontend-hosting Region Singapore (ap-southeast-1) Source GitHub (QuangHieu-lab/Flyora-shop) Buildspec buildspec.yml (in repo root) Logs CloudWatch: /aws/codebuild/flyora-frontend Cost Estimate Item Cost CodePipeline $1/month per active pipeline CodeBuild (Free Tier) 100 build minutes/month for 12 months S3 Storage ~$0.023/GB/month S3 Requests Minimal for static hosting CloudFront Free tier: 1TB data transfer/month Monthly (estimated) $1-3/month Summary ✅ Frontend CI/CD Pipeline Configured!\nAccomplished:\n✅ CodeBuild project for React application ✅ CodePipeline for automated workflow ✅ GitHub integration with automatic triggers ✅ Automatic deployment to S3 ✅ Optional CloudFront CDN integration Your pipeline now:\nAutomatically detects code changes in GitHub Builds React application with npm Deploys static files to S3 Serves website via S3 or CloudFront "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/2-backend/2.5/","title":"Testing API using Postman","tags":[],"description":"","content":"Objective Test the API Gateway REST endpoint integrated with the Lambda Function to verify the data operations performed on DynamoDB.\nDownload and install Postman before starting this section.\n1. Update Authorization Settings Go to AWS Console → API Gateway Select FlyoraAPI Navigate to\n/api/v1/{myProxy+} → ANY → Method request → Edit Set Authorization to AWS_IAM(Note: Only enable when testing with Postman, then remember to turn it off afterwards) 2. Create an Access Key Go to AWS Console → IAM → Users Click Create User Set username: test Confirm user creation Open the test user → Security credentials → Create access key Choose Local code Copy the Access key and Secret access key Testing GET Request Open Postman\nChoose GET\nEnter URL:https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev/api/v1/reviews/product/1\nHeaders tab:\nKey: Content-Type | Value: application/json Authorization tab:\nType: AWS Signature Enter AccessKey Enter SecretKey AWS Region: ap-southeast-1 Service Name: execute-api Click Send\nResult: Returns the list of Items from the reviews table.\nTesting POST Request Choose POST\nURL:https://\u0026lt;api_id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/dev/api/v1/reviews/submit\nBody → raw → JSON\n{ \u0026#34;customerId\u0026#34;: 2, \u0026#34;rating\u0026#34;: 4, \u0026#34;comment\u0026#34;: \u0026#34;Chim ăn ngon và vui vẻ!\u0026#34;, \u0026#34;customerName\u0026#34;: \u0026#34;Nguyễn Văn B\u0026#34; } Click Send\nResult: Inserts a new Item into the Review table.\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploying the Flyora E-commerce System on AWS Overview In this workshop, you will deploy the core components of the Flyora platform using a Serverless architecture on AWS.\nThe objective is to build a system that is scalable, cost-efficient, and easy to maintain.\nComponents to be deployed:\nFrontend: Store \u0026amp; deliver UI via S3 + CloudFront Backend API: Handle business logic with API Gateway + AWS Lambda Database: Manage product / order data using DynamoDB + S3 Chatbot: Product consultation assistant integrated into UI (handled by AI Team) The workshop is divided into group roles for parallel development: Backend (BE), AI (Chatbot), and Frontend (FE).\nSystem Architecture Workshop Content Introduction: Objectives \u0026amp; Expected Outcomes\nBackend Workshop (BE) — Build API + Automated Data Import Pipeline\nPrepare \u0026amp; upload CSV data to S3 Create Lambda to automatically write CSV data to DynamoDB (S3 Trigger) Create API Gateway and integrate Lambda as Backend API Test API via Postman / API Gateway Console AI Workshop (Chatbot) — Product Consultation Support\n(Create VPC \u0026amp; Configure Security Groups for RDS and Lambda) (Configure RDS and connect to Dbeaver) (Creating logic for lambda function) Frontend Workshop (FE) — Display Data \u0026amp; Hosting website\nHosting website with S3 Distribute with CloudFront API Integration Set Up CI/CD for Automatic Deployment\nResource Cleanup to Avoid Unnecessary Charges\nThis workshop is designed to run within the AWS Free Tier,\nusing no EC2, no SSH, and no paid services beyond free tier limits.\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/6-self-evaluation/","title":"Self-evaluation","tags":[],"description":"","content":"Throughout my internship at AWS Vietnam from September 8th to November 28, I had the opportunity to learn, practice, and apply the knowledge I acquired at university to a real working environment.\nI participated in a project building an e-commerce website specializing in selling products for pet birds, through which I improved my skills in programming, analysis, report writing, workplace communication, and self-learning.\nRegarding work attitude, I always strived to complete tasks well, comply with regulations, and actively communicate with colleagues to improve work efficiency.\nTo objectively reflect on the internship process, I would like to self-evaluate based on the following criteria:\nNo. Criteria Description Excellent Good Average 1 Professional knowledge and skills Industry understanding, practical knowledge application, tool proficiency, work quality ☐ ✅ ☐ 2 Learning ability Absorbing new knowledge, learning quickly ☐ ✅ ☐ 3 Proactiveness Self-researching, taking on tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing work on time, ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, regulations, work processes ✅ ☐ ☐ 6 Growth mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas, reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues, participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, work environment ✅ ☐ ☐ 10 Problem-solving mindset Identifying problems, proposing solutions, creativity ☐ ✅ ☐ 11 Contribution to project/organization Work effectiveness, improvement initiatives, recognition from team ✅ ☐ ☐ 12 Overall General assessment of the entire internship process ✅ ☐ ☐ Areas for Improvement I need to learn how to analyze problems from multiple perspectives, understand the root causes deeply to provide optimal and quick solutions. I need to improve my ability to present ideas and respond in meetings and workplace communication to ensure exchanges are always clear and effective. Although I have been proactive in learning, I need to further enhance my ability to find resources, approach new technologies, and apply them to work. "},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/6-cleanup/","title":"Clean Up Resources to Avoid AWS Charges","tags":[],"description":"","content":"Clean Up AWS Resources To avoid unexpected charges, you need to delete the AWS resources created during this workshop in the following order:\n1. Delete EventBridge Rule Go to AWS Console → EventBridge Select Rules Select the DatabaseBackup rule Click Delete Confirm deletion 2. Delete API Gateway Go to AWS Console → API Gateway Select FlyoraAPI Choose Actions → Delete API Confirm deletion by entering the API name Click Delete 3. Delete Lambda Functions Go to AWS Console → Lambda Delete the following Lambda functions: DynamoDB_API_Handler AutoImportCSVtoDynamoDB DatabaseBackupFunction For each function: Select the function → Actions → Delete Confirm deletion 4. Delete DynamoDB Tables Go to AWS Console → DynamoDB Select Tables Delete all tables created from CSV files: Select a table → Delete Confirm deletion by typing delete Repeat for all tables (Products, Orders, Customers, Reviews, etc.) 5. Delete S3 Buckets and Objects 5.1. Delete Database Bucket Go to AWS Console → S3 Select the flyora-bucket-database bucket Delete all objects in the bucket: Click Empty bucket Confirm by typing permanently delete After the bucket is empty: Select the bucket → Delete bucket Confirm by entering the bucket name 5.2. Delete Backup Bucket Select the flyora-bucket-backup bucket Delete all objects: Click Empty bucket Confirm deletion Delete the bucket: Click Delete bucket Confirm by entering the bucket name 6. Delete IAM User and Access Key Go to AWS Console → IAM → Users Select the test user Go to the Security credentials tab Delete the Access Key you created: Select the Access Key → Actions → Delete Return to the Users list Select the test user → Delete user Confirm deletion 7. Delete IAM Roles 7.1. Delete LambdaAPIAccessRole Go to AWS Console → IAM → Roles Select LambdaAPIAccessRole Detach the attached policies: AmazonDynamoDBFullAccess CloudWatchLogsFullAccess AWSXRayDaemonWriteAccess Click Delete role Confirm deletion 7.2. Delete LambdaS3DynamoDBRole Select LambdaS3DynamoDBRole Detach the policies: AmazonS3FullAccess AmazonDynamoDBFullAccess_v2 Click Delete role Confirm deletion 7.3. Delete LambdaDynamoDBBackupRole Select LambdaDynamoDBBackupRole Detach the policies: AmazonDynamoDBReadOnlyAccess AmazonS3FullAccess AWSLambdaBasicExecutionRole Click Delete role Confirm deletion 8. Delete CloudWatch Logs Go to AWS Console → CloudWatch Select Logs → Log groups Find and delete the related log groups: /aws/lambda/DynamoDB_API_Handler /aws/lambda/AutoImportCSVtoDynamoDB /aws/lambda/DatabaseBackupFunction /aws/apigateway/FlyoraAPI Select log group → Actions → Delete log group(s) Confirm deletion 9. Delete X-Ray Traces (Optional) X-Ray traces automatically expire after 30 days and don\u0026rsquo;t incur storage charges, but you can manually delete them if desired.\nGo to AWS Console → X-Ray Select Traces Traces will be automatically deleted after the default retention period 10. Delete RDS and Subnet groups Go to Subnet groups, select the created subnet group, and click Delete. Go to Databases, select the created database → Actions → Delete. 11. Delete BirdShopChatBot Lambda and Layer Go to Functions, select BirdShopChatBot → Actions → Delete. Go to Layers, select the created layer, and click Delete. 12. Delete VPC, NAT Gateway, Elastic IP, and EC2 Go to VPC, select the created NAT gateway → Actions → Delete NAT gateway. Select Elastic IPs → Actions → Release Elastic IP addresses. After deleting the NAT gateway and Elastic IP, go to Your VPCs, select the created VPC → Actions → Delete VPC. Go to EC2, select Instances, choose the created EC2 instance → Instance state → Terminate instance. 13. Delete Cloudfront Go to CloudFront, select the created distribution → Actions → Disable. Wait until the status changes to Disabled. Select the checkbox for the disabled distribution again. Choose Delete and confirm the deletion. The distribution cannot be recovered once deleted. Final Verification After completing the steps above, verify the following services to ensure no resources remain:\n✅ EventBridge: No rules remaining ✅ API Gateway: No APIs remaining ✅ Lambda: No functions remaining (3 functions) ✅ DynamoDB: No tables remaining ✅ S3: No buckets remaining (2 buckets) ✅ Cloudfront: No more distribution ✅ IAM Users: No test user remaining ✅ IAM Roles: No created roles remaining (3 roles) ✅ CloudWatch Logs: No related log groups remaining ✅ X-Ray: Traces will expire automatically ✅ RDS: Successfully deleted ✅ NAT gateway: No longer exists ✅ Elastic IP: No longer exists ✅ EC2 : Terminated Make sure you\u0026rsquo;ve deleted all resources to avoid unexpected charges. Pay special attention to S3 buckets as they can accumulate data over time.\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/2-backend/2.6/","title":"S3 CSV Backup","tags":[],"description":"","content":"S3 CSV Backup Create IAM Role for Lambda Go to AWS Management Console → search for IAM. Select Roles → Create Role. Choose Trusted entity type: AWS service. Choose Use case: Lambda, then click Next. Attach Permissions to the Role Attach the following policies:\nAmazonDynamoDBReadOnlyAccess AmazonS3FullAccess AWSLambdaBasicExecutionRole Click Next, then name the role: LambdaDynamoDBBackupRole.\nThis role allows the Lambda function to scan all DynamoDB tables and store the backup as CSV files in an S3 bucket.\nCreate S3 Bucket Open the S3 service. In the S3 dashboard, click Create bucket. In the Create bucket screen:\nBucket name: Enter a name, for example:\nflyora-bucket-backup (If the name is already taken, add a number at the end.)\nLeave all other configuration settings as default.\nReview the configuration and click Create bucket to finish. Configure Lambda Trigger for S3 Create Lambda Function Go to Lambda → Create function. Choose Author from scratch Name: DatabaseBackupFunction Runtime: Python 3.14 Role: select LambdaDynamoDBBackupRole created earlier Go to Configuration → Environment variables Click Edit Add environment variable: Key: BUCKET_NAME Value: flyora-bucket-backup Click Save Code\nimport boto3 import csv import io import os from datetime import datetime from boto3.dynamodb.conditions import Key s3 = boto3.client(\u0026#39;s3\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) def scan_all(table): \u0026#34;\u0026#34;\u0026#34;Scan toàn bộ bảng DynamoDB, xử lý paging.\u0026#34;\u0026#34;\u0026#34; items = [] response = table.scan() items.extend(response.get(\u0026#39;Items\u0026#39;, [])) while \u0026#39;LastEvaluatedKey\u0026#39; in response: response = table.scan(ExclusiveStartKey=response[\u0026#39;LastEvaluatedKey\u0026#39;]) items.extend(response.get(\u0026#39;Items\u0026#39;, [])) return items def lambda_handler(event, context): bucket = os.environ[\u0026#34;BUCKET_NAME\u0026#34;] tables = [t.strip() for t in os.environ[\u0026#34;TABLE_LIST\u0026#34;].split(\u0026#34;,\u0026#34;)] timestamp = datetime.utcnow().strftime(\u0026#34;%Y-%m-%d-%H-%M-%S\u0026#34;) for table_name in tables: try: table = dynamodb.Table(table_name) data = scan_all(table) if not data: print(f\u0026#34;Table {table_name} EMPTY → skip\u0026#34;) continue # Lấy danh sách tất cả fields all_keys = sorted({key for item in data for key in item.keys()}) # Convert to CSV csv_buffer = io.StringIO() writer = csv.DictWriter(csv_buffer, fieldnames=all_keys) writer.writeheader() for item in data: writer.writerow({k: item.get(k, \u0026#34;\u0026#34;) for k in all_keys}) key = f\u0026#34;dynamo_backup/{table_name}/{table_name}_{timestamp}.csv\u0026#34; s3.put_object( Bucket=bucket, Key=key, Body=csv_buffer.getvalue().encode(\u0026#34;utf-8\u0026#34;) ) print(f\u0026#34;Backup xong bảng {table_name} → {key}\u0026#34;) except Exception as e: print(f\u0026#34;Lỗi khi backup bảng {table_name}: {e}\u0026#34;) return { \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;tables\u0026#34;: tables } Click Deploy in the Lambda console. Configure Automatic Schedule Go to Lambda → Triggers → Add Trigger → EventBridge (Schedule)\nRule name: DatabaseBackup\nRule description: AutoBackup in 4 days\nRule type: Schedule expression\nSchedule expression: rate(4 days)\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/7-feedback/","title":"Feedback and Suggestions","tags":[],"description":"","content":" Here you can freely share your personal opinions about your experiences participating in the First Cloud Journey program, helping the FCJ team improve areas that need enhancement based on the following categories:\nGeneral Assessment 1. Work Environment\nThe work environment at FCJ feels professional yet very approachable. The atmosphere is comfortable and friendly, with everyone ready to support each other, even outside working hours. The workspace is well-organized and modern, helping me stay focused and work more effectively each day.\n2. Support from Mentors / Admin Team\nMentors always explain every issue thoroughly, patiently guide through implementation steps, and provide suggestions that help me find solutions independently. The admin team provides very quick support, especially with documentation, processes, and necessary information, ensuring my learning process remains uninterrupted.\n3. Alignment Between Work and Academic Major\nThe assigned tasks align well with the foundational knowledge I learned but are still challenging enough to expand my skills. Many new technologies and processes not covered at school have helped me supplement practical knowledge, bridging the gap between theory and real-world business applications.\n4. Event Organization\nFCJ regularly organizes events, workshops, and internal sharing sessions that enhance knowledge and strengthen connections among members. These events are well-prepared with valuable content, creating spaces for everyone to network, learn, and relax after work. This is a major plus as it helps interns integrate faster and feel like part of the community.\n5. Culture \u0026amp; Team Spirit\nThe culture at FCJ is very positive: respect – support – grow together. Everyone is always willing to share knowledge, provide sincere feedback, and help each other when projects get intense. The teamwork spirit is very strong, making me feel included even as just an intern.\nAdditional Questions What are you most satisfied with during your internship?\nI\u0026rsquo;m most satisfied with the dynamic learning environment, enthusiastic mentors, comprehensive internal documentation for newcomers, and opportunities to participate in meaningful events and workshops. What do you think the company should improve for future interns?\nCurrently, I don\u0026rsquo;t have any improvement suggestions. The company\u0026rsquo;s internship program has supported me very well and I feel satisfied. If recommending to friends, would you encourage them to intern here? Why?\nYes. Because this is a suitable environment to develop real skills, with a supportive community that helps each other progress. Proposals \u0026amp; Aspirations Do you have any suggestions to improve the internship experience?\nCurrently, I don\u0026rsquo;t have any additional suggestions. My internship experience has been quite good and I feel satisfied with the support from my colleagues. Would you like to continue this program in the future?\nI would like to continue participating or accompanying the program in the future because this is a good environment for personal development. Other feedback (feel free to share):\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/5-workshop/2-backend/2.7/","title":"Integrating AWS X-Ray","tags":[],"description":"","content":"Objective Use AWS X-Ray to trace and inspect the entire processing flow of API Gateway → Lambda → DynamoDB.\nX-Ray helps visualize traces, latency, errors, and segments/subsegments to ensure the API behaves correctly and efficiently.\nImplementation Steps 1. Access IAM Service Go to AWS Console → IAM Select Roles → LambdaAPIAccessRole Choose Add permissions → Attach policies → AWSXRayDaemonWriteAccess 2. Access Lambda Go to AWS Console → Lambda Select Functions → DynamoDB_API_Handler Go to\nConfiguration → Monitoring and operations tools → Additional monitoring tools → Edit Under Lambda service traces, enable Enable 3. Access API Gateway Go to AWS Console → API Gateway Select APIs → FlyoraAPI Go to\nStages → Logs and tracing → Edit Check X-Ray tracing 4. Testing Go to AWS Console → Lambda In the Test tab, click Create new event\nEvent name: test\nPaste the JSON below into the event:\n{ \u0026#34;resource\u0026#34;: \u0026#34;/{myProxy+}\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/api/v1/bird-types\u0026#34;, \u0026#34;httpMethod\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;headers\u0026#34;: {}, \u0026#34;multiValueHeaders\u0026#34;: {}, \u0026#34;queryStringParameters\u0026#34;: {}, \u0026#34;multiValueQueryStringParameters\u0026#34;: {}, \u0026#34;pathParameters\u0026#34;: {}, \u0026#34;stageVariables\u0026#34;: {}, \u0026#34;requestContext\u0026#34;: { \u0026#34;identity\u0026#34;: {} }, \u0026#34;body\u0026#34;: null, \u0026#34;isBase64Encoded\u0026#34;: false } Save -\u0026gt; Test\nNext, go to AWS Console → X-ray In tab Traces, a new trace ID will appear Click it to view detailed trace information\n"},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://LuuViKhanh.github.io/AWS-internship-report/tags/","title":"Tags","tags":[],"description":"","content":""}]